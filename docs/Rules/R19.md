# Smell: Threshold-dependent validation metrics

## Motivation  

Many classification scores (accuracy, precision, recall, F-score, confusion-matrix-based rates, … ) assume a single cutoff on the model’s probability or logit output.

When a task is imbalanced or the optimal threshold is unknown, reporting only (or mostly) these threshold-dependent metrics can paint a very optimistic or very pessimistic picture.
Healthier practice is to balance them with threshold-independent metrics that evaluate the full score distribution:

**Threshold-dependent**: 	
accuracy, precision, recall, F1, specificity... 

**Threshold-independent**	ROC AUC, PR AUC, log-loss, hinge-loss, mean-squared-error (for regressors treated as probabilistic outputs), ...

Rule R19 flags code where the number of threshold-independent metrics **is not strictly larger** than the number of threshold-dependent ones.

---

## Detection strategy (high‑level view)

1. **AST parsing**  
   The source file is transformed into its Abstract Syntax Tree so each
   constructor call can be inspected structurally.

2. **Metric calls enumeration**  
    Walk the AST and count every call that looks like a metric function (`isMetricCall`).

3. **Classification into two buckets**
    - `isThresholdDependent(m)`
    - `isThresholdIndependent(m)`

4. **Threshold check**  

```python
if count_independent <= count_dependent:
    report("Too many threshold-dependent metrics")
```

5. **Reporting**  

   A single, global warning (no line number) because the problem is aggregate, not local to a single statement.




---

## Practical meaning for the developer 


### Minimal demo – will trigger the rule
```python
from sklearn.metrics import accuracy_score, precision_score, roc_auc_score

acc  = accuracy_score(y_true, y_pred_labels)
prec = precision_score(y_true, y_pred_labels)
auc  = roc_auc_score(y_true, y_pred_probs)
```
Dependent metrics: 2 (accuracy, precision)
Independent metrics: 1 (ROC AUC)
“Too many threshold-dependent metrics”.

### Pandas

```python
# BEFORE – logic bug
bad_rows = df[df["amount"] != np.nan]   # returns the FULL DataFrame

# AFTER  – explicit is-missing test
bad_rows = df[df["amount"].notna()]

```

### Fixed version
```python
from sklearn.metrics import (
    accuracy_score, precision_score,
    roc_auc_score, average_precision_score, log_loss
)

acc   = accuracy_score(y_true, y_pred_labels)
prec  = precision_score(y_true, y_pred_labels)

roc   = roc_auc_score(y_true, y_pred_probs)          # independent
pr_auc= average_precision_score(y_true, y_pred_probs) # independent
ll    = log_loss(y_true, y_pred_probs)               # independent
```
Dependent: 2  Independent: 3 → No warning.

---

## Limitations  

- The detector uses name heuristics (f1_score, roc_auc_score, …). If a project wraps metrics inside custom helpers, the rule may miss them unless those helpers are added to the lookup tables.

- The rule doesn’t consider regression metrics or ranking losses except those explicitly listed – extend predicate `isThresholdIndependent` if your stack uses others.