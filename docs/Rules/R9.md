# Smell: Gradients Not Cleared before Backward Propagation

## Motivation  

In iterative training loops you **must reset accumulated gradients** before calling
`loss.backward()` (PyTorch) or `tape.gradient()` (TensorFlow).  
If you forget to do so, gradients from previous batches are *added* to the new ones,
producing:

* exploding / drifting parameter updates,  
* unstable learning curves,  
* misleading loss & metric plots.

The canonical reset is:

```python
optimizer.zero_grad()   # PyTorch
# or
optimizer.clear_grad()  # PaddlePaddle
# or
with tf.GradientTape() as tape:  # TensorFlow records separately
    ...
```

---

## Detection strategy (high-level view)

1. **AST parsing**  
   Parse file into an AST, attach `parent` pointers and pre‑collect all lines containing `model.train()` or `optimizer.step()` – used to identify the training phase.

2. **Locate .Call() invocations**  
   For every `ast.Call` that is a loss `backward()` (PyTorch) **or** generic gradient computation, check if it is **preceded** (on any earlier line in the same function/file) by `optimizer.zero_grad()` (or, in Paddle, followed by `optimizer.clear_grad()`).

3. **Legitimate exceptions**  
   Ignore calls inside a `with torch.no_grad():` block – no gradients expected.

5. **Reporting**
    Each offending site is reported as
REPORT: `optimizer.zero_grad()` not called before `loss_fn.backward()` at line  `n`
where `n` is the exact line of the bad call.

---

## Practical fix  

```python
# BEFORE – buggy
loss.backward()
optimizer.step()

# AFTER  – correct
optimizer.zero_grad()      #  flush old grads
loss.backward()            #  compute new grads
optimizer.step()           #  update
```

For PaddlePaddle use `optimizer.clear_grad()`; in mixed‑precision or distributed
setups still call the reset on the main optimizer object.

---

## Limitations  

- Inter‑file analysis is not performed; if `zero_grad()` is called in a helper
  function the rule may raise a false positive.  
- The heuristic assumes the standard API names (`zero_grad`, `clear_grad`,
  `backward`). Custom wrappers require additional patterns.  
- The rule does **not** verify that `zero_grad()` *always* precedes
  *every* backward in complex control‑flow (e.g. conditional branches inside loops).  
