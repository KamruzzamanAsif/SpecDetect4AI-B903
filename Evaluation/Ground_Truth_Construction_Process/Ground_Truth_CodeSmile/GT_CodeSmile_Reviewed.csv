Rule_ID;Old_Type;New_Type;File;Line;Description;Code_Context
R10;False_Negative;False_Negative;./files/EvalID_49/codebook.py;76;Missed detection (in ground truth but not detected);"68:     km_models: list of KMeans fitted models
69:   """"""
70:   assert len(activation_indexes) > 0
71:   assert 0.0 < sample_size <= 1.0
72:   km_models = [KMeans(2**bits)] * len(activation_indexes)
73:   cb_tables = [[]] * len(activation_indexes)
74:   models = []
75:   x = x_in = model.layers[0].output
76:   for i in range(1, len(model.layers)):
77:     layer = model.layers[i]
78:     x = layer(x)
79:     if i in activation_indexes or i == len(model.layers) - 1:
80:       print(""\nCreating submodel..."")
81:       models.append(Model([x_in], [x]))
82:       x = x_in = Input(layer.output[0].shape,
83:                        batch_size=layer.output.shape[0],
84:                        dtype=layer.output.dtype)"
R10;False_Negative;True_Negative;./files/EvalID_49/codebook.py;139;Missed detection (in ground truth but not detected);"131:     codebook_table: array of codebook values
132:   """"""
133:   assert bits <= 8
134:   n = 2**bits
135:   index_table = []
136:   codebook_table = np.zeros((weights.shape[axis], n))
137:   km_models = [None] * weights.shape[axis]
138: 
139:   for i, w in tqdm(enumerate(np.split(weights, weights.shape[axis], axis))):
140:     original_shape = w.shape
141:     w = w.ravel()
142:     km = KMeans(n)
143:     km.fit(w.reshape(-1, 1))
144:     if quantizer:
145:       km.cluster_centers_ = quantizer(km.cluster_centers_).numpy()
146:     km.cluster_centers_.sort(axis=0)
147:"
R10;False_Negative;False_Negative;./files/EvalID_49/codebook.py;187;Missed detection (in ground truth but not detected);"179:   codebook_table = np.zeros((n, n))
180: 
181:   km1 = KMeans(n)
182:   km1.fit(embeddings)
183:   tier1 = km1.predict(embeddings)
184: 
185:   km_models = [0] * n
186:   block_sizes = [0] * n
187:   for block_label in tqdm(range(n)):
188:     mask = block_label == tier1
189:     indices = np.arange(embeddings.shape[0])[mask]
190:     block = embeddings[mask]
191:     km2 = KMeans(n)
192:     km2.fit(block.flatten().reshape(-1, 1))
193:     if quantizer:
194:       km2.cluster_centers_ = quantizer(km2.cluster_centers_).numpy()
195:     km2.cluster_centers_.sort(axis=0)"
R10;False_Negative;False_Negative;./files/EvalID_46/polblogs_experiment.py;296;Missed detection (in ground truth but not detected);"288:                   training_ratios=numpy.arange(0.01, 0.10, 0.01),
289:                   max_iter=1000,
290:                   scale_columns=True):
291:   n = weights.shape[0]
292:   if scale_columns:
293:     weights = scale(weights, with_mean=False, axis=0)
294:   macro_scores = dict(zip(training_ratios, [0.0] * len(training_ratios)))
295:   micro_scores = dict(zip(training_ratios, [0.0] * len(training_ratios)))
296:   for r in training_ratios:
297:     macros = 0.0
298:     micros = 0.0
299:     for _ in range(num_fits):
300:       training_sample = numpy.random.choice(list(range(n)), int(n * r))
301:       multi_linsvm = OneVsRestClassifier(LinearSVC(max_iter=max_iter))
302:       multi_linsvm.fit(weights[training_sample], labels[training_sample])
303:       macros += f1_score(
304:           labels, multi_linsvm.predict(weights), average='macro') / num_fits"
R10;False_Negative;True_Negative;./files/EvalID_46/polblogs_experiment.py;299;Missed detection (in ground truth but not detected);"291:   n = weights.shape[0]
292:   if scale_columns:
293:     weights = scale(weights, with_mean=False, axis=0)
294:   macro_scores = dict(zip(training_ratios, [0.0] * len(training_ratios)))
295:   micro_scores = dict(zip(training_ratios, [0.0] * len(training_ratios)))
296:   for r in training_ratios:
297:     macros = 0.0
298:     micros = 0.0
299:     for _ in range(num_fits):
300:       training_sample = numpy.random.choice(list(range(n)), int(n * r))
301:       multi_linsvm = OneVsRestClassifier(LinearSVC(max_iter=max_iter))
302:       multi_linsvm.fit(weights[training_sample], labels[training_sample])
303:       macros += f1_score(
304:           labels, multi_linsvm.predict(weights), average='macro') / num_fits
305:       micros += f1_score(
306:           labels, multi_linsvm.predict(weights), average='micro') / num_fits
307:     macro_scores[r] = macros"
R10;False_Negative;False_Negative;./files/EvalID_46/polblogs_experiment.py;453;Missed detection (in ground truth but not detected);"445:   }
446:   gamma_range = numpy.logspace(-6, 1, 6)
447:   C_range = numpy.logspace(C_log_lims[0], 1, C_log_lims[1])
448:   gamma_range = numpy.logspace(gamma_log_lims[0], 1, gamma_log_lims[1])
449:   lin_pipe = Pipeline([('scale', StandardScaler()), ('clf', LinearSVC())])
450:   lin_param_grid = dict(clf__C=C_range)
451:   rbf_pipe = Pipeline([('scale', StandardScaler()), ('clf', SVC())])
452:   rbf_param_grid = dict(clf__C=C_range, clf__gamma=gamma_range)
453:   for j, r in enumerate(training_ratios):
454:     print('--training ratio %0.3f' % r)
455:     # Choose training set
456:     numpy.random.seed(random_state + j)
457:     train_set = numpy.random.randint(low=0, high=n, size=int(r * n))
458:     train_set_set = set(train_set)
459:     X_train = X[train_set]
460:     y_train = y[train_set]
461:     X_test = numpy.array([v for i, v in enumerate(X) if i not in train_set_set])"
R10;False_Negative;False_Negative;./files/EvalID_45/backbone_utils.py;472;Missed detection (in ground truth but not detected);"464:                           mobilenet_backbones, efficientnet_backbones,
465:                           efficientnet_v2_backbones])
466:         raise ValueError('Invalid value for `backbone`. Must be one of: %s' %
467:                          ', '.join(backbones))
468: 
469:     if frames_per_batch > 1:
470: 
471:         time_distributed_outputs = []
472:         for i, out in enumerate(layer_outputs):
473:             td_name = f'td_{i}'
474:             model_name = f'model_{i}'
475:             time_distributed_outputs.append(
476:                 TimeDistributed(Model(model.input, out, name=model_name),
477:                                 name=td_name)(input_tensor))
478: 
479:         if time_distributed_outputs:
480:             layer_outputs = time_distributed_outputs"
R10;False_Negative;False_Negative;./files/EvalID_44/codebook.py;76;Missed detection (in ground truth but not detected);"68:     km_models: list of KMeans fitted models
69:   """"""
70:   assert len(activation_indexes) > 0
71:   assert 0.0 < sample_size <= 1.0
72:   km_models = [KMeans(2**bits)] * len(activation_indexes)
73:   cb_tables = [[]] * len(activation_indexes)
74:   models = []
75:   x = x_in = model.layers[0].output
76:   for i in range(1, len(model.layers)):
77:     layer = model.layers[i]
78:     x = layer(x)
79:     if i in activation_indexes or i == len(model.layers) - 1:
80:       print(""\nCreating submodel..."")
81:       models.append(Model([x_in], [x]))
82:       x = x_in = Input(layer.output[0].shape,
83:                        batch_size=layer.output.shape[0],
84:                        dtype=layer.output.dtype)"
R10;False_Negative;False_Negative;./files/EvalID_44/codebook.py;139;Missed detection (in ground truth but not detected);"131:     codebook_table: array of codebook values
132:   """"""
133:   assert bits <= 8
134:   n = 2**bits
135:   index_table = []
136:   codebook_table = np.zeros((weights.shape[axis], n))
137:   km_models = [None] * weights.shape[axis]
138: 
139:   for i, w in tqdm(enumerate(np.split(weights, weights.shape[axis], axis))):
140:     original_shape = w.shape
141:     w = w.ravel()
142:     km = KMeans(n)
143:     km.fit(w.reshape(-1, 1))
144:     if quantizer:
145:       km.cluster_centers_ = quantizer(km.cluster_centers_).numpy()
146:     km.cluster_centers_.sort(axis=0)
147:"
R10;False_Negative;False_Negative;./files/EvalID_44/codebook.py;187;Missed detection (in ground truth but not detected);"179:   codebook_table = np.zeros((n, n))
180: 
181:   km1 = KMeans(n)
182:   km1.fit(embeddings)
183:   tier1 = km1.predict(embeddings)
184: 
185:   km_models = [0] * n
186:   block_sizes = [0] * n
187:   for block_label in tqdm(range(n)):
188:     mask = block_label == tier1
189:     indices = np.arange(embeddings.shape[0])[mask]
190:     block = embeddings[mask]
191:     km2 = KMeans(n)
192:     km2.fit(block.flatten().reshape(-1, 1))
193:     if quantizer:
194:       km2.cluster_centers_ = quantizer(km2.cluster_centers_).numpy()
195:     km2.cluster_centers_.sort(axis=0)"
R10;False_Negative;False_Negative;./files/EvalID_42/toy_helper.py;151;Missed detection (in ground truth but not detected);"143:   tcav_model.summary()
144: 
145:   n_cluster = concept_arraynew_active.shape[0]
146:   n_percluster = concept_arraynew_active.shape[1]
147:   print(concept_arraynew_active.shape)
148:   weight_ace = np.zeros((200, n_cluster))
149:   tcav_list_rand = np.zeros((15, 200))
150:   tcav_list_ace = np.zeros((15, n_cluster))
151:   for i in range(n_cluster):
152:     y = np.zeros((n_cluster * n_percluster))
153:     y[i * n_percluster:(i + 1) * n_percluster] = 1
154:     clf = LogisticRegression(
155:         random_state=0,
156:         solver='lbfgs',
157:         max_iter=10000,
158:         C=10.0,
159:         multi_class='ovr').fit(concept_arraynew_active.reshape((-1, 200)), y)"
R10;False_Negative;True_Negative;./files/EvalID_42/toy_helper.py;163;Missed detection (in ground truth but not detected);"155:         random_state=0,
156:         solver='lbfgs',
157:         max_iter=10000,
158:         C=10.0,
159:         multi_class='ovr').fit(concept_arraynew_active.reshape((-1, 200)), y)
160:     weight_ace[:, i] = clf.coef_
161: 
162:   weight_rand = np.zeros((200, 200))
163:   for i in range(200):
164:     y = np.random.randint(2, size=n_cluster * n_percluster)
165:     clf = LogisticRegression(
166:         random_state=0,
167:         solver='lbfgs',
168:         max_iter=10000,
169:         C=10.0,
170:         multi_class='ovr').fit(concept_arraynew_active.reshape((-1, 200)), y)
171:     weight_rand[:, i] = clf.coef_"
R10;False_Negative;False_Negative;./files/EvalID_47/awa_helper.py;201;Missed detection (in ground truth but not detected);"193:   tcav_model.summary()
194: 
195:   n_cluster = concept_arraynew_active.shape[0]
196:   n_percluster = concept_arraynew_active.shape[1]
197:   print(concept_arraynew_active.shape)
198:   weight_ace = np.zeros((1024, n_cluster))
199:   tcav_list_rand = np.zeros((50, 200))
200:   tcav_list_ace = np.zeros((50, 134))
201:   for i in range(n_cluster):
202:     y = np.zeros((n_cluster * n_percluster))
203:     y[i * n_percluster:(i + 1) * n_percluster] = 1
204:     clf = LogisticRegression(
205:         random_state=0,
206:         solver='lbfgs',
207:         max_iter=10000,
208:         C=10.0,
209:         multi_class='ovr').fit(concept_arraynew_active.reshape((-1, 1024)), y)"
R10;False_Negative;False_Negative;./files/EvalID_48/codebook.py;76;Missed detection (in ground truth but not detected);"68:     km_models: list of KMeans fitted models
69:   """"""
70:   assert len(activation_indexes) > 0
71:   assert 0.0 < sample_size <= 1.0
72:   km_models = [KMeans(2**bits)] * len(activation_indexes)
73:   cb_tables = [[]] * len(activation_indexes)
74:   models = []
75:   x = x_in = model.layers[0].output
76:   for i in range(1, len(model.layers)):
77:     layer = model.layers[i]
78:     x = layer(x)
79:     if i in activation_indexes or i == len(model.layers) - 1:
80:       print(""\nCreating submodel..."")
81:       models.append(Model([x_in], [x]))
82:       x = x_in = Input(layer.output[0].shape,
83:                        batch_size=layer.output.shape[0],
84:                        dtype=layer.output.dtype)"
R10;False_Negative;True_Negative;./files/EvalID_48/codebook.py;139;Missed detection (in ground truth but not detected);"131:     codebook_table: array of codebook values
132:   """"""
133:   assert bits <= 8
134:   n = 2**bits
135:   index_table = []
136:   codebook_table = np.zeros((weights.shape[axis], n))
137:   km_models = [None] * weights.shape[axis]
138: 
139:   for i, w in tqdm(enumerate(np.split(weights, weights.shape[axis], axis))):
140:     original_shape = w.shape
141:     w = w.ravel()
142:     km = KMeans(n)
143:     km.fit(w.reshape(-1, 1))
144:     if quantizer:
145:       km.cluster_centers_ = quantizer(km.cluster_centers_).numpy()
146:     km.cluster_centers_.sort(axis=0)
147:"
R10;False_Negative;False_Negative;./files/EvalID_48/codebook.py;187;Missed detection (in ground truth but not detected);"179:   codebook_table = np.zeros((n, n))
180: 
181:   km1 = KMeans(n)
182:   km1.fit(embeddings)
183:   tier1 = km1.predict(embeddings)
184: 
185:   km_models = [0] * n
186:   block_sizes = [0] * n
187:   for block_label in tqdm(range(n)):
188:     mask = block_label == tier1
189:     indices = np.arange(embeddings.shape[0])[mask]
190:     block = embeddings[mask]
191:     km2 = KMeans(n)
192:     km2.fit(block.flatten().reshape(-1, 1))
193:     if quantizer:
194:       km2.cluster_centers_ = quantizer(km2.cluster_centers_).numpy()
195:     km2.cluster_centers_.sort(axis=0)"
R10;False_Negative;False_Negative;./files/EvalID_43/uflow_model.py;120;Missed detection (in ground truth but not detected);"112: 
113:   # Pad features2 such that shifts do not go out of bounds.
114:   features2_padded = tf.pad(
115:       tensor=features2,
116:       paddings=[[0, 0], [max_disp, max_disp], [max_disp, max_disp], [0, 0]],
117:       mode='CONSTANT')
118:   cost_list = []
119:   for i in range(num_shifts):
120:     for j in range(num_shifts):
121:       corr = tf.reduce_mean(
122:           input_tensor=features1 *
123:           features2_padded[:, i:(height + i), j:(width + j), :],
124:           axis=-1,
125:           keepdims=True)
126:       cost_list.append(corr)
127:   cost_volume = tf.concat(cost_list, axis=-1)
128:   return cost_volume"
R10;False_Negative;False_Negative;./files/EvalID_43/uflow_model.py;185;Missed detection (in ground truth but not detected);"177:     """"""Run the model.""""""
178:     context = None
179:     flow = None
180:     flow_up = None
181:     context_up = None
182:     flows = []
183: 
184:     # Go top down through the levels to the second to last one to estimate flow.
185:     for level, (features1, features2) in reversed(
186:         list(enumerate(zip(feature_pyramid1, feature_pyramid2)))[1:]):
187: 
188:       # init flows with zeros for coarsest level if needed
189:       if self._shared_flow_decoder and flow_up is None:
190:         batch_size, height, width, _ = features1.shape.as_list()
191:         flow_up = tf.zeros(
192:             [batch_size, height, width, 2],
193:             dtype=tf.bfloat16 if self._use_bfloat16 else tf.float32)"
R10;False_Negative;False_Negative;./files/EvalID_43/uflow_model.py;287;Missed detection (in ground truth but not detected);"279:           tf.math.greater(tf.random.uniform([]), self._drop_out_rate),
280:           tf.bfloat16 if self._use_bfloat16 else tf.float32)
281:     refined_flow = flow + refinement
282:     flows[0] = refined_flow
283:     return [tf.cast(flow, tf.float32) for flow in flows]
284: 
285:   def _build_cost_volume_surrogate_convs(self):
286:     layers = []
287:     for _ in range(self._num_levels):
288:       layers.append(
289:           Conv2D(
290:               int(64 * self._channel_multiplier),
291:               kernel_size=(4, 4),
292:               padding='same',
293:               dtype=self._dtype_policy))
294:     return layers
295:"
R10;False_Negative;True_Negative;./files/EvalID_43/uflow_model.py;313;Missed detection (in ground truth but not detected);"305:               padding='same',
306:               dtype=self._dtype_policy))
307:     return layers
308: 
309:   def _build_flow_layers(self):
310:     """"""Build layers for flow estimation.""""""
311:     # Empty list of layers level 0 because flow is only estimated at levels > 0.
312:     result = [[]]
313:     for _ in range(1, self._num_levels):
314:       layers = []
315:       for c in [128, 128, 96, 64, 32]:
316:         layers.append(
317:             Sequential([
318:                 Conv2D(
319:                     int(c * self._channel_multiplier),
320:                     kernel_size=(3, 3),
321:                     strides=1,"
R10;False_Negative;False_Negative;./files/EvalID_43/uflow_model.py;315;Missed detection (in ground truth but not detected);"307:     return layers
308: 
309:   def _build_flow_layers(self):
310:     """"""Build layers for flow estimation.""""""
311:     # Empty list of layers level 0 because flow is only estimated at levels > 0.
312:     result = [[]]
313:     for _ in range(1, self._num_levels):
314:       layers = []
315:       for c in [128, 128, 96, 64, 32]:
316:         layers.append(
317:             Sequential([
318:                 Conv2D(
319:                     int(c * self._channel_multiplier),
320:                     kernel_size=(3, 3),
321:                     strides=1,
322:                     padding='same',
323:                     dtype=self._dtype_policy),"
R10;False_Negative;True_Negative;./files/EvalID_43/uflow_model.py;465;Missed detection (in ground truth but not detected);"457:       self._convs.append(group)
458: 
459:   def call(self, x, split_features_by_sample=False):
460:     if self._use_bfloat16:
461:       x = tf.cast(x, tf.bfloat16)
462:     x = x * 2. - 1.  # Rescale input from [0,1] to [-1, 1]
463:     features = []
464:     for level, conv_tuple in enumerate(self._convs):
465:       for i, conv in enumerate(conv_tuple):
466:         if level > 0 or i < len(conv_tuple) - self._level1_num_1x1:
467:           x = tf.pad(
468:               tensor=x,
469:               paddings=[[0, 0], [1, 1], [1, 1], [0, 0]],
470:               mode='CONSTANT')
471:         x = conv(x)
472:         x = LeakyReLU(alpha=self._leaky_relu_alpha, dtype=self._dtype_policy)(x)
473:       features.append(x)"
R12;False_Positive;True_Positive;./files/EvalID_22/ppca.py;113;Incorrectly detected (not in ground truth);"105:   return m - m.mean(axis=0)
106: 
107: 
108: def PartialCorr(x, y, covar):
109:   """"""Calculate partial correlation.""""""
110:   cvar = np.atleast_2d(covar)
111:   beta_x = np.linalg.lstsq(cvar, x, rcond=None)[0]
112:   beta_y = np.linalg.lstsq(cvar, y, rcond=None)[0]
113:   res_x = x - np.dot(cvar, beta_x)
114:   res_y = y - np.dot(cvar, beta_y)
115:   return spearmanr(res_x, res_y)
116: 
117: 
118: def Varimax(phi, gamma=1, q=20, tol=1e-6):
119:   """"""Source: https://stackoverflow.com/questions/17628589/perform-varimax-rotation-in-python-using-numpy.""""""
120:   p, k = phi.shape
121:   r = np.eye(k)"
R12;False_Positive;True_Positive;./files/EvalID_22/ppca.py;114;Incorrectly detected (not in ground truth);"106: 
107: 
108: def PartialCorr(x, y, covar):
109:   """"""Calculate partial correlation.""""""
110:   cvar = np.atleast_2d(covar)
111:   beta_x = np.linalg.lstsq(cvar, x, rcond=None)[0]
112:   beta_y = np.linalg.lstsq(cvar, y, rcond=None)[0]
113:   res_x = x - np.dot(cvar, beta_x)
114:   res_y = y - np.dot(cvar, beta_y)
115:   return spearmanr(res_x, res_y)
116: 
117: 
118: def Varimax(phi, gamma=1, q=20, tol=1e-6):
119:   """"""Source: https://stackoverflow.com/questions/17628589/perform-varimax-rotation-in-python-using-numpy.""""""
120:   p, k = phi.shape
121:   r = np.eye(k)
122:   d = 0"
R12;False_Positive;True_Positive;./files/EvalID_22/ppca.py;125;Incorrectly detected (not in ground truth);"117: 
118: def Varimax(phi, gamma=1, q=20, tol=1e-6):
119:   """"""Source: https://stackoverflow.com/questions/17628589/perform-varimax-rotation-in-python-using-numpy.""""""
120:   p, k = phi.shape
121:   r = np.eye(k)
122:   d = 0
123:   for _ in range(q):
124:     d_old = d
125:     l = np.dot(phi, r)
126:     u, s, vh = LA.svd(
127:         np.dot(
128:             phi.T,
129:             np.asarray(l)**3 -
130:             (gamma / p) * np.dot(l, np.diag(np.diag(np.dot(l.T, l))))))
131:     r = np.dot(u, vh)
132:     d = np.sum(s)
133:     if d / d_old < tol:"
R12;False_Positive;True_Positive;./files/EvalID_22/ppca.py;127;Incorrectly detected (not in ground truth);"119:   """"""Source: https://stackoverflow.com/questions/17628589/perform-varimax-rotation-in-python-using-numpy.""""""
120:   p, k = phi.shape
121:   r = np.eye(k)
122:   d = 0
123:   for _ in range(q):
124:     d_old = d
125:     l = np.dot(phi, r)
126:     u, s, vh = LA.svd(
127:         np.dot(
128:             phi.T,
129:             np.asarray(l)**3 -
130:             (gamma / p) * np.dot(l, np.diag(np.diag(np.dot(l.T, l))))))
131:     r = np.dot(u, vh)
132:     d = np.sum(s)
133:     if d / d_old < tol:
134:       break
135:   return np.dot(phi, r)"
R12;False_Positive;True_Positive;./files/EvalID_22/ppca.py;130;Incorrectly detected (not in ground truth);"122:   d = 0
123:   for _ in range(q):
124:     d_old = d
125:     l = np.dot(phi, r)
126:     u, s, vh = LA.svd(
127:         np.dot(
128:             phi.T,
129:             np.asarray(l)**3 -
130:             (gamma / p) * np.dot(l, np.diag(np.diag(np.dot(l.T, l))))))
131:     r = np.dot(u, vh)
132:     d = np.sum(s)
133:     if d / d_old < tol:
134:       break
135:   return np.dot(phi, r)
136: 
137: 
138: def LeaveOut(ratings, rater_msk, worker2examples, worker_id):"
R12;False_Positive;True_Positive;./files/EvalID_22/ppca.py;131;Incorrectly detected (not in ground truth);"123:   for _ in range(q):
124:     d_old = d
125:     l = np.dot(phi, r)
126:     u, s, vh = LA.svd(
127:         np.dot(
128:             phi.T,
129:             np.asarray(l)**3 -
130:             (gamma / p) * np.dot(l, np.diag(np.diag(np.dot(l.T, l))))))
131:     r = np.dot(u, vh)
132:     d = np.sum(s)
133:     if d / d_old < tol:
134:       break
135:   return np.dot(phi, r)
136: 
137: 
138: def LeaveOut(ratings, rater_msk, worker2examples, worker_id):
139:   """"""Calculate correlations and partial correlations for a particular rater."""""""
R12;False_Positive;True_Positive;./files/EvalID_22/ppca.py;135;Incorrectly detected (not in ground truth);"127:         np.dot(
128:             phi.T,
129:             np.asarray(l)**3 -
130:             (gamma / p) * np.dot(l, np.diag(np.diag(np.dot(l.T, l))))))
131:     r = np.dot(u, vh)
132:     d = np.sum(s)
133:     if d / d_old < tol:
134:       break
135:   return np.dot(phi, r)
136: 
137: 
138: def LeaveOut(ratings, rater_msk, worker2examples, worker_id):
139:   """"""Calculate correlations and partial correlations for a particular rater.""""""
140:   examples = worker2examples[worker_id]
141:   use_examples = copy.deepcopy(ratings[[idx for idx, _ in examples]])
142:   use_examples_msk = copy.deepcopy(rater_msk[[idx for idx, _ in examples
143:                                              ]]).sum(axis=1).astype(int)"
R12;False_Positive;True_Positive;./files/EvalID_32/recom_amr.py;287;Incorrectly detected (not in ground truth);"279: 
280:         """"""
281:         if item_idx is None:
282:             known_item_scores = np.zeros(self.gamma_item.shape[0], dtype=np.float32)
283:             fast_dot(self.gamma_user[user_idx], self.gamma_item, known_item_scores)
284:             fast_dot(self.gamma_user[user_idx], self.theta_item, known_item_scores)
285:             return known_item_scores
286:         else:
287:             item_score = np.dot(self.gamma_item[item_idx], self.gamma_user[user_idx])
288:             item_score += np.dot(self.theta_item[item_idx], self.gamma_user[user_idx])
289:             return item_score
290: 
291:     def get_vector_measure(self):
292:         """"""Getting a valid choice of vector measurement in ANNMixin._measures.
293: 
294:         Returns
295:         -------"
R12;False_Positive;True_Positive;./files/EvalID_32/recom_amr.py;288;Incorrectly detected (not in ground truth);"280:         """"""
281:         if item_idx is None:
282:             known_item_scores = np.zeros(self.gamma_item.shape[0], dtype=np.float32)
283:             fast_dot(self.gamma_user[user_idx], self.gamma_item, known_item_scores)
284:             fast_dot(self.gamma_user[user_idx], self.theta_item, known_item_scores)
285:             return known_item_scores
286:         else:
287:             item_score = np.dot(self.gamma_item[item_idx], self.gamma_user[user_idx])
288:             item_score += np.dot(self.theta_item[item_idx], self.gamma_user[user_idx])
289:             return item_score
290: 
291:     def get_vector_measure(self):
292:         """"""Getting a valid choice of vector measurement in ANNMixin._measures.
293: 
294:         Returns
295:         -------
296:         measure: MEASURE_DOT"
R12;False_Negative;False_Negative;./files/EvalID_41/plotting.py;200;Missed detection (in ground truth but not detected);"192:   _ = datasets.make_moons(n_samples=n_samples, noise=.05)
193:   blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)
194:   no_structure = onp.random.rand(n_samples, 2), None
195: 
196:   # Anisotropicly distributed data
197:   random_state = 170
198:   X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)
199:   transformation = [[0.6, -0.6], [-0.4, 0.8]]
200:   X_aniso = onp.dot(X, transformation)
201:   aniso = (X_aniso, y)
202: 
203:   # blobs with varied variances
204:   varied = datasets.make_blobs(n_samples=n_samples,
205:                                cluster_std=[1.0, 2.5, 0.5],
206:                                random_state=random_state)
207:   return varied, aniso, blobs, no_structure
208:"
R14;True_Positive;True_Positive;./files/EvalID_19/predictor.py;82;Correctly detected;"74:     else:
75:         return flask.Response(
76:             response=""This predictor only supports CSV data"", status=415, mimetype=""text/plain""
77:         )
78: 
79:     print(""Invoked with {} records"".format(data.shape[0]))
80: 
81:     # Do the prediction
82:     predictions = ScoringService.predict(data.values)
83: 
84:     # Convert from numpy back to CSV
85:     out = StringIO()
86:     pd.DataFrame({""results"": predictions}).to_csv(out, header=False, index=False)
87:     result = out.getvalue().rstrip(
88:         ""\n""
89:     )  # this is essential for Model Monitoring to work correctly.
90:"
R14;True_Positive;True_Positive;./files/EvalID_24/data_loader.py;142;Correctly detected;"134:       val_sample_index = 0
135: 
136:       # Every customer (an entire time series) divide the customer data
137:       # into training/testing/validation,
138:       # Scale indpendently since customers have very different behaviors
139:       # Create time series based on the sequence and prediction length
140:       for i in range(1, self.num_ts):
141: 
142:         data = inputs.values[:, i]
143:         data = data.reshape(-1, 1)
144:         if self.scale:
145:           train_data = np.array(data[:training_hours, :], dtype=np.float64)
146:           self.scaler.fit(train_data)
147:           df_data = self.scaler.transform(data)
148:         else:
149:           df_data = np.array(data, dtype=np.float64)
150:"
R14;True_Positive;True_Positive;./files/EvalID_24/data_loader.py;242;Correctly detected;"234: 
235:     Returns:
236:      x: data in the form of numpy array
237:      y: labels in the form of numpy array
238: 
239:     """"""
240: 
241:     x = pd.read_csv(file_name, sep='\t', header=0 if header_in_data else None)
242:     y = x[target_col].values
243:     x.drop(target_col, axis=1, inplace=True)
244:     return x, y
245: 
246:   def __read_data__(self):
247:     # Standard scaler is used to scale data
248:     if self.scale:
249:       self.data_scaler = StandardScaler()
250:       self.label_scaler = StandardScaler()"
R14;True_Positive;True_Positive;./files/EvalID_24/data_loader.py;254;Correctly detected;"246:   def __read_data__(self):
247:     # Standard scaler is used to scale data
248:     if self.scale:
249:       self.data_scaler = StandardScaler()
250:       self.label_scaler = StandardScaler()
251: 
252:     cols = pd.read_csv(self.cd_path, sep='\t', header=None)
253:     target_col = np.where(cols[1] == 'Label')[0][0]
254:     cat_cols = cols[cols[1] == 'Categ'][0].values
255:     train_x, train_y = self.read_file(self.train_path, target_col,
256:                                       self.header_in_data)
257:     test_x, test_y = self.read_file(self.test_path, target_col,
258:                                     self.header_in_data)
259:     # Divide the training data into trian and validation
260:     data = pd.concat([train_x, test_x])
261:     data[cat_cols] = data[cat_cols].apply(
262:         lambda x: x.astype('category').cat.codes)"
R14;True_Positive;True_Positive;./files/EvalID_22/ppca.py;429;Correctly detected;"421:   ppc_scores = all_ratings_avg.dot(w_vari)  # project onto ppcs
422:   ppc_scores_abs = np.absolute(ppc_scores)
423: 
424:   # Load maximally distinct colors
425:   colors = pd.read_csv(
426:       FLAGS.rgb_colors, sep=""\t"", header=None, names=np.arange(3))
427: 
428:   # Set colors (todo(ddemszky): add names to colors in file)
429:   palette_rgb = colors.values
430:   with open(FLAGS.emotion_color_order) as f:
431:     color_order = f.read().splitlines()
432:   ppc2color = {emotion2ppc[e]: i for i, e in enumerate(color_order)}
433:   # get rgb value for each example based on weighted average of top emotions
434:   rgb_vals = []
435:   hex_vals = []
436:   top_categories = []
437:   threshold = 0.5  # exclude points not loading on any of the top 10 categories"
R14;True_Positive;True_Positive;./files/EvalID_23/util.py;36;Correctly detected;"28: from tensorflow.python.lib.io import file_io
29: 
30: 
31: def create_dataset(dataset, window_size = 1):
32:     data_X, data_y = [], []
33:     df = pd.DataFrame(dataset)
34:     columns = [df.shift(i) for i in reversed(range(1, window_size+1))]
35:     data_X = pd.concat(columns, axis=1).dropna().values
36:     data_y = df.shift(-window_size).dropna().values
37:     return data_X, data_y
38: 
39: def load_data(data_file_url, window_size):
40:     """"""Loads data into preprocessed (train_X, train_y, eval_X, eval_y) dataframes.
41: 
42:     Returns:
43:       A tuple (train_X, train_y, eval_X, eval_y), where train_X and eval_X are
44:       Pandas dataframes with features for training and train_y and eval_y are"
R14;True_Positive;True_Positive;./files/EvalID_23/util.py;59;Correctly detected;"51:     file_stream = file_io.FileIO(data_file_url, mode='r')
52:     df = pd.read_csv(StringIO(file_stream.read()))
53:     df.index = df[df.columns[0]]
54:     df = df[['count']]
55: 
56:     scaler = StandardScaler()
57: 
58:     # Time series: split latest data into test set
59:     train = df.values[:int(TRAINING_SPLIT * len(df)), :]
60:     print(train)
61:     train = scaler.fit_transform(train)
62:     test = df.values[int(TRAINING_SPLIT * len(df)):, :]
63:     test = scaler.transform(test)
64: 
65:     # Create test and training sets
66:     train_X, train_y = create_dataset(train, window_size)
67:     test_X, test_y = create_dataset(test, window_size)"
R14;True_Positive;True_Positive;./files/EvalID_23/util.py;62;Correctly detected;"54:     df = df[['count']]
55: 
56:     scaler = StandardScaler()
57: 
58:     # Time series: split latest data into test set
59:     train = df.values[:int(TRAINING_SPLIT * len(df)), :]
60:     print(train)
61:     train = scaler.fit_transform(train)
62:     test = df.values[int(TRAINING_SPLIT * len(df)):, :]
63:     test = scaler.transform(test)
64: 
65:     # Create test and training sets
66:     train_X, train_y = create_dataset(train, window_size)
67:     test_X, test_y = create_dataset(test, window_size)
68: 
69:     # Reshape input data
70:     train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))"
R14;True_Positive;True_Positive;./files/EvalID_18/regression_dataset.py;100;Correctly detected;"92:                 df.loc[~priv, attr] = unprivileged_values[0]
93: 
94:             privileged_protected_attributes.append(
95:                 np.array(privileged_values, dtype=np.float64))
96:             unprivileged_protected_attributes.append(
97:                 np.array(unprivileged_values, dtype=np.float64))
98: 
99:         # 6. Normalize df values
100:         df = pd.DataFrame(MinMaxScaler().fit_transform(df.values),
101:                           columns=list(df), index=df.index)
102: 
103:         super(RegressionDataset, self).__init__(df=df,
104:             label_names=[dep_var_name],
105:             protected_attribute_names=protected_attribute_names,
106:             privileged_protected_attributes=privileged_protected_attributes,
107:             unprivileged_protected_attributes=unprivileged_protected_attributes,
108:             instance_weights_name=instance_weights_name,"
R14;True_Positive;True_Positive;./files/EvalID_20/wine.py;237;Correctly detected;"229:   test_metrics = []
230: 
231:   with tf.Session() as sess:
232:     sess.run(init)
233: 
234:     # Training autoencoder
235:     for _ in range(num_steps_autoencoder):
236:       batch_index = random.sample(range(len(train_labels)), BATCH_SIZE)
237:       batch_x = train_features.iloc[batch_index, :].values
238:       batch_y = train_labels.iloc[batch_index].values.reshape(BATCH_SIZE, 1)
239:       _, _ = sess.run([autoencoder_optimizer, autoencoder_loss],
240:                       feed_dict={
241:                           x: batch_x,
242:                           y: batch_y,
243:                       })
244: 
245:     # GetCandidatesAlpha (Algorithm 2 in paper)"
R14;True_Positive;True_Positive;./files/EvalID_20/wine.py;284;Correctly detected;"276: 
277:           # Add candidate to the GP, assuming the metric observation is the LCB.
278:           sample_thetas = np.concatenate([sample_thetas, best_theta])
279:           sample_validation_metrics.append(best_theta_metric_ucb)
280: 
281:       # Training regressors
282:       for _ in range(TRAINING_STEPS):
283:         batch_index = random.sample(range(len(train_labels)), BATCH_SIZE)
284:         batch_x = train_features.iloc[batch_index, :].values
285:         batch_y = train_labels.iloc[batch_index].values.reshape(BATCH_SIZE, 1)
286:         batch_embedding = sess.run(
287:             autoencoder_embedding_layer, feed_dict={
288:                 x: batch_x,
289:                 y: batch_y,
290:             })
291:         _, _ = sess.run(
292:             [parallel_optimizers, parallel_losses],"
R14;True_Positive;True_Positive;./files/EvalID_20/wine.py;303;Correctly detected;"295:                 y: batch_y,
296:                 embedding: batch_embedding,
297:                 parallel_thetas: theta_batch,
298:             })
299: 
300:       parallel_validation_outputs = sess.run(
301:           parallel_outputs,
302:           feed_dict={
303:               x: validation_features.values,
304:               y: validation_labels.values.reshape(len(validation_labels), 1),
305:           })
306:       parallel_validation_metrics = [
307:           metric(validation_labels, validation_output, validation_price)
308:           for validation_output in parallel_validation_outputs
309:       ]
310:       thetas = np.concatenate([thetas, theta_batch])
311:       validation_metrics.extend(parallel_validation_metrics)"
R14;True_Positive;True_Positive;./files/EvalID_20/wine.py;316;Correctly detected;"308:           for validation_output in parallel_validation_outputs
309:       ]
310:       thetas = np.concatenate([thetas, theta_batch])
311:       validation_metrics.extend(parallel_validation_metrics)
312: 
313:       parallel_test_outputs = sess.run(
314:           parallel_outputs,
315:           feed_dict={
316:               x: test_features.values,
317:               y: test_labels.values.reshape(len(test_labels), 1),
318:           })
319:       parallel_test_metrics = [
320:           metric(test_labels, test_output, test_price)
321:           for test_output in parallel_test_outputs
322:       ]
323:       test_metrics.extend(parallel_test_metrics)
324:"
R14;True_Positive;True_Positive;./files/EvalID_21/transcode.py;118;Correctly detected;"110:         for chunks in zipped:
111:             mapping_df = pd.concat(chunks, axis=1)  # This takes care of making sure feature names are unique
112: 
113:             # Choose the right columns
114:             numerical_df = mapping_df[input_numerical_features_list]
115:             categorical_df = mapping_df[input_categorical_features_list]
116:             label_df = mapping_df[[input_label_feature_name]]
117: 
118:             numerical = torch.tensor(numerical_df.values)
119:             label = torch.tensor(label_df.values)
120:             categorical = torch.tensor(categorical_df.values)
121: 
122:             # Append them to the binary files
123:             numerical_f.write(numerical.to(torch.float16).cpu().numpy().tobytes())
124:             label_f.write(label.to(torch.bool).cpu().numpy().tobytes())
125:             for cat_idx, cat_feature_type in enumerate(categorical_feature_types):
126:                 categorical_fs[cat_idx].write("
R14;True_Positive;True_Positive;./files/EvalID_21/transcode.py;119;Correctly detected;"111:             mapping_df = pd.concat(chunks, axis=1)  # This takes care of making sure feature names are unique
112: 
113:             # Choose the right columns
114:             numerical_df = mapping_df[input_numerical_features_list]
115:             categorical_df = mapping_df[input_categorical_features_list]
116:             label_df = mapping_df[[input_label_feature_name]]
117: 
118:             numerical = torch.tensor(numerical_df.values)
119:             label = torch.tensor(label_df.values)
120:             categorical = torch.tensor(categorical_df.values)
121: 
122:             # Append them to the binary files
123:             numerical_f.write(numerical.to(torch.float16).cpu().numpy().tobytes())
124:             label_f.write(label.to(torch.bool).cpu().numpy().tobytes())
125:             for cat_idx, cat_feature_type in enumerate(categorical_feature_types):
126:                 categorical_fs[cat_idx].write(
127:                     categorical[:, cat_idx].cpu().numpy().astype(cat_feature_type).tobytes())"
R14;True_Positive;True_Positive;./files/EvalID_21/transcode.py;120;Correctly detected;"112: 
113:             # Choose the right columns
114:             numerical_df = mapping_df[input_numerical_features_list]
115:             categorical_df = mapping_df[input_categorical_features_list]
116:             label_df = mapping_df[[input_label_feature_name]]
117: 
118:             numerical = torch.tensor(numerical_df.values)
119:             label = torch.tensor(label_df.values)
120:             categorical = torch.tensor(categorical_df.values)
121: 
122:             # Append them to the binary files
123:             numerical_f.write(numerical.to(torch.float16).cpu().numpy().tobytes())
124:             label_f.write(label.to(torch.bool).cpu().numpy().tobytes())
125:             for cat_idx, cat_feature_type in enumerate(categorical_feature_types):
126:                 categorical_fs[cat_idx].write(
127:                     categorical[:, cat_idx].cpu().numpy().astype(cat_feature_type).tobytes())
128:"
R14;True_Positive;True_Positive;./files/EvalID_17/ukb.py;648;Correctly detected;"640:   features_only = features.drop(['image_id'], axis=1)
641:   output = pd.DataFrame(features['image_id'])
642: 
643:   if extract_id:
644:     eid = [_eid_from_image_id(item) for item in features['image_id']]
645:     output['eid'] = eid
646:   for i in range(b):
647:     gam, eta1, eta0 = _generation_weights(features_only.shape[1], i)
648:     pi = _pi_x_function(features_only.values, gam)
649:     mu1, mu0 = _mu_x_function(features_only.values, eta1, eta0)
650:     output['sim_' + str(i) + '_pi'] = pi
651:     output['sim_' + str(i) + '_mu1'] = mu1
652:     output['sim_' + str(i) + '_mu0'] = mu0
653: 
654:   with gfile.GFile(os.path.join(path, output_name), 'wt') as out:
655:     out.write(output.to_csv(index=False))"
R14;True_Positive;True_Positive;./files/EvalID_17/ukb.py;649;Correctly detected;"641:   output = pd.DataFrame(features['image_id'])
642: 
643:   if extract_id:
644:     eid = [_eid_from_image_id(item) for item in features['image_id']]
645:     output['eid'] = eid
646:   for i in range(b):
647:     gam, eta1, eta0 = _generation_weights(features_only.shape[1], i)
648:     pi = _pi_x_function(features_only.values, gam)
649:     mu1, mu0 = _mu_x_function(features_only.values, eta1, eta0)
650:     output['sim_' + str(i) + '_pi'] = pi
651:     output['sim_' + str(i) + '_mu1'] = mu1
652:     output['sim_' + str(i) + '_mu0'] = mu0
653: 
654:   with gfile.GFile(os.path.join(path, output_name), 'wt') as out:
655:     out.write(output.to_csv(index=False))"
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;286;Correctly detected;"278:   validation_metrics = []
279:   test_metrics = []
280: 
281:   with tf.Session() as sess:
282:     sess.run(init)
283:     # Training autoencoder
284:     for _ in range(num_steps_autoencoder):
285:       batch_index = random.sample(range(len(train_labels)), BATCH_SIZE)
286:       batch_x = train_features.iloc[batch_index, :].values
287:       batch_y = train_labels.iloc[batch_index].values.reshape(BATCH_SIZE, 1)
288:       _, _ = sess.run([autoencoder_optimizer, autoencoder_loss],
289:                       feed_dict={
290:                           x: batch_x,
291:                           y: batch_y,
292:                       })
293: 
294:     # GetCandidatesAlpha (Algorithm 2 in paper)"
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;287;Correctly detected;"279:   test_metrics = []
280: 
281:   with tf.Session() as sess:
282:     sess.run(init)
283:     # Training autoencoder
284:     for _ in range(num_steps_autoencoder):
285:       batch_index = random.sample(range(len(train_labels)), BATCH_SIZE)
286:       batch_x = train_features.iloc[batch_index, :].values
287:       batch_y = train_labels.iloc[batch_index].values.reshape(BATCH_SIZE, 1)
288:       _, _ = sess.run([autoencoder_optimizer, autoencoder_loss],
289:                       feed_dict={
290:                           x: batch_x,
291:                           y: batch_y,
292:                       })
293: 
294:     # GetCandidatesAlpha (Algorithm 2 in paper)
295:     for alpha_batch_index in range(NUM_ALPHA_BATCHES):"
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;332;Correctly detected;"324: 
325:           # Add candidate to the GP, assuming the metric observation is the LCB.
326:           sample_alphas = np.concatenate([sample_alphas, best_alpha])
327:           sample_validation_metrics.append(best_alpha_metric_ucb)
328: 
329:       # Training classifiers
330:       for _ in range(TRAINING_STEPS):
331:         batch_index = random.sample(range(len(train_labels)), BATCH_SIZE)
332:         batch_x = train_features.iloc[batch_index, :].values
333:         batch_y = train_labels.iloc[batch_index].values.reshape(BATCH_SIZE, 1)
334:         batch_population = train_population.iloc[batch_index].values.reshape(
335:             BATCH_SIZE, 1)
336:         batch_embedding = sess.run(
337:             autoencoder_embedding_layer, feed_dict={
338:                 x: batch_x,
339:                 y: batch_y,
340:             })"
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;333;Correctly detected;"325:           # Add candidate to the GP, assuming the metric observation is the LCB.
326:           sample_alphas = np.concatenate([sample_alphas, best_alpha])
327:           sample_validation_metrics.append(best_alpha_metric_ucb)
328: 
329:       # Training classifiers
330:       for _ in range(TRAINING_STEPS):
331:         batch_index = random.sample(range(len(train_labels)), BATCH_SIZE)
332:         batch_x = train_features.iloc[batch_index, :].values
333:         batch_y = train_labels.iloc[batch_index].values.reshape(BATCH_SIZE, 1)
334:         batch_population = train_population.iloc[batch_index].values.reshape(
335:             BATCH_SIZE, 1)
336:         batch_embedding = sess.run(
337:             autoencoder_embedding_layer, feed_dict={
338:                 x: batch_x,
339:                 y: batch_y,
340:             })
341:         _, _ = sess.run("
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;334;Correctly detected;"326:           sample_alphas = np.concatenate([sample_alphas, best_alpha])
327:           sample_validation_metrics.append(best_alpha_metric_ucb)
328: 
329:       # Training classifiers
330:       for _ in range(TRAINING_STEPS):
331:         batch_index = random.sample(range(len(train_labels)), BATCH_SIZE)
332:         batch_x = train_features.iloc[batch_index, :].values
333:         batch_y = train_labels.iloc[batch_index].values.reshape(BATCH_SIZE, 1)
334:         batch_population = train_population.iloc[batch_index].values.reshape(
335:             BATCH_SIZE, 1)
336:         batch_embedding = sess.run(
337:             autoencoder_embedding_layer, feed_dict={
338:                 x: batch_x,
339:                 y: batch_y,
340:             })
341:         _, _ = sess.run(
342:             [parallel_optimizers, parallel_losses],"
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;354;Correctly detected;"346:                 population: batch_population,
347:                 embedding: batch_embedding,
348:                 parallel_alphas: alpha_batch,
349:             })
350: 
351:       parallel_train_logits = sess.run(
352:           parallel_logits,
353:           feed_dict={
354:               x: train_features.values,
355:               y: train_labels.values.reshape(len(train_labels), 1),
356:           })
357:       alphas = np.concatenate([alphas, alpha_batch])
358:       parallel_validation_logits = sess.run(
359:           parallel_logits,
360:           feed_dict={
361:               x: validation_features.values,
362:               y: validation_labels.values.reshape(len(validation_labels), 1),"
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;355;Correctly detected;"347:                 embedding: batch_embedding,
348:                 parallel_alphas: alpha_batch,
349:             })
350: 
351:       parallel_train_logits = sess.run(
352:           parallel_logits,
353:           feed_dict={
354:               x: train_features.values,
355:               y: train_labels.values.reshape(len(train_labels), 1),
356:           })
357:       alphas = np.concatenate([alphas, alpha_batch])
358:       parallel_validation_logits = sess.run(
359:           parallel_logits,
360:           feed_dict={
361:               x: validation_features.values,
362:               y: validation_labels.values.reshape(len(validation_labels), 1),
363:           })"
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;361;Correctly detected;"353:           feed_dict={
354:               x: train_features.values,
355:               y: train_labels.values.reshape(len(train_labels), 1),
356:           })
357:       alphas = np.concatenate([alphas, alpha_batch])
358:       parallel_validation_logits = sess.run(
359:           parallel_logits,
360:           feed_dict={
361:               x: validation_features.values,
362:               y: validation_labels.values.reshape(len(validation_labels), 1),
363:           })
364:       parallel_test_logits = sess.run(
365:           parallel_logits,
366:           feed_dict={
367:               x: test_features.values,
368:               y: test_labels.values.reshape(len(test_labels), 1),
369:           })"
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;362;Correctly detected;"354:               x: train_features.values,
355:               y: train_labels.values.reshape(len(train_labels), 1),
356:           })
357:       alphas = np.concatenate([alphas, alpha_batch])
358:       parallel_validation_logits = sess.run(
359:           parallel_logits,
360:           feed_dict={
361:               x: validation_features.values,
362:               y: validation_labels.values.reshape(len(validation_labels), 1),
363:           })
364:       parallel_test_logits = sess.run(
365:           parallel_logits,
366:           feed_dict={
367:               x: test_features.values,
368:               y: test_labels.values.reshape(len(test_labels), 1),
369:           })
370:       parallel_thresholds = ["
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;367;Correctly detected;"359:           parallel_logits,
360:           feed_dict={
361:               x: validation_features.values,
362:               y: validation_labels.values.reshape(len(validation_labels), 1),
363:           })
364:       parallel_test_logits = sess.run(
365:           parallel_logits,
366:           feed_dict={
367:               x: test_features.values,
368:               y: test_labels.values.reshape(len(test_labels), 1),
369:           })
370:       parallel_thresholds = [
371:           find_threshold(train_labels, train_logits, train_wqs,
372:                          FLAGS.post_shift)
373:           for train_logits in parallel_train_logits
374:       ]
375:       logits_thresholds = zip(parallel_validation_logits, parallel_thresholds)"
R14;True_Positive;True_Positive;./files/EvalID_88/crime.py;368;Correctly detected;"360:           feed_dict={
361:               x: validation_features.values,
362:               y: validation_labels.values.reshape(len(validation_labels), 1),
363:           })
364:       parallel_test_logits = sess.run(
365:           parallel_logits,
366:           feed_dict={
367:               x: test_features.values,
368:               y: test_labels.values.reshape(len(test_labels), 1),
369:           })
370:       parallel_thresholds = [
371:           find_threshold(train_labels, train_logits, train_wqs,
372:                          FLAGS.post_shift)
373:           for train_logits in parallel_train_logits
374:       ]
375:       logits_thresholds = zip(parallel_validation_logits, parallel_thresholds)
376:       parallel_validation_metrics = ["
R14;False_Positive;True_Positive;./files/EvalID_1/remove_label_tree.py;17;Incorrectly detected (not in ground truth);"9: 
10: logging.basicConfig(level=logging.INFO, format='(%(levelname)s): %(message)s')
11: 
12: def main(args):
13:     lostconfig = config.LOSTConfig()
14:     dbm = access.DBMan(lostconfig)
15:     if args.csv_file is not None:
16:         df = pd.read_csv(args.csv_file)
17:         name = df[df['parent_leaf_id'].isnull()]['name'].values[0]
18:     elif args.name is not None:
19:         name = args.name
20:     else:
21:         logging.error(""Either a *csv_file* or a *name* for a label tree needs to be provided!"")
22:         return
23:     root_leaf = next(filter(lambda x: x.name==name, dbm.get_all_label_trees()),None)
24:     if root_leaf is None:
25:         logging.warning('LabelTree not present in database! {}'.format(args.csv_file))"
R14;False_Positive;True_Positive;./files/EvalID_56/time_series.py;154;Incorrectly detected (not in ground truth);"146: 
147: def is_between_dates(dates, start=None, end=None):
148:   """"""Return boolean indices indicating if dates occurs between start and end.""""""
149:   if start is None:
150:     start = pd.to_datetime(0)
151:   if end is None:
152:     end = pd.to_datetime(sys.maxsize)
153:   date_series = pd.Series(pd.to_datetime(dates))
154:   return date_series.between(start, end).values
155: 
156: 
157: def _count_holidays(dates, months, weeks):
158:   """"""Count number of holidays spanned in prediction windows.""""""
159:   cal = calendar()
160:   holidays = cal.holidays(start=dates.min(), end=dates.max())
161: 
162:   def count_holidays_during_month(date):"
R14;False_Positive;True_Positive;./files/EvalID_60/utils.py;84;Incorrectly detected (not in ground truth);"76:     img = np.zeros(shape[1] * shape[0], dtype=np.uint8)
77:     for lo, hi in zip(starts, ends):
78:         img[lo:hi] = 255
79:     return img.reshape((shape[1], shape[0])).T
80: 
81: def get_salt_existence():
82:     train_mask = pd.read_csv(settings.LABEL_FILE)
83:     salt_exists_dict = {}
84:     for row in train_mask.values:
85:         salt_exists_dict[row[0]] = 0 if (row[1] is np.nan or len(row[1]) < 1) else 1
86:     return salt_exists_dict
87: 
88: def generate_metadata(train_images_dir, test_images_dir, depths_filepath):
89:     depths = pd.read_csv(depths_filepath)
90:     salt_exists_dict = get_salt_existence()
91: 
92:     metadata = {}"
R14;False_Positive;True_Positive;./files/EvalID_60/utils.py;97;Incorrectly detected (not in ground truth);"89:     depths = pd.read_csv(depths_filepath)
90:     salt_exists_dict = get_salt_existence()
91: 
92:     metadata = {}
93:     for filename in tqdm(os.listdir(os.path.join(train_images_dir, 'images'))):
94:         image_filepath = os.path.join(train_images_dir, 'images', filename)
95:         mask_filepath = os.path.join(train_images_dir, 'masks', filename)
96:         image_id = filename.split('.')[0]
97:         depth = depths[depths['id'] == image_id]['z'].values[0]
98: 
99:         metadata.setdefault('file_path_image', []).append(image_filepath)
100:         metadata.setdefault('file_path_mask', []).append(mask_filepath)
101:         metadata.setdefault('is_train', []).append(1)
102:         metadata.setdefault('id', []).append(image_id)
103:         metadata.setdefault('z', []).append(depth)
104:         metadata.setdefault('salt_exists', []).append(salt_exists_dict[image_id])
105:"
R14;False_Positive;True_Positive;./files/EvalID_60/utils.py;109;Incorrectly detected (not in ground truth);"101:         metadata.setdefault('is_train', []).append(1)
102:         metadata.setdefault('id', []).append(image_id)
103:         metadata.setdefault('z', []).append(depth)
104:         metadata.setdefault('salt_exists', []).append(salt_exists_dict[image_id])
105: 
106:     for filename in tqdm(os.listdir(os.path.join(test_images_dir, 'images'))):
107:         image_filepath = os.path.join(test_images_dir, 'images', filename)
108:         image_id = filename.split('.')[0]
109:         depth = depths[depths['id'] == image_id]['z'].values[0]
110: 
111:         metadata.setdefault('file_path_image', []).append(image_filepath)
112:         metadata.setdefault('file_path_mask', []).append(None)
113:         metadata.setdefault('is_train', []).append(0)
114:         metadata.setdefault('id', []).append(image_id)
115:         metadata.setdefault('z', []).append(depth)
116:         metadata.setdefault('salt_exists', []).append(0)
117:"
R14;False_Positive;True_Positive;./files/EvalID_60/utils.py;156;Incorrectly detected (not in ground truth);"148: def get_nfold_split(ifold, nfold=10, meta_version=1):
149:     if meta_version == 2:
150:         return get_nfold_split2(ifold, nfold)
151: 
152:     meta = pd.read_csv(settings.META_FILE, na_filter=False)
153:     meta_train = meta[meta['is_train'] == 1]
154: 
155:     kf = KFold(n_splits=nfold)
156:     for i, (train_index, valid_index) in enumerate(kf.split(meta_train[settings.ID_COLUMN].values.reshape(-1))):
157:         if i == ifold:
158:             break
159:     return meta_train.iloc[train_index], meta_train.iloc[valid_index]
160: 
161: def get_nfold_split2(ifold, nfold=10):
162:     meta_train = pd.read_csv(os.path.join(settings.DATA_DIR, 'train_meta2.csv'))
163: 
164:     with open(os.path.join(settings.DATA_DIR, 'train_split.json'), 'r') as f:"
R14;False_Positive;True_Positive;./files/EvalID_60/utils.py;175;Incorrectly detected (not in ground truth);"167:     valid_index = train_splits[str(ifold)]['val_index']
168: 
169:     return meta_train.iloc[train_index], meta_train.iloc[valid_index]
170: 
171: 
172: def get_test_meta():
173:     meta = pd.read_csv(settings.META_FILE, na_filter=False)
174:     test_meta = meta[meta['is_train'] == 0]
175:     print(len(test_meta.values))
176:     return test_meta
177: 
178: if __name__ == '__main__':
179:     get_nfold_split(2)"
R14;False_Positive;True_Positive;./files/EvalID_40/stgat_data.py;111;Incorrectly detected (not in ground truth);"103:     for station_i in selected_stations:
104:         for station_j in selected_stations:
105:             if station_i == station_j:
106:                 distances.at[station_j, station_i] = 0
107:             else:
108:                 # Compute distance between stations
109:                 station_i_meta = station_metadata[station_metadata['ID'] == station_i]
110:                 station_j_meta = station_metadata[station_metadata['ID'] == station_j]
111:                 if np.isnan(station_i_meta['Latitude'].values[0]) or np.isnan(station_i_meta['Longitude'].values[0]) or np.isnan(station_j_meta['Latitude'].values[0]) or np.isnan(station_j_meta['Longitude'].values[0]):
112:                     d_ij = 0
113:                 else:
114:                     d_ij = geopy.distance.geodesic(
115:                         (station_i_meta['Latitude'].values[0], station_i_meta['Longitude'].values[0]),
116:                         (station_j_meta['Latitude'].values[0], station_j_meta['Longitude'].values[0])).m
117:                 distances.at[station_j, station_i] = d_ij
118:                 distances_std.append(d_ij)
119:     distances_std = np.std(distances_std)"
R14;False_Positive;True_Positive;./files/EvalID_40/stgat_data.py;115;Incorrectly detected (not in ground truth);"107:             else:
108:                 # Compute distance between stations
109:                 station_i_meta = station_metadata[station_metadata['ID'] == station_i]
110:                 station_j_meta = station_metadata[station_metadata['ID'] == station_j]
111:                 if np.isnan(station_i_meta['Latitude'].values[0]) or np.isnan(station_i_meta['Longitude'].values[0]) or np.isnan(station_j_meta['Latitude'].values[0]) or np.isnan(station_j_meta['Longitude'].values[0]):
112:                     d_ij = 0
113:                 else:
114:                     d_ij = geopy.distance.geodesic(
115:                         (station_i_meta['Latitude'].values[0], station_i_meta['Longitude'].values[0]),
116:                         (station_j_meta['Latitude'].values[0], station_j_meta['Longitude'].values[0])).m
117:                 distances.at[station_j, station_i] = d_ij
118:                 distances_std.append(d_ij)
119:     distances_std = np.std(distances_std)
120:     distances.head()
121:     W = pd.crosstab(station_metadata.ID, station_metadata.ID, normalize=True)
122:     epsilon = 0.1
123:     sigma = distances_std"
R14;False_Positive;True_Positive;./files/EvalID_40/stgat_data.py;116;Incorrectly detected (not in ground truth);"108:                 # Compute distance between stations
109:                 station_i_meta = station_metadata[station_metadata['ID'] == station_i]
110:                 station_j_meta = station_metadata[station_metadata['ID'] == station_j]
111:                 if np.isnan(station_i_meta['Latitude'].values[0]) or np.isnan(station_i_meta['Longitude'].values[0]) or np.isnan(station_j_meta['Latitude'].values[0]) or np.isnan(station_j_meta['Longitude'].values[0]):
112:                     d_ij = 0
113:                 else:
114:                     d_ij = geopy.distance.geodesic(
115:                         (station_i_meta['Latitude'].values[0], station_i_meta['Longitude'].values[0]),
116:                         (station_j_meta['Latitude'].values[0], station_j_meta['Longitude'].values[0])).m
117:                 distances.at[station_j, station_i] = d_ij
118:                 distances_std.append(d_ij)
119:     distances_std = np.std(distances_std)
120:     distances.head()
121:     W = pd.crosstab(station_metadata.ID, station_metadata.ID, normalize=True)
122:     epsilon = 0.1
123:     sigma = distances_std
124:     for station_i in selected_stations:"
R14;False_Positive;True_Positive;./files/EvalID_86/reader.py;67;Incorrectly detected (not in ground truth);"59:     df = pd.read_csv(filepath)
60:     df.iloc[:,1].fillna('MISSINGVALUE', inplace=True)
61: 
62:     texts_list = []
63:     for j in range(0, df.shape[0]):
64:         texts_list.append(df.iloc[j,1])
65: 
66:     classes = df.iloc[:,2:]
67:     classes_list = classes.values.tolist()
68: 
69:     return np.asarray(texts_list), np.asarray(classes_list)
70: 
71: 
72: def load_texts_and_classes_pandas_no_id(filepath):
73:     """"""
74:     Load texts and classes from a file in csv format using pandas dataframe, with format as follow:
75:"
R14;False_Positive;True_Positive;./files/EvalID_86/reader.py;97;Incorrectly detected (not in ground truth);"89:     df = pd.read_csv(filepath)
90:     df.iloc[:,1].fillna('MISSINGVALUE', inplace=True)
91: 
92:     texts_list = []
93:     for j in range(0, df.shape[0]):
94:         texts_list.append(df.iloc[j,0])
95: 
96:     classes = df.iloc[:,1:]
97:     classes_list = classes.values.tolist()
98: 
99:     return np.asarray(texts_list), np.asarray(classes_list)
100: 
101: 
102: def load_texts_pandas(filepath):
103:     """"""
104:     Load texts from a file in csv format using pandas dataframe:
105:"
R14;False_Positive;True_Positive;./files/EvalID_86/reader.py;263;Incorrectly detected (not in ground truth);"255: 
256:     texts_list = []
257:     for j in range(0, df.shape[0]):
258:         texts_list.append(df.iloc[j,1])
259: 
260:     if 'reuse' in df.columns:
261:         # we simply get the reuse boolean value for the examples
262:         datareuses = df.iloc[:,2]
263:         reuse_list = datareuses.values.tolist()
264:         reuse_list = np.asarray(reuse_list)
265:         # map boolean values to [0,1]
266:         def map_boolean(x):
267:             return [1.0,0.0] if x == 'no_reuse' else [0.0,1.0]
268:         reuse_list = np.array(list(map(map_boolean, reuse_list)))
269:         print(reuse_list)
270:         return np.asarray(texts_list), reuse_list, None, None, [""no_reuse"", ""reuse""], None, None
271:"
R14;False_Positive;True_Positive;./files/EvalID_86/reader.py;274;Incorrectly detected (not in ground truth);"266:         def map_boolean(x):
267:             return [1.0,0.0] if x == 'no_reuse' else [0.0,1.0]
268:         reuse_list = np.array(list(map(map_boolean, reuse_list)))
269:         print(reuse_list)
270:         return np.asarray(texts_list), reuse_list, None, None, [""no_reuse"", ""reuse""], None, None
271: 
272:     # otherwise we have the list of datatypes, and optionally subtypes and leaf datatypes
273:     datatypes = df.iloc[:,2]
274:     datatypes_list = datatypes.values.tolist()
275:     datatypes_list = np.asarray(datatypes_list)
276:     datatypes_list_lower = np.char.lower(datatypes_list)
277:     list_classes_datatypes = np.unique(datatypes_list_lower)
278:     datatypes_final = normalize_classes(datatypes_list_lower, list_classes_datatypes)
279: 
280:     print(df.shape, df.shape[0], df.shape[1])
281: 
282:     if df.shape[1] > 3:"
R14;False_Positive;True_Positive;./files/EvalID_86/reader.py;286;Incorrectly detected (not in ground truth);"278:     datatypes_final = normalize_classes(datatypes_list_lower, list_classes_datatypes)
279: 
280:     print(df.shape, df.shape[0], df.shape[1])
281: 
282:     if df.shape[1] > 3:
283:         # remove possible row with 'no_dataset'
284:         df = df[~df.datatype.str.contains(""no_dataset"")]
285:         datasubtypes = df.iloc[:,3]
286:         datasubtypes_list = datasubtypes.values.tolist()
287:         datasubtypes_list = np.asarray(datasubtypes_list)
288:         datasubtypes_list_lower = np.char.lower(datasubtypes_list)
289:         list_classes_datasubtypes = np.unique(datasubtypes_list_lower)
290:         datasubtypes_final = normalize_classes(datasubtypes_list_lower, list_classes_datasubtypes)
291: 
292:     '''
293:     if df.shape[1] > 4:
294:         leafdatatypes = df.iloc[:,4]"
R14;False_Positive;True_Positive;./files/EvalID_51/script_download_data.py;521;Incorrectly detected (not in ground truth);"513:     df.reset_index(drop=True, inplace=True)
514:     df.to_csv(os.path.join(data_folder, 'pems_bay.csv'))
515:     print(""Pems dataset created"")
516:     # Construct graph
517:     print(""Constructing graph"")
518:     metafile= 'd04_text_meta_2017_01_04.txt'
519:     meta = pd.read_csv(os.path.join(data_folder, metafile), delimiter='\t', index_col='ID')
520:     meta = meta.loc[ids]
521:     nodes_loc = meta.loc[:,['Latitude', 'Longitude']].values
522:     graph = construct_graph(nodes_loc)
523:     normalized_loc = nodes_loc - nodes_loc.min(axis=0)
524:     normalized_loc /= normalized_loc.max(axis=0)
525:     graph.ndata['normalized_loc'] = torch.Tensor(normalized_loc) #Used for pretty printing
526:     pickle.dump(graph, open(os.path.join(data_folder, 'graph.bin'), 'wb'))
527: 
528: def main(args):
529:     """"""Runs main download routine."
R14;False_Positive;True_Positive;./files/EvalID_91/borzoi_satg_gene_gpu_focused_ism.py;351;Incorrectly detected (not in ground truth);"343:   gene_df = gene_df.drop(columns=['Unnamed: 0'])
344: 
345:   print(""len(gene_df) = "" + str(len(gene_df)))
346: 
347:   #Truncate by maximum number of genes
348:   gene_df = gene_df.iloc[:options.max_n_genes].copy().reset_index(drop=True)
349: 
350:   #Get list of genes for tissue
351:   tissue_genes = gene_df['gene_base'].values.tolist()
352: 
353:   #print(""len(tissue_genes) = "" + str(len(tissue_genes)))
354: 
355:   #Filter transcriptome gene list
356:   gene_list = [gene for gene in gene_list if gene.split(""."")[0] in set(tissue_genes)]
357:   num_genes = len(gene_list)
358: 
359:   print(""num_genes = "" + str(num_genes))"
R14;False_Positive;True_Positive;./files/EvalID_91/borzoi_satg_gene_gpu_focused_ism.py;412;Incorrectly detected (not in ground truth);"404:       if options.aggregate_tracks is not None and options.pseudo_qtl is not None :
405: 
406:         #Load gene dataframe and select active tissue
407:         gene_df_all = pd.read_csv(options.gene_file, sep='\t')
408:         gene_df_all = gene_df_all.query(""tissue == '"" + str(scores_h5_tissue) + ""'"").copy().reset_index(drop=True)
409:         gene_df_all = gene_df_all.drop(columns=['Unnamed: 0'])
410: 
411:         #Get list of genes for active tissue
412:         tissue_genes_all = gene_df_all['gene_base'].values.tolist()
413: 
414:         #Filter transcriptome gene list
415:         gene_list_tissue = [gene for gene in gene_list_all if gene.split(""."")[0] in set(tissue_genes_all)]
416:         num_genes_tissue = len(gene_list_tissue)
417: 
418:         print("" - num_genes_tissue = "" + str(num_genes_tissue))
419: 
420:         #Get index of genes beloning to active tissue"
R14;False_Positive;True_Positive;./files/EvalID_91/borzoi_satg_gene_gpu_focused_ism.py;571;Incorrectly detected (not in ground truth);"563:             gene_ism_end = seq_len - genes_ism_start[gi] - 1
564: 
565:           # slice relevant strand targets
566:           if genes_strand[gi] == '+':
567:             gene_strand_mask = (targets_df.strand != '-') if not rev_comp else (targets_df.strand != '+')
568:           else:
569:             gene_strand_mask = (targets_df.strand != '+') if not rev_comp else (targets_df.strand != '-')
570: 
571:           gene_target = np.array(targets_df.index[gene_strand_mask].values)
572: 
573:           # broadcast to singleton batch
574:           seq_1hot = seq_1hot[None, ...]
575:           gene_slice = gene_slice[None, ...]
576:           gene_target = gene_target[None, ...]
577: 
578:           # ism computation
579:           ism = get_ism("
R14;False_Positive;True_Positive;./files/EvalID_14/LinearGaussianCPD.py;127;Incorrectly detected (not in ground truth);"119:         # Later we compute beta_0 and append it.
120:         for i in range(0, x_len):
121:             x.append(self.sum_of_product(x_df[""(Y|X)""], x_df[self.evidence[i]]))
122:             for j in range(0, x_len):
123:                 coef_matrix.loc[i, sym_coefs[j]] = self.sum_of_product(
124:                     x_df[self.evidence[i]], x_df[self.evidence[j]]
125:                 )
126: 
127:         coef_matrix.insert(0, ""b0_coef"", sum_x[self.evidence].values)
128:         row_1 = np.append([len(x_df)], sum_x[self.evidence].values)
129:         coef_matrix.loc[-1] = row_1
130:         coef_matrix.index = coef_matrix.index + 1  # shifting index
131:         coef_matrix.sort_index(inplace=True)
132: 
133:         beta_coef_matrix = np.matrix(coef_matrix.values, dtype=""float"")
134:         coef_inv = np.linalg.inv(beta_coef_matrix)
135:         beta_est = np.array(np.matmul(coef_inv, np.transpose(x)))"
R14;False_Positive;True_Positive;./files/EvalID_14/LinearGaussianCPD.py;128;Incorrectly detected (not in ground truth);"120:         for i in range(0, x_len):
121:             x.append(self.sum_of_product(x_df[""(Y|X)""], x_df[self.evidence[i]]))
122:             for j in range(0, x_len):
123:                 coef_matrix.loc[i, sym_coefs[j]] = self.sum_of_product(
124:                     x_df[self.evidence[i]], x_df[self.evidence[j]]
125:                 )
126: 
127:         coef_matrix.insert(0, ""b0_coef"", sum_x[self.evidence].values)
128:         row_1 = np.append([len(x_df)], sum_x[self.evidence].values)
129:         coef_matrix.loc[-1] = row_1
130:         coef_matrix.index = coef_matrix.index + 1  # shifting index
131:         coef_matrix.sort_index(inplace=True)
132: 
133:         beta_coef_matrix = np.matrix(coef_matrix.values, dtype=""float"")
134:         coef_inv = np.linalg.inv(beta_coef_matrix)
135:         beta_est = np.array(np.matmul(coef_inv, np.transpose(x)))
136:         self.beta = beta_est[0]"
R14;False_Positive;True_Positive;./files/EvalID_14/LinearGaussianCPD.py;133;Incorrectly detected (not in ground truth);"125:                 )
126: 
127:         coef_matrix.insert(0, ""b0_coef"", sum_x[self.evidence].values)
128:         row_1 = np.append([len(x_df)], sum_x[self.evidence].values)
129:         coef_matrix.loc[-1] = row_1
130:         coef_matrix.index = coef_matrix.index + 1  # shifting index
131:         coef_matrix.sort_index(inplace=True)
132: 
133:         beta_coef_matrix = np.matrix(coef_matrix.values, dtype=""float"")
134:         coef_inv = np.linalg.inv(beta_coef_matrix)
135:         beta_est = np.array(np.matmul(coef_inv, np.transpose(x)))
136:         self.beta = beta_est[0]
137: 
138:         sigma_est = 0
139:         x_len_df = len(x_df)
140:         for i in range(0, x_len):
141:             for j in range(0, x_len):"
R14;False_Negative;False_Negative;./files/EvalID_22/ppca.py;357;Missed detection (in ground truth but not detected);"349:   # Apply varimax rotation
350:   w_vari = Varimax(w)
351: 
352:   # Get mapping between ppcs and emotions
353:   map_df = pd.DataFrame(
354:       w_vari, index=all_emotions, columns=np.arange(len(all_emotions))).round(4)
355:   # Sort to move values to diagonal
356:   map_df = map_df[list(
357:       np.argsort(map_df.apply(lambda x: pd.Series.nonzero(x)[0]).values)[0])]
358:   f = plt.figure(figsize=(10, 6), dpi=300)
359:   sns.heatmap(
360:       map_df,
361:       center=0,
362:       cmap=sns.diverging_palette(240, 10, n=50),
363:       yticklabels=all_emotions)
364:   plt.xlabel(""Component"")
365:   plt.savefig("
R14;False_Negative;False_Negative;./files/EvalID_23/util.py;35;Missed detection (in ground truth but not detected);"27: import tensorflow as tf
28: from tensorflow.python.lib.io import file_io
29: 
30: 
31: def create_dataset(dataset, window_size = 1):
32:     data_X, data_y = [], []
33:     df = pd.DataFrame(dataset)
34:     columns = [df.shift(i) for i in reversed(range(1, window_size+1))]
35:     data_X = pd.concat(columns, axis=1).dropna().values
36:     data_y = df.shift(-window_size).dropna().values
37:     return data_X, data_y
38: 
39: def load_data(data_file_url, window_size):
40:     """"""Loads data into preprocessed (train_X, train_y, eval_X, eval_y) dataframes.
41: 
42:     Returns:
43:       A tuple (train_X, train_y, eval_X, eval_y), where train_X and eval_X are"
R14;False_Negative;False_Negative;./files/EvalID_20/wine.py;238;Missed detection (in ground truth but not detected);"230: 
231:   with tf.Session() as sess:
232:     sess.run(init)
233: 
234:     # Training autoencoder
235:     for _ in range(num_steps_autoencoder):
236:       batch_index = random.sample(range(len(train_labels)), BATCH_SIZE)
237:       batch_x = train_features.iloc[batch_index, :].values
238:       batch_y = train_labels.iloc[batch_index].values.reshape(BATCH_SIZE, 1)
239:       _, _ = sess.run([autoencoder_optimizer, autoencoder_loss],
240:                       feed_dict={
241:                           x: batch_x,
242:                           y: batch_y,
243:                       })
244: 
245:     # GetCandidatesAlpha (Algorithm 2 in paper)
246:     for theta_batch_index in range(num_theta_batches):"
R14;False_Negative;False_Negative;./files/EvalID_20/wine.py;285;Missed detection (in ground truth but not detected);"277:           # Add candidate to the GP, assuming the metric observation is the LCB.
278:           sample_thetas = np.concatenate([sample_thetas, best_theta])
279:           sample_validation_metrics.append(best_theta_metric_ucb)
280: 
281:       # Training regressors
282:       for _ in range(TRAINING_STEPS):
283:         batch_index = random.sample(range(len(train_labels)), BATCH_SIZE)
284:         batch_x = train_features.iloc[batch_index, :].values
285:         batch_y = train_labels.iloc[batch_index].values.reshape(BATCH_SIZE, 1)
286:         batch_embedding = sess.run(
287:             autoencoder_embedding_layer, feed_dict={
288:                 x: batch_x,
289:                 y: batch_y,
290:             })
291:         _, _ = sess.run(
292:             [parallel_optimizers, parallel_losses],
293:             feed_dict={"
R14;False_Negative;False_Negative;./files/EvalID_20/wine.py;304;Missed detection (in ground truth but not detected);"296:                 embedding: batch_embedding,
297:                 parallel_thetas: theta_batch,
298:             })
299: 
300:       parallel_validation_outputs = sess.run(
301:           parallel_outputs,
302:           feed_dict={
303:               x: validation_features.values,
304:               y: validation_labels.values.reshape(len(validation_labels), 1),
305:           })
306:       parallel_validation_metrics = [
307:           metric(validation_labels, validation_output, validation_price)
308:           for validation_output in parallel_validation_outputs
309:       ]
310:       thetas = np.concatenate([thetas, theta_batch])
311:       validation_metrics.extend(parallel_validation_metrics)
312:"
R14;False_Negative;False_Negative;./files/EvalID_20/wine.py;317;Missed detection (in ground truth but not detected);"309:       ]
310:       thetas = np.concatenate([thetas, theta_batch])
311:       validation_metrics.extend(parallel_validation_metrics)
312: 
313:       parallel_test_outputs = sess.run(
314:           parallel_outputs,
315:           feed_dict={
316:               x: test_features.values,
317:               y: test_labels.values.reshape(len(test_labels), 1),
318:           })
319:       parallel_test_metrics = [
320:           metric(test_labels, test_output, test_price)
321:           for test_output in parallel_test_outputs
322:       ]
323:       test_metrics.extend(parallel_test_metrics)
324: 
325:   best_observed_index = np.argmin(validation_metrics)"
R14;False_Negative;False_Negative;./files/EvalID_17/ukb.py;262;Missed detection (in ground truth but not detected);"254:     y_name = f'sim_{b}_y'
255:     t_name = f'sim_{b}_pi'
256:     mu1_name = f'sim_{b}_mu1'
257:     mu0_name = f'sim_{b}_mu0'
258:     t = clinical_sim[t_name]
259:     t = t == 1
260: 
261:     if simulate_y:
262:       mu0 = copy.deepcopy(clinical_sim[mu0_name].values.reshape(-1, 1))
263:       mu1 = copy.deepcopy(clinical_sim[mu1_name].values.reshape(-1, 1))
264:       y = clinical_sim['sim_' + str(b) + '_mu0'].values
265:       y[t] = clinical_sim['sim_' + str(b) + '_mu1'].values[t]
266:       y = scaler.fit_transform(y.reshape(-1, 1))
267:       mu0 = scaler.transform(mu0)
268:       mu1 = scaler.transform(mu1)
269:       dif = mu0 - mu1
270:       true_treatment_effect.append(dif.mean())"
R14;False_Negative;False_Negative;./files/EvalID_17/ukb.py;263;Missed detection (in ground truth but not detected);"255:     t_name = f'sim_{b}_pi'
256:     mu1_name = f'sim_{b}_mu1'
257:     mu0_name = f'sim_{b}_mu0'
258:     t = clinical_sim[t_name]
259:     t = t == 1
260: 
261:     if simulate_y:
262:       mu0 = copy.deepcopy(clinical_sim[mu0_name].values.reshape(-1, 1))
263:       mu1 = copy.deepcopy(clinical_sim[mu1_name].values.reshape(-1, 1))
264:       y = clinical_sim['sim_' + str(b) + '_mu0'].values
265:       y[t] = clinical_sim['sim_' + str(b) + '_mu1'].values[t]
266:       y = scaler.fit_transform(y.reshape(-1, 1))
267:       mu0 = scaler.transform(mu0)
268:       mu1 = scaler.transform(mu1)
269:       dif = mu0 - mu1
270:       true_treatment_effect.append(dif.mean())
271:       y = y.ravel()"
R14;False_Negative;False_Negative;./files/EvalID_17/ukb.py;264;Missed detection (in ground truth but not detected);"256:     mu1_name = f'sim_{b}_mu1'
257:     mu0_name = f'sim_{b}_mu0'
258:     t = clinical_sim[t_name]
259:     t = t == 1
260: 
261:     if simulate_y:
262:       mu0 = copy.deepcopy(clinical_sim[mu0_name].values.reshape(-1, 1))
263:       mu1 = copy.deepcopy(clinical_sim[mu1_name].values.reshape(-1, 1))
264:       y = clinical_sim['sim_' + str(b) + '_mu0'].values
265:       y[t] = clinical_sim['sim_' + str(b) + '_mu1'].values[t]
266:       y = scaler.fit_transform(y.reshape(-1, 1))
267:       mu0 = scaler.transform(mu0)
268:       mu1 = scaler.transform(mu1)
269:       dif = mu0 - mu1
270:       true_treatment_effect.append(dif.mean())
271:       y = y.ravel()
272:     else:"
R14;False_Negative;False_Negative;./files/EvalID_17/ukb.py;265;Missed detection (in ground truth but not detected);"257:     mu0_name = f'sim_{b}_mu0'
258:     t = clinical_sim[t_name]
259:     t = t == 1
260: 
261:     if simulate_y:
262:       mu0 = copy.deepcopy(clinical_sim[mu0_name].values.reshape(-1, 1))
263:       mu1 = copy.deepcopy(clinical_sim[mu1_name].values.reshape(-1, 1))
264:       y = clinical_sim['sim_' + str(b) + '_mu0'].values
265:       y[t] = clinical_sim['sim_' + str(b) + '_mu1'].values[t]
266:       y = scaler.fit_transform(y.reshape(-1, 1))
267:       mu0 = scaler.transform(mu0)
268:       mu1 = scaler.transform(mu1)
269:       dif = mu0 - mu1
270:       true_treatment_effect.append(dif.mean())
271:       y = y.ravel()
272:     else:
273:       clinical_sim = clinical_sim.dropna(subset=[var])"
R14;False_Negative;False_Negative;./files/EvalID_17/ukb.py;275;Missed detection (in ground truth but not detected);"267:       mu0 = scaler.transform(mu0)
268:       mu1 = scaler.transform(mu1)
269:       dif = mu0 - mu1
270:       true_treatment_effect.append(dif.mean())
271:       y = y.ravel()
272:     else:
273:       clinical_sim = clinical_sim.dropna(subset=[var])
274:       clinical_sim.reset_index(inplace=True, drop=True)
275:       y_ = scaler.fit_transform(clinical_sim[var].values.reshape(-1, 1))
276:       np.random.seed(b)
277:       tau = np.random.uniform(0, 5, 1)[0]
278:       y = [
279:           y_[i][0] + tau if t[i] else y_[i][0]
280:           for i in range(clinical_sim.shape[0])
281:       ]
282:       true_treatment_effect.append(tau)
283:     clinical_sim[y_name] = y"
R15;False_Negative;True_Negative;./files/EvalID_50/_m5.py;55;Missed detection (in ground truth but not detected);"47:     )
48:     sales_train_evaluation.sort_index(inplace=True)
49: 
50:     sell_prices = pd.read_csv(sell_prices_path, index_col=[""item_id"", ""store_id""])
51:     sell_prices.sort_index(inplace=True)
52: 
53:     @lru_cache(maxsize=None)
54:     def get_sell_price(item_id, store_id):
55:         return calendar.merge(
56:             sell_prices.loc[item_id, store_id], on=[""wm_yr_wk""], how=""left""
57:         ).sell_price
58: 
59:     # Build dynamic features
60:     kernel = squared_exponential_kernel(alpha=alpha)
61:     event_1 = CustomDateFeatureSet(calendar[calendar.event_name_1.notna()].date, kernel)
62:     event_2 = CustomDateFeatureSet(calendar[calendar.event_name_2.notna()].date, kernel)
63:"
R15;False_Negative;True_Negative;./files/EvalID_54/kdd2012_track2_preprocess_data.py;51;Missed detection (in ground truth but not detected);"43:     df_user = pd.read_csv(
44:         ""track2/userid_profile.txt"",
45:         sep=""\t"",
46:         header=None,
47:         names=[""user_id"", ""gender"", ""age""]
48:     )
49: 
50:     df_enriched = (
51:         df
52:             .merge(df_title, on=""title_id"")
53:             .merge(df_query, on=""query_id"")
54:             .merge(df_user, on=""user_id"", how=""left"")
55:     )
56:     df_final = preprocess_tabular_features(df_enriched)
57:     print(df_final.shape)
58:     print(df_final.head())
59:     print(df_final[""label""].value_counts())"
R15;False_Negative;True_Negative;./files/EvalID_54/kdd2012_track2_preprocess_data.py;52;Missed detection (in ground truth but not detected);"44:         ""track2/userid_profile.txt"",
45:         sep=""\t"",
46:         header=None,
47:         names=[""user_id"", ""gender"", ""age""]
48:     )
49: 
50:     df_enriched = (
51:         df
52:             .merge(df_title, on=""title_id"")
53:             .merge(df_query, on=""query_id"")
54:             .merge(df_user, on=""user_id"", how=""left"")
55:     )
56:     df_final = preprocess_tabular_features(df_enriched)
57:     print(df_final.shape)
58:     print(df_final.head())
59:     print(df_final[""label""].value_counts())
60:     df_train, df_test = train_test_split(df_final, test_size=0.1, random_state=1234, stratify=df[""label""])"
R15;False_Negative;True_Negative;./files/EvalID_54/kdd2012_track2_preprocess_data.py;53;Missed detection (in ground truth but not detected);"45:         sep=""\t"",
46:         header=None,
47:         names=[""user_id"", ""gender"", ""age""]
48:     )
49: 
50:     df_enriched = (
51:         df
52:             .merge(df_title, on=""title_id"")
53:             .merge(df_query, on=""query_id"")
54:             .merge(df_user, on=""user_id"", how=""left"")
55:     )
56:     df_final = preprocess_tabular_features(df_enriched)
57:     print(df_final.shape)
58:     print(df_final.head())
59:     print(df_final[""label""].value_counts())
60:     df_train, df_test = train_test_split(df_final, test_size=0.1, random_state=1234, stratify=df[""label""])
61:"
R15;False_Negative;True_Negative;./files/EvalID_56/time_series.py;138;Missed detection (in ground truth but not detected);"130:   features['mean'] = x.mean(axis=1)
131:   features['std'] = x.std(axis=1)
132:   features['min'] = x.min(axis=1)
133:   features['max'] = x.max(axis=1)
134:   percentiles = range(10, 100, 20)
135:   for p in percentiles:
136:     features['{}_per'.format(p)] = np.percentile(x, p, axis=1)
137:   df_features = pd.DataFrame(features, index=x.index)
138:   return df_features.merge(df, left_index=True, right_index=True)
139: 
140: 
141: def move_column_to_end(df, column_name):
142:   temp = df[column_name]
143:   df.drop(column_name, axis=1, inplace=True)
144:   df[column_name] = temp
145: 
146:"
R15;False_Negative;True_Negative;./files/EvalID_53/kaldi2json.py;91;Missed detection (in ground truth but not detected);"83:     )
84:     # add offset if needed
85:     if len(segments.offset) > len(segments.offset[segments.offset == 0.0]):
86:         logging.info(""Adding offset field."")
87:         output_names.insert(2, ""offset"")
88:     segments[""duration""] = (segments.end - segments.offset).round(decimals=3)
89: 
90:     # merge data
91:     wav_segments_text = pd.merge(
92:         pd.merge(segments, wavscp, how=""inner"", on=""wav_label""), text, how=""inner"", on=""label"",
93:     )
94: 
95:     if args.with_aux_data:
96:         # check if auxiliary data is present
97:         for name, aux_file in aux_data.items():
98:             if os.path.exists(aux_file):
99:                 logging.info(f""Adding info from '{os.path.basename(aux_file)}'."")"
R15;False_Negative;True_Negative;./files/EvalID_53/kaldi2json.py;92;Missed detection (in ground truth but not detected);"84:     # add offset if needed
85:     if len(segments.offset) > len(segments.offset[segments.offset == 0.0]):
86:         logging.info(""Adding offset field."")
87:         output_names.insert(2, ""offset"")
88:     segments[""duration""] = (segments.end - segments.offset).round(decimals=3)
89: 
90:     # merge data
91:     wav_segments_text = pd.merge(
92:         pd.merge(segments, wavscp, how=""inner"", on=""wav_label""), text, how=""inner"", on=""label"",
93:     )
94: 
95:     if args.with_aux_data:
96:         # check if auxiliary data is present
97:         for name, aux_file in aux_data.items():
98:             if os.path.exists(aux_file):
99:                 logging.info(f""Adding info from '{os.path.basename(aux_file)}'."")
100:                 wav_segments_text = pd.merge("
R15;False_Negative;True_Negative;./files/EvalID_53/kaldi2json.py;100;Missed detection (in ground truth but not detected);"92:         pd.merge(segments, wavscp, how=""inner"", on=""wav_label""), text, how=""inner"", on=""label"",
93:     )
94: 
95:     if args.with_aux_data:
96:         # check if auxiliary data is present
97:         for name, aux_file in aux_data.items():
98:             if os.path.exists(aux_file):
99:                 logging.info(f""Adding info from '{os.path.basename(aux_file)}'."")
100:                 wav_segments_text = pd.merge(
101:                     wav_segments_text,
102:                     pd.read_csv(aux_file, sep="" "", header=None, names=[""label"", name]),
103:                     how=""left"",
104:                     on=""label"",
105:                 )
106:                 output_names.append(name)
107:             else:
108:                 logging.info(f""'{os.path.basename(aux_file)}' does not exist. Skipping ..."")"
R15;False_Negative;True_Negative;./files/EvalID_55/cross_feature_stats_generator.py;199;Missed detection (in ground truth but not detected);"191:     for feat_name_x, feat_name_y in feature_crosses:
192:       feat_cross = (feat_name_x, feat_name_y)
193:       if feat_cross not in accumulator:
194:         accumulator[feat_cross] = _PartialCrossFeatureStats()
195:       df_x, df_y = (features_for_cross[feat_name_x],
196:                     features_for_cross[feat_name_y])
197:       # Join based on parent index so that we have the value pairs
198:       # corresponding to each example.
199:       merged_df = pd.merge(df_x, df_y, on='parent_index')
200:       # Update the partial cross stats.
201:       accumulator[feat_cross].update(merged_df[feat_name_x],
202:                                      merged_df[feat_name_y])
203: 
204:     return accumulator
205: 
206:   # Merge together a list of cross feature statistics.
207:   def merge_accumulators("
R15;False_Negative;True_Negative;./files/EvalID_51/script_download_data.py;360;Missed detection (in ground truth but not detected);"352:                       'weights_evaluation.csv']
353: 
354:     for file in required_files:
355:         assert os.path.exists(os.path.join(data_folder, file)), ""There are files missing from the data_folder. Please download following files from https://github.com/Mcompetitions/M5-methods""
356: 
357:     core_frame = pd.read_csv(os.path.join(data_folder, ""sales_train_evaluation.csv""))
358:     test_frame = pd.read_csv(os.path.join(data_folder, ""sales_test_evaluation.csv""))
359:     # Add 28 prediction values for final model evaluation
360:     core_frame = core_frame.merge(test_frame, on=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])
361:     del test_frame
362: 
363:     id_vars = [""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""]
364:     ts_cols = [col for col in core_frame.columns if col not in id_vars]
365: 
366:     core_frame['id'] = core_frame.item_id + '_' + core_frame.store_id
367:     prices = pd.read_csv(os.path.join(data_folder, ""sell_prices.csv""))
368:     calendar = pd.read_csv(os.path.join(data_folder, ""calendar.csv""))"
R15;False_Negative;True_Negative;./files/EvalID_51/script_download_data.py;379;Missed detection (in ground truth but not detected);"371:     calendar['d'] = [f'd_{i}' for i in range(1, calendar.shape[0]+1)]
372: 
373:     core_frame = core_frame.melt(
374:         id_vars,
375:         value_vars=ts_cols,
376:         var_name='d',
377:         value_name='items_sold'
378:     )
379:     core_frame = core_frame.merge(calendar, left_on=""d"", right_on='d')
380:     core_frame = core_frame.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='outer')
381: 
382:     # According to M5-Comperition-Guide-Final-10-March-2020:
383:     # if not available, this means that the product was not sold during the examined week.
384:     core_frame.sell_price.fillna(-1, inplace=True)
385: 
386:     core_frame['weight'] = 1.0
387:     core_frame.loc[core_frame.sell_price == -1, 'weight'] = 0"
R15;False_Negative;True_Negative;./files/EvalID_51/script_download_data.py;380;Missed detection (in ground truth but not detected);"372: 
373:     core_frame = core_frame.melt(
374:         id_vars,
375:         value_vars=ts_cols,
376:         var_name='d',
377:         value_name='items_sold'
378:     )
379:     core_frame = core_frame.merge(calendar, left_on=""d"", right_on='d')
380:     core_frame = core_frame.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='outer')
381: 
382:     # According to M5-Comperition-Guide-Final-10-March-2020:
383:     # if not available, this means that the product was not sold during the examined week.
384:     core_frame.sell_price.fillna(-1, inplace=True)
385: 
386:     core_frame['weight'] = 1.0
387:     core_frame.loc[core_frame.sell_price == -1, 'weight'] = 0
388:"
R15;False_Negative;True_Negative;./files/EvalID_57/identity_spacy_annotator.py;83;Missed detection (in ground truth but not detected);"75:       return pd.DataFrame()
76: 
77:     keys = ['text']
78:     i1 = mentions_df.set_index(keys).index
79:     i2 = token_span_matches_df.set_index(keys).index
80: 
81:     mentions_df = mentions_df[i1.isin(i2)]
82: 
83:     mentions_df = pd.merge(
84:         mentions_df,
85:         token_span_matches_df,
86:         on=['text', 'IdentityGroup', 'bytes.start', 'bytes.limit'],
87:     )
88:     return mentions_df"
R15;False_Negative;True_Negative;./files/EvalID_52/afd_profile.py;214;Missed detection (in ground truth but not detected);"206:     df_s1  = df.agg(['count', 'nunique',]).transpose().reset_index().rename(columns={""index"":""_column""})
207:     df_s1['count'] = df_s1['count'].astype('int64')
208:     df_s1['nunique'] = df_s1['nunique'].astype('int64')
209:     df_s1[""null""] = (rowcnt - df_s1[""count""]).astype('int64')
210:     df_s1[""not_null""] = rowcnt - df_s1[""null""]
211:     df_s1[""null_pct""] = df_s1[""null""] / rowcnt
212:     df_s1[""nunique_pct""] = df_s1['nunique'] / rowcnt
213:     dt = pd.DataFrame(df.dtypes).reset_index().rename(columns={""index"":""_column"", 0:""_dtype""})
214:     df_stats = pd.merge(dt, df_s1, on='_column', how='inner')
215:     df_stats = df_stats.round(4)
216:     df_stats[['_feature', '_message']] = df_stats.apply(lambda x: set_feature(x,config), axis = 1, result_type=""expand"")
217: 
218:     return df_stats, df_stats.loc[df_stats[""_feature""]==""exclude""]
219: 
220: def get_email(config, df):
221:     """""" gets the email statisitcs and performs email checks """"""
222:     message = {}"
R15;False_Negative;True_Negative;./files/EvalID_52/afd_profile.py;380;Missed detection (in ground truth but not detected);"372:     df_s1  = df.agg(['count', 'nunique',]).transpose().reset_index().rename(columns={""index"":""_column""})
373:     df_s1['count'] = df_s1['count'].astype('int64')
374:     df_s1['nunique'] = df_s1['nunique'].astype('int64')
375:     df_s1[""null""] = (rowcnt - df_s1[""count""]).astype('int64')
376:     df_s1[""not_null""] = rowcnt - df_s1[""null""]
377:     df_s1[""null_pct""] = df_s1[""null""] / rowcnt
378:     df_s1[""nunique_pct""] = df_s1['nunique'] / rowcnt
379:     dt = pd.DataFrame(df.dtypes).reset_index().rename(columns={""index"":""_column"", 0:""_dtype""})
380:     df_stats = pd.merge(dt, df_s1, on='_column', how='inner').round(4)
381: 
382:     cat_list = []
383:     for rec in df_stats.to_dict('records'):
384:         if rec['_column'] != target:
385:             cat_summary = col_stats(df, target, rec['_column'])
386:             rec['top_n'] = cat_summary[rec['_column']].tolist()
387:             rec['top_n_count'] = cat_summary['total'].tolist()
388:             rec['fraud_pct'] = cat_summary['fraud_pct'].tolist()"
R15;False_Negative;True_Negative;./files/EvalID_52/afd_profile.py;444;Missed detection (in ground truth but not detected);"436:     df_s1  = df.agg(['count', 'nunique','mean','min','max']).transpose().reset_index().rename(columns={""index"":""_column""})
437:     df_s1['count'] = df_s1['count'].astype('int64')
438:     df_s1['nunique'] = df_s1['nunique'].astype('int64')
439:     df_s1[""null""] = (rowcnt - df_s1[""count""]).astype('int64')
440:     df_s1[""not_null""] = rowcnt - df_s1[""null""]
441:     df_s1[""null_pct""] = df_s1[""null""] / rowcnt
442:     df_s1[""nunique_pct""] = df_s1['nunique'] / rowcnt
443:     dt = pd.DataFrame(df.dtypes).reset_index().rename(columns={""index"":""_column"", 0:""_dtype""})
444:     df_stats = pd.merge(dt, df_s1, on='_column', how='inner').round(4)
445:     num_list = []
446:     for rec in df_stats.to_dict('records'):
447:         if rec['_column'] != target and rec['count'] > 1:
448:             n_summary = ncol_stats(df, target, rec['_column'])
449:             rec['bin_label'] = n_summary['bin_label'].tolist()
450:             rec['legit_count'] = n_summary['legit'].tolist()
451:             rec['fraud_count'] = n_summary['fraud'].tolist()
452:             rec['total'] = n_summary['total'].tolist()"
R16;False_Negative;True_Negative;./files/EvalID_33/_transformer.py;213;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_94/main_rllim_on_real_data.py;164;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_35/aggregate_predictions.py;124;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_36/preprocess-scikit-label-split.py;63;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_36/preprocess-scikit-label-split.py;76;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_36/preprocess-scikit-label-split.py;78;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_36/preprocess-scikit-label-split.py;80;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_36/preprocess-scikit-label-split.py;84;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_36/preprocess-scikit-label-split.py;86;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_36/preprocess-scikit-label-split.py;91;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_36/preprocess-scikit-label-split.py;129;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_37/intel_xgboost_daal4py_pipeline.pyask.py;117;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_40/stgcn_data.py;87;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_40/stgcn_data.py;88;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_40/stgcn_data.py;93;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_40/stgcn_data.py;99;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_40/stgcn_data.py;100;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_40/stgcn_data.py;121;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_40/stgcn_data.py;136;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_34/evaluate.py;111;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_39/emirnnTrainer.py;637;Missed detection (in ground truth but not detected);Fichier introuvable
R16;False_Negative;True_Negative;./files/EvalID_92/stgat_data.py;86;Missed detection (in ground truth but not detected);Fichier introuvable
R17;False_Positive;True_Positive;./files/EvalID_94/corpus.py;285;Incorrectly detected (not in ground truth);"277:   elif datasets_dir:
278:     source = os.path.join(datasets_dir, language + "".tsv"")
279:   print(""Reading corpus from \""{}\"" ..."".format(source))
280:   with open(source, mode=""r"", encoding=""utf-8"") as f:
281:     data = pd.read_csv(f, sep=""\t"", header=None, dtype=str)
282:   print(""Number of original verses in file: {}"".format(data.shape[0]))
283:   table = {}
284:   nrows = 0
285:   for _, row in data.iterrows():
286:     try:
287:       verse, text = row[0], row[1]
288:       if pd.isnull(text):
289:         continue
290:       text_list = []
291:       for wp in text.split():
292:         wp = wp.split(""/"")
293:         if len(wp) == 2:"
R17;False_Positive;True_Positive;./files/EvalID_50/_m5.py;139;Incorrectly detected (not in ground truth);"131:         {""name"": ""event_1"", ""cardinality"": 1},
132:         {""name"": ""event_2"", ""cardinality"": 1},
133:         {""name"": ""snap"", ""cardinality"": 1},
134:     ]
135: 
136:     # Build training set
137:     train_file = dataset_path / ""train"" / ""data.json""
138:     train_ds = []
139:     for index, item in sales_train_validation.iterrows():
140:         id, item_id, dept_id, cat_id, store_id, state_id = index
141:         start_index = np.nonzero(item.iloc[:1913].values)[0][0]
142:         start_date = time_index[start_index]
143:         time_series = {}
144: 
145:         state_enc, store_enc, cat_enc, dept_enc, item_enc = item.iloc[1913:]
146: 
147:         time_series[""start""] = str(start_date)"
R17;False_Positive;True_Positive;./files/EvalID_50/_m5.py;206;Incorrectly detected (not in ground truth);"198:                     ""cardinality"": len(train_ds),
199:                 }
200:             )
201:         )
202: 
203:     # Build testing set
204:     test_file = dataset_path / ""test"" / ""data.json""
205:     test_ds = []
206:     for index, item in sales_train_evaluation.iterrows():
207:         id, item_id, dept_id, cat_id, store_id, state_id = index
208:         start_index = np.nonzero(item.iloc[:1941].values)[0][0]
209:         start_date = time_index[start_index]
210:         time_series = {}
211: 
212:         state_enc, store_enc, cat_enc, dept_enc, item_enc = item.iloc[1941:]
213: 
214:         time_series[""start""] = str(start_date)"
R17;False_Negative;True_Negative;./files/EvalID_79/protseq_analysis.py;166;Missed detection (in ground truth but not detected);"158: 
159:   counter = 0
160:   while r1 and counter < num_reads:
161:     seq = r1.sequence
162:     if END_SUBSEQ:
163:       index = seq.find(END_SUBSEQ)
164:       if index >= 0:
165:         seq = seq[:(index + len(END_SUBSEQ))]
166:     reads[seq] += 1
167:     r1 = next(r1_reader, False)
168:     counter += 1
169:   return reads
170: 
171: 
172: def update_dataframe_with_tags_r1(df, mers):
173:   """"""Creates a dataframe with locations of mers within DNA sequences.
174:"
R17;False_Negative;True_Negative;./files/EvalID_79/protseq_analysis.py;216;Missed detection (in ground truth but not detected);"208:   Returns:
209:     (str) A consensus DNA sequence.
210:   """"""
211:   n = len(reads[0])
212:   consensus = []
213:   for i in range(n):
214:     nuc_count = collections.Counter()
215:     for j in range(len(reads)):
216:       nuc_count[reads[j][i]] += 1
217:     consensus.append(nuc_count.most_common(1)[0][0])
218:   return """".join(consensus)
219: 
220: 
221: def simple_cluster_R1(reads, n_most_common=100, allowed_dist=1):
222:   """"""Clusters DNA sequences based on hamming distance.
223: 
224:   Args:"
R17;False_Negative;True_Negative;./files/EvalID_79/protseq_analysis.py;364;Missed detection (in ground truth but not detected);"356:           r1_counter, n_most_common=n_common, allowed_dist=dist_for_clustering)
357:       if df is None:  # Ignore if empty.
358:         continue
359:       df = update_dataframe_with_tags_r1(df, subseqs)
360:       df[""lane""] = ""L001""  # Hard code lane as it is not in fastq filename.
361:       df[""expt""] = expt
362:       df[""sub_plot_index""] = sub_plot_index
363:       top_freq = list(df[""freq""])[0]
364:       expt_freq += top_freq
365:       sub_plot_index += 1
366: 
367:     row += 1
368:     if sub_plot_index == 0:
369:       expt_freq = 0
370:     else:
371:       expt_freq /= sub_plot_index  # Average over all related fastqs (lanes).
372:       freqs.append(expt_freq)"
R17;False_Negative;True_Negative;./files/EvalID_79/protseq_analysis.py;459;Missed detection (in ground truth but not detected);"451:   """"""
452: 
453:   # Generate ordered set of subseq matches to r1 sequence.
454:   match_tups = []
455:   for mer_label in subseqs_to_track:
456:     mer = rc_component_dict[mer_label]
457:     for match in re.finditer(mer, r1):
458:       xstart = match.start()
459:       xend = xstart + len(mer)
460:       match_tups.append((xstart, xend, mer_label))
461:   match_tups.sort(reverse=True)
462: 
463:   # Create a maximal independent set that does not allow overlapping subseqs.
464:   if not allow_overlaps and len(match_tups) > 0:
465:     mer_graph = nx.Graph()
466:     mer_graph.add_nodes_from(match_tups)
467:     for i in range(len(match_tups)):"
R17;False_Negative;True_Negative;./files/EvalID_79/protseq_analysis.py;596;Missed detection (in ground truth but not detected);"588: 
589:   # Add counts to cycle -> foundation -> partner dict.
590:   foundations_at_cycle_0 = 0
591:   for foundation_str, row in foundation_pair_df.iterrows():
592:     count = row[col_to_sum]
593:     foundation_and_partners = foundation_str.split(BCS_SEP)
594:     foundation = foundation_and_partners[0]
595:     if foundation in foundation_to_targets:
596:       foundations_at_cycle_0 += count
597:       partner_start_index = 1
598:     else:
599:       foundation = EMPTY_FOUNDATION
600:       partner_start_index = 0
601:     # Only include partners up to the CYCLES_TO_CHECK.
602:     for cycle in range(partner_start_index,
603:                        min(len(foundation_and_partners), cycles_to_check + 1)):
604:       foundation_partner = foundation_and_partners[cycle]"
R17;False_Negative;True_Negative;./files/EvalID_79/protseq_analysis.py;606;Missed detection (in ground truth but not detected);"598:     else:
599:       foundation = EMPTY_FOUNDATION
600:       partner_start_index = 0
601:     # Only include partners up to the CYCLES_TO_CHECK.
602:     for cycle in range(partner_start_index,
603:                        min(len(foundation_and_partners), cycles_to_check + 1)):
604:       foundation_partner = foundation_and_partners[cycle]
605:       foundation_partner_pair_counts[cycle][foundation][
606:           foundation_partner] += count
607: 
608:   # sort the foundations and partners so that heatmaps will be easier to read.
609:   foundations.sort()
610:   foundation_partners.sort()
611: 
612:   # Convert dict of Counters to list of dataframes.
613:   cycle_dfs = []
614:   cycle_correspondence = np.zeros((cycles_to_check + 1, cycles_to_check + 1))"
R17;False_Negative;True_Negative;./files/EvalID_79/protseq_analysis.py;628;Missed detection (in ground truth but not detected);"620:       for partner in foundation_partners:
621:         count = foundation_partner_pair_counts[cycle][foundation][partner]
622:         counts.append(count)
623:         # NOTE: Until we standardize this protocol a bit more we use a hack to
624:         # extract the expected cycle number from label string.
625:         expected_cycle = partner.split(CYCLE_SEP)[CYCLE_NUM_INDEX]
626:         if expected_cycle.isdigit():
627:           expected_cycle = int(expected_cycle)
628:           cycle_correspondence[cycle][expected_cycle] += count
629:       current_cycle_dict[foundation] = counts
630:     cycle_df = pd.DataFrame.from_dict(current_cycle_dict)
631:     cycle_df.index = foundation_partners
632:     cycle_dfs.append(cycle_df)
633: 
634:   # Render heatmap.
635:   if heatmap:
636:     plt.figure(figsize=(15, 5))"
R17;False_Negative;False_Negative;./files/EvalID_79/protseq_analysis.py;661;Missed detection (in ground truth but not detected);"653: 
654: def make_plot_df(cycle_df):
655:   targets = []
656:   base_targets = []
657:   partners = []
658:   fracs = []
659:   counts = []
660:   cols = list(cycle_df.columns)[:-1]
661:   for _, row in cycle_df.iterrows():
662:     row_sum = row[cols].sum()
663:     for col in cols:
664:       partners.append(row[""binder""])
665:       targets.append(col)
666:       base_targets.append(col.split(""."")[0].split(""_"")[0])
667:       counts.append(row[col])
668:       fracs.append(row[col] / row_sum)
669:   plot_df = pd.DataFrame.from_dict({"
R17;False_Negative;True_Negative;./files/EvalID_77/DEP_xsolution.py;100;Missed detection (in ground truth but not detected);"92:             attributes=pd.DataFrame(ids_array, columns=['id'])
93:         )
94: 
95:         return self.meshes
96: 
97:     @staticmethod
98:     def extract_each_surface_representations(ids, last_idx, simplex,
99:                                              surf_ver_sim, vertex):
100:         for index, row in surf_ver_sim.iterrows():
101:             v_ = row['vertices']
102:             e_ = row['edges'] + last_idx
103:             if v_ is not np.nan and e_ is not np.nan:
104:                 i_ = np.ones(e_.shape[0]) * row['id']
105:                 vertex.append(v_)
106:                 simplex.append(e_)
107:                 ids.append(i_)
108:                 last_idx = e_[-20:].max() + 1"
R18;True_Positive;True_Positive;./files/EvalID_60/utils.py;85;Correctly detected;"77:     for lo, hi in zip(starts, ends):
78:         img[lo:hi] = 255
79:     return img.reshape((shape[1], shape[0])).T
80: 
81: def get_salt_existence():
82:     train_mask = pd.read_csv(settings.LABEL_FILE)
83:     salt_exists_dict = {}
84:     for row in train_mask.values:
85:         salt_exists_dict[row[0]] = 0 if (row[1] is np.nan or len(row[1]) < 1) else 1
86:     return salt_exists_dict
87: 
88: def generate_metadata(train_images_dir, test_images_dir, depths_filepath):
89:     depths = pd.read_csv(depths_filepath)
90:     salt_exists_dict = get_salt_existence()
91: 
92:     metadata = {}
93:     for filename in tqdm(os.listdir(os.path.join(train_images_dir, 'images'))):"
R18;True_Positive;True_Positive;./files/EvalID_59/__init__.py;298;Correctly detected;"290:         if self.action_pdtype in (None, 'default'):
291:             self.action_pdtype = policy_util.ACTION_PDS[self.action_type][0]
292:         self.ActionPD = policy_util.get_action_pd_cls(self.action_pdtype, self.action_type)
293: 
294:     def update(self, state, action, reward, next_state, done):
295:         '''Interface update method for body at agent.update()'''
296:         if hasattr(self.env.u_env, 'raw_reward'):  # use raw_reward if reward is preprocessed
297:             reward = self.env.u_env.raw_reward
298:         if self.ckpt_total_reward is np.nan:  # init
299:             self.ckpt_total_reward = reward
300:         else:  # reset on epi_start, else keep adding. generalized for vec env
301:             self.ckpt_total_reward = self.ckpt_total_reward * (1 - self.epi_start) + reward
302:         self.total_reward = done * self.ckpt_total_reward + (1 - done) * self.total_reward
303:         self.epi_start = done
304: 
305:     def __str__(self):
306:         return f'body: {util.to_json(util.get_class_attr(self))}'"
R18;True_Positive;True_Positive;./files/EvalID_58/sklearn.py;129;Correctly detected;"121:         """"""
122:         if self._Booster is None:
123:             raise XGBoostError('need to call fit beforehand')
124:         return self._Booster
125: 
126:     def get_params(self, deep=False):
127:         """"""Get parameter.s""""""
128:         params = super(XGBModel, self).get_params(deep=deep)
129:         if params['missing'] is np.nan:
130:             params['missing'] = None  # sklearn doesn't handle nan. see #4725
131:         if not params.get('eval_metric', True):
132:             del params['eval_metric']  # don't give as None param to Booster
133:         return params
134: 
135:     def get_xgb_params(self):
136:         """"""Get xgboost type parameters.""""""
137:         xgb_params = self.get_params()"
R18;False_Positive;True_Positive;./files/EvalID_77/DEP_xsolution.py;103;Incorrectly detected (not in ground truth);"95:         return self.meshes
96: 
97:     @staticmethod
98:     def extract_each_surface_representations(ids, last_idx, simplex,
99:                                              surf_ver_sim, vertex):
100:         for index, row in surf_ver_sim.iterrows():
101:             v_ = row['vertices']
102:             e_ = row['edges'] + last_idx
103:             if v_ is not np.nan and e_ is not np.nan:
104:                 i_ = np.ones(e_.shape[0]) * row['id']
105:                 vertex.append(v_)
106:                 simplex.append(e_)
107:                 ids.append(i_)
108:                 last_idx = e_[-20:].max() + 1
109: 
110:     def set_values(self,
111:                    values: list,"
R20;True_Positive;True_Positive;./files/EvalID_1/remove_label_tree.py;17;Correctly detected;"9: 
10: logging.basicConfig(level=logging.INFO, format='(%(levelname)s): %(message)s')
11: 
12: def main(args):
13:     lostconfig = config.LOSTConfig()
14:     dbm = access.DBMan(lostconfig)
15:     if args.csv_file is not None:
16:         df = pd.read_csv(args.csv_file)
17:         name = df[df['parent_leaf_id'].isnull()]['name'].values[0]
18:     elif args.name is not None:
19:         name = args.name
20:     else:
21:         logging.error(""Either a *csv_file* or a *name* for a label tree needs to be provided!"")
22:         return
23:     root_leaf = next(filter(lambda x: x.name==name, dbm.get_all_label_trees()),None)
24:     if root_leaf is None:
25:         logging.warning('LabelTree not present in database! {}'.format(args.csv_file))"
R20;True_Positive;True_Positive;./files/EvalID_8/evaluate.py;25;Correctly detected;"17:         meta_info = pd.read_csv(os.path.join(os.path.dirname(__file__), 'master.csv'), index_col=0, keep_default_na=False)
18:         if not self.name in meta_info:
19:             print(self.name)
20:             error_mssg = 'Invalid dataset name {}.\n'.format(self.name)
21:             error_mssg += 'Available datasets are as follows:\n'
22:             error_mssg += '\n'.join(meta_info.keys())
23:             raise ValueError(error_mssg)
24: 
25:         self.num_tasks = int(meta_info[self.name]['num tasks'])
26:         self.eval_metric = meta_info[self.name]['eval metric']
27: 
28: 
29:     def _parse_and_check_input(self, input_dict):
30:         if self.eval_metric == 'rocauc' or self.eval_metric == 'ap' or self.eval_metric == 'rmse' or self.eval_metric == 'acc':
31:             if not 'y_true' in input_dict:
32:                 raise RuntimeError('Missing key of y_true')
33:             if not 'y_pred' in input_dict:"
R20;True_Positive;True_Positive;./files/EvalID_8/evaluate.py;26;Correctly detected;"18:         if not self.name in meta_info:
19:             print(self.name)
20:             error_mssg = 'Invalid dataset name {}.\n'.format(self.name)
21:             error_mssg += 'Available datasets are as follows:\n'
22:             error_mssg += '\n'.join(meta_info.keys())
23:             raise ValueError(error_mssg)
24: 
25:         self.num_tasks = int(meta_info[self.name]['num tasks'])
26:         self.eval_metric = meta_info[self.name]['eval metric']
27: 
28: 
29:     def _parse_and_check_input(self, input_dict):
30:         if self.eval_metric == 'rocauc' or self.eval_metric == 'ap' or self.eval_metric == 'rmse' or self.eval_metric == 'acc':
31:             if not 'y_true' in input_dict:
32:                 raise RuntimeError('Missing key of y_true')
33:             if not 'y_pred' in input_dict:
34:                 raise RuntimeError('Missing key of y_pred')"
R20;True_Positive;True_Positive;./files/EvalID_4/result_collector.py;96;Correctly detected;"88:       c2 = analysis_constants.C + str(n)
89:       with open(saving_dir + c1 + '_' + c2 + '.json', 'r') as fp:
90:         agg_list.append(json.load(fp))
91:   metrics_df2 = pd.DataFrame(agg_list)
92: 
93:   metrics_df_sscl = pd.concat([metrics_df1, metrics_df2])
94:   metrics_df_sscl.to_csv('../results/label_metrics_df_sscl.csv', index=False)
95: 
96:   c1_c2_list = metrics_df_sscl[
97:       metrics_df_sscl['percentage_instances_left'] >= 30
98:   ][['c1', 'c2']].tolist()
99: 
100:   saving_dir = '../results/dist_dicts/sscl_'
101: 
102:   agg_list = []
103:   for c1, c2 in c1_c2_list:
104:     with open(saving_dir + c1 + '_' + c2 + '.json', 'r') as fp:"
R20;True_Positive;True_Positive;./files/EvalID_4/result_collector.py;147;Correctly detected;"139:   )
140: 
141:   x_train = preprocessing.StandardScaler().fit_transform(
142:       np.array(metrics_df[['ratio_of_means']])
143:   )
144:   clusters = KMeans(n_clusters=4, random_state=42).fit_predict(x_train)
145:   metrics_df['ratio_clusters'] = clusters
146:   cluster_dict = {
147:       i: metrics_df[metrics_df['ratio_clusters'] == i]['ratio_of_means'].mean()
148:       for i in range(4)
149:   }
150:   name_dict = zip(
151:       np.sort(cluster_dict.values()),
152:       ['Less-separated', 'Medium-separated', 'Well-separated', 'Far-separated'],
153:   )
154:   metrics_df['ratio_clusters'] = metrics_df['ratio_clusters'].apply(
155:       lambda x: name_dict[cluster_dict[x]]"
R20;True_Positive;True_Positive;./files/EvalID_4/result_collector.py;164;Correctly detected;"156:   )
157: 
158:   x_train = preprocessing.StandardScaler().fit_transform(
159:       np.array(metrics_df[['std_label_prop']])
160:   )
161:   clusters = KMeans(n_clusters=4, random_state=42).fit_predict(x_train)
162:   metrics_df['std_label_prop_clusters'] = clusters
163:   cluster_dict = {
164:       i: metrics_df[metrics_df['std_label_prop_clusters'] == i][
165:           'std_label_prop'
166:       ].mean()
167:       for i in range(4)
168:   }
169:   name_dict = zip(
170:       np.sort(cluster_dict.values()),
171:       ['Very Low', 'Low', 'Medium', 'High'],
172:   )"
R20;True_Positive;True_Positive;./files/EvalID_4/result_collector.py;194;Correctly detected;"186:               'bag_size_85_percentile',
187:               'bag_size_95_percentile',
188:           ]]
189:       )
190:   )
191:   clusters = KMeans(n_clusters=3, random_state=42).fit_predict(x_train)
192:   metrics_df_sscl['bag_size_clusters'] = clusters
193:   cluster_dict = {
194:       i: metrics_df_sscl[metrics_df_sscl['bag_size_clusters'] == i][
195:           'bag_size_85_percentile'
196:       ].mean()
197:       for i in range(3)
198:   }
199:   name_dict = zip(
200:       np.sort(cluster_dict.values()),
201:       ['Short-tailed', 'Medium-tailed', 'Long-tailed'],
202:   )"
R20;True_Positive;True_Positive;./files/EvalID_4/result_collector.py;213;Correctly detected;"205:   ].apply(lambda x: name_dict[cluster_dict[x]])
206: 
207:   x_train = preprocessing.StandardScaler().fit_transform(
208:       np.array(metrics_df_sscl[['ratio_of_means']])
209:   )
210:   clusters = KMeans(n_clusters=3, random_state=42).fit_predict(x_train)
211:   metrics_df_sscl['ratio_clusters'] = clusters
212:   cluster_dict = {
213:       i: metrics_df_sscl[metrics_df_sscl['ratio_clusters'] == i][
214:           'ratio_of_means'
215:       ].mean()
216:       for i in range(3)
217:   }
218:   name_dict = zip(
219:       np.sort(cluster_dict.values()),
220:       ['Less-separated', 'Medium-separated', 'Well-separated'],
221:   )"
R20;True_Positive;True_Positive;./files/EvalID_4/result_collector.py;232;Correctly detected;"224:   )
225: 
226:   x_train = preprocessing.StandardScaler().fit_transform(
227:       np.array(metrics_df_sscl[['std_label_prop']])
228:   )
229:   clusters = KMeans(n_clusters=3, random_state=42).fit_predict(x_train)
230:   metrics_df_sscl['std_label_prop_clusters'] = clusters
231:   cluster_dict = {
232:       i: metrics_df_sscl[metrics_df_sscl['std_label_prop_clusters'] == i][
233:           'std_label_prop'
234:       ].mean()
235:       for i in range(3)
236:   }
237:   name_dict = zip(
238:       np.sort(cluster_dict.values()),
239:       ['Low', 'Medium', 'High'],
240:   )"
R20;True_Positive;True_Positive;./files/EvalID_4/result_collector.py;360;Correctly detected;"352:               'r',
353:           ) as fp:
354:             res_dict = json.load(fp)
355:           res_dict_list.append(res_dict)
356:   feat_rand_res_df = pd.DataFrame(res_dict_list)
357: 
358:   feat_rand_auc_df = None
359:   for method in method_list:
360:     temp_df = feat_rand_res_df[feat_rand_res_df['method'] == method][
361:         ['c1', 'c2', 'bag_size', 'auc']
362:     ]
363:     if feat_rand_auc_df is None:
364:       feat_rand_auc_df = (
365:           temp_df.groupby(['c1', 'c2', 'bag_size'])
366:           .apply(lambda x: (100 * x['auc']).mean().round(2))
367:           .reset_index()
368:       )"
R20;True_Positive;True_Positive;./files/EvalID_7/speech_synthesis.py;63;Correctly detected;"55:         """"""
56:         logging.info(""Loading data from {}"".format(file_path))
57:         lines = pd.read_csv(file_path,""\t"")
58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]
66:             speaker = ""global"" if ""speaker"" not in headers else lines[""speaker""][l]
67:             self.entries.append(
68:                 tuple([wav_filename, wav_length, transcript, speaker])
69:             )
70:             if speaker not in self.speakers:
71:                 self.speakers.append(speaker)"
R20;True_Positive;True_Positive;./files/EvalID_7/speech_synthesis.py;64;Correctly detected;"56:         logging.info(""Loading data from {}"".format(file_path))
57:         lines = pd.read_csv(file_path,""\t"")
58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]
66:             speaker = ""global"" if ""speaker"" not in headers else lines[""speaker""][l]
67:             self.entries.append(
68:                 tuple([wav_filename, wav_length, transcript, speaker])
69:             )
70:             if speaker not in self.speakers:
71:                 self.speakers.append(speaker)
72:"
R20;True_Positive;True_Positive;./files/EvalID_7/speech_synthesis.py;65;Correctly detected;"57:         lines = pd.read_csv(file_path,""\t"")
58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]
66:             speaker = ""global"" if ""speaker"" not in headers else lines[""speaker""][l]
67:             self.entries.append(
68:                 tuple([wav_filename, wav_length, transcript, speaker])
69:             )
70:             if speaker not in self.speakers:
71:                 self.speakers.append(speaker)
72: 
73:         speakers_ids = list(range(len(self.speakers)))"
R20;True_Positive;True_Positive;./files/EvalID_7/speech_synthesis.py;66;Correctly detected;"58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]
66:             speaker = ""global"" if ""speaker"" not in headers else lines[""speaker""][l]
67:             self.entries.append(
68:                 tuple([wav_filename, wav_length, transcript, speaker])
69:             )
70:             if speaker not in self.speakers:
71:                 self.speakers.append(speaker)
72: 
73:         speakers_ids = list(range(len(self.speakers)))
74:         self.speakers_dict = dict(zip(self.speakers, speakers_ids))"
R20;True_Positive;True_Positive;./files/EvalID_2/clean.py;64;Correctly detected;"56:         :param centered: boolean flag that determines whether to center the input covariates.
57:         :return X, X_prime, y: pandas dataframes of attributes, sensitive attributes, labels
58:     """"""
59: 
60:     df = pd.read_csv(dataset)
61:     sens_df = pd.read_csv(attributes)
62: 
63:     ## Get and remove label Y
64:     y_col = [str(c) for c in sens_df.columns if sens_df[c][0] == 2]
65:     print('label feature: {}'.format(y_col))
66:     if (len(y_col) > 1):
67:         raise ValueError('More than 1 label column used')
68:     if (len(y_col) < 1):
69:         raise ValueError('No label column used')
70: 
71:     y = df[y_col[0]]
72:"
R20;True_Positive;True_Positive;./files/EvalID_2/clean.py;77;Correctly detected;"69:         raise ValueError('No label column used')
70: 
71:     y = df[y_col[0]]
72: 
73:     ## Do not use labels in rest of data
74:     X = df.loc[:, df.columns != y_col[0]]
75:     X = X.loc[:, X.columns != 'Unnamed: 0']
76:     ## Create X_prime, by getting protected attributes
77:     sens_cols = [str(c) for c in sens_df.columns if sens_df[c][0] == 1]
78:     print('sensitive features: {}'.format(sens_cols))
79:     sens_dict = {c: 1 if c in sens_cols else 0 for c in df.columns}
80:     X, sens_dict = one_hot_code(X, sens_dict)
81:     sens_names = [key for key in sens_dict.keys() if sens_dict[key] == 1]
82:     print(
83:         'there are {} sensitive features including derivative features'.format(
84:             len(sens_names)))
85:     X_prime = X[sens_names]"
R20;True_Positive;True_Positive;./files/EvalID_5/evaluate.py;25;Correctly detected;"17:         meta_info = pd.read_csv(os.path.join(os.path.dirname(__file__), 'master.csv'), index_col=0, keep_default_na=False)
18:         if not self.name in meta_info:
19:             print(self.name)
20:             error_mssg = 'Invalid dataset name {}.\n'.format(self.name)
21:             error_mssg += 'Available datasets are as follows:\n'
22:             error_mssg += '\n'.join(meta_info.keys())
23:             raise ValueError(error_mssg)
24: 
25:         self.eval_metric = meta_info[self.name]['eval metric']
26: 
27:         if 'hits@' in self.eval_metric:
28:             ### Hits@K
29: 
30:             self.K = int(self.eval_metric.split('@')[1])
31: 
32: 
33:     def _parse_and_check_input(self, input_dict):"
R20;True_Positive;True_Positive;./files/EvalID_3/result_collector.py;96;Correctly detected;"88:       c2 = analysis_constants.C + str(n)
89:       with open(saving_dir + c1 + '_' + c2 + '.json', 'r') as fp:
90:         agg_list.append(json.load(fp))
91:   metrics_df2 = pd.DataFrame(agg_list)
92: 
93:   metrics_df_sscl = pd.concat([metrics_df1, metrics_df2])
94:   metrics_df_sscl.to_csv('../results/label_metrics_df_sscl.csv', index=False)
95: 
96:   c1_c2_list = metrics_df_sscl[
97:       metrics_df_sscl['percentage_instances_left'] >= 30
98:   ][['c1', 'c2']].tolist()
99: 
100:   saving_dir = '../results/dist_dicts/sscl_'
101: 
102:   agg_list = []
103:   for c1, c2 in c1_c2_list:
104:     with open(saving_dir + c1 + '_' + c2 + '.json', 'r') as fp:"
R20;True_Positive;True_Positive;./files/EvalID_3/result_collector.py;128;Correctly detected;"120:               'bag_size_85_percentile',
121:               'bag_size_95_percentile',
122:           ]]
123:       )
124:   )
125:   clusters = KMeans(n_clusters=4, random_state=42).fit_predict(x_train)
126:   metrics_df['bag_size_clusters'] = clusters
127:   cluster_dict = {
128:       i: metrics_df[metrics_df['bag_size_clusters'] == i][
129:           'bag_size_85_percentile'
130:       ].mean()
131:       for i in range(4)
132:   }
133:   name_dict = zip(
134:       np.sort(cluster_dict.values()),
135:       ['Very Short-tailed', 'Short-tailed', 'Long-tailed', 'Very Long-tailed'],
136:   )"
R20;True_Positive;True_Positive;./files/EvalID_3/result_collector.py;147;Correctly detected;"139:   )
140: 
141:   x_train = preprocessing.StandardScaler().fit_transform(
142:       np.array(metrics_df[['ratio_of_means']])
143:   )
144:   clusters = KMeans(n_clusters=4, random_state=42).fit_predict(x_train)
145:   metrics_df['ratio_clusters'] = clusters
146:   cluster_dict = {
147:       i: metrics_df[metrics_df['ratio_clusters'] == i]['ratio_of_means'].mean()
148:       for i in range(4)
149:   }
150:   name_dict = zip(
151:       np.sort(cluster_dict.values()),
152:       ['Less-separated', 'Medium-separated', 'Well-separated', 'Far-separated'],
153:   )
154:   metrics_df['ratio_clusters'] = metrics_df['ratio_clusters'].apply(
155:       lambda x: name_dict[cluster_dict[x]]"
R20;True_Positive;True_Positive;./files/EvalID_3/result_collector.py;164;Correctly detected;"156:   )
157: 
158:   x_train = preprocessing.StandardScaler().fit_transform(
159:       np.array(metrics_df[['std_label_prop']])
160:   )
161:   clusters = KMeans(n_clusters=4, random_state=42).fit_predict(x_train)
162:   metrics_df['std_label_prop_clusters'] = clusters
163:   cluster_dict = {
164:       i: metrics_df[metrics_df['std_label_prop_clusters'] == i][
165:           'std_label_prop'
166:       ].mean()
167:       for i in range(4)
168:   }
169:   name_dict = zip(
170:       np.sort(cluster_dict.values()),
171:       ['Very Low', 'Low', 'Medium', 'High'],
172:   )"
R20;True_Positive;True_Positive;./files/EvalID_3/result_collector.py;194;Correctly detected;"186:               'bag_size_85_percentile',
187:               'bag_size_95_percentile',
188:           ]]
189:       )
190:   )
191:   clusters = KMeans(n_clusters=3, random_state=42).fit_predict(x_train)
192:   metrics_df_sscl['bag_size_clusters'] = clusters
193:   cluster_dict = {
194:       i: metrics_df_sscl[metrics_df_sscl['bag_size_clusters'] == i][
195:           'bag_size_85_percentile'
196:       ].mean()
197:       for i in range(3)
198:   }
199:   name_dict = zip(
200:       np.sort(cluster_dict.values()),
201:       ['Short-tailed', 'Medium-tailed', 'Long-tailed'],
202:   )"
R20;True_Positive;True_Positive;./files/EvalID_3/result_collector.py;213;Correctly detected;"205:   ].apply(lambda x: name_dict[cluster_dict[x]])
206: 
207:   x_train = preprocessing.StandardScaler().fit_transform(
208:       np.array(metrics_df_sscl[['ratio_of_means']])
209:   )
210:   clusters = KMeans(n_clusters=3, random_state=42).fit_predict(x_train)
211:   metrics_df_sscl['ratio_clusters'] = clusters
212:   cluster_dict = {
213:       i: metrics_df_sscl[metrics_df_sscl['ratio_clusters'] == i][
214:           'ratio_of_means'
215:       ].mean()
216:       for i in range(3)
217:   }
218:   name_dict = zip(
219:       np.sort(cluster_dict.values()),
220:       ['Less-separated', 'Medium-separated', 'Well-separated'],
221:   )"
R20;True_Positive;True_Positive;./files/EvalID_3/result_collector.py;232;Correctly detected;"224:   )
225: 
226:   x_train = preprocessing.StandardScaler().fit_transform(
227:       np.array(metrics_df_sscl[['std_label_prop']])
228:   )
229:   clusters = KMeans(n_clusters=3, random_state=42).fit_predict(x_train)
230:   metrics_df_sscl['std_label_prop_clusters'] = clusters
231:   cluster_dict = {
232:       i: metrics_df_sscl[metrics_df_sscl['std_label_prop_clusters'] == i][
233:           'std_label_prop'
234:       ].mean()
235:       for i in range(3)
236:   }
237:   name_dict = zip(
238:       np.sort(cluster_dict.values()),
239:       ['Low', 'Medium', 'High'],
240:   )"
R20;True_Positive;True_Positive;./files/EvalID_3/result_collector.py;360;Correctly detected;"352:               'r',
353:           ) as fp:
354:             res_dict = json.load(fp)
355:           res_dict_list.append(res_dict)
356:   feat_rand_res_df = pd.DataFrame(res_dict_list)
357: 
358:   feat_rand_auc_df = None
359:   for method in method_list:
360:     temp_df = feat_rand_res_df[feat_rand_res_df['method'] == method][
361:         ['c1', 'c2', 'bag_size', 'auc']
362:     ]
363:     if feat_rand_auc_df is None:
364:       feat_rand_auc_df = (
365:           temp_df.groupby(['c1', 'c2', 'bag_size'])
366:           .apply(lambda x: (100 * x['auc']).mean().round(2))
367:           .reset_index()
368:       )"
R20;False_Positive;True_Positive;./files/EvalID_24/data_loader.py;254;Incorrectly detected (not in ground truth);"246:   def __read_data__(self):
247:     # Standard scaler is used to scale data
248:     if self.scale:
249:       self.data_scaler = StandardScaler()
250:       self.label_scaler = StandardScaler()
251: 
252:     cols = pd.read_csv(self.cd_path, sep='\t', header=None)
253:     target_col = np.where(cols[1] == 'Label')[0][0]
254:     cat_cols = cols[cols[1] == 'Categ'][0].values
255:     train_x, train_y = self.read_file(self.train_path, target_col,
256:                                       self.header_in_data)
257:     test_x, test_y = self.read_file(self.test_path, target_col,
258:                                     self.header_in_data)
259:     # Divide the training data into trian and validation
260:     data = pd.concat([train_x, test_x])
261:     data[cat_cols] = data[cat_cols].apply(
262:         lambda x: x.astype('category').cat.codes)"
R20;False_Positive;True_Positive;./files/EvalID_60/utils.py;97;Incorrectly detected (not in ground truth);"89:     depths = pd.read_csv(depths_filepath)
90:     salt_exists_dict = get_salt_existence()
91: 
92:     metadata = {}
93:     for filename in tqdm(os.listdir(os.path.join(train_images_dir, 'images'))):
94:         image_filepath = os.path.join(train_images_dir, 'images', filename)
95:         mask_filepath = os.path.join(train_images_dir, 'masks', filename)
96:         image_id = filename.split('.')[0]
97:         depth = depths[depths['id'] == image_id]['z'].values[0]
98: 
99:         metadata.setdefault('file_path_image', []).append(image_filepath)
100:         metadata.setdefault('file_path_mask', []).append(mask_filepath)
101:         metadata.setdefault('is_train', []).append(1)
102:         metadata.setdefault('id', []).append(image_id)
103:         metadata.setdefault('z', []).append(depth)
104:         metadata.setdefault('salt_exists', []).append(salt_exists_dict[image_id])
105:"
R20;False_Positive;True_Positive;./files/EvalID_60/utils.py;109;Incorrectly detected (not in ground truth);"101:         metadata.setdefault('is_train', []).append(1)
102:         metadata.setdefault('id', []).append(image_id)
103:         metadata.setdefault('z', []).append(depth)
104:         metadata.setdefault('salt_exists', []).append(salt_exists_dict[image_id])
105: 
106:     for filename in tqdm(os.listdir(os.path.join(test_images_dir, 'images'))):
107:         image_filepath = os.path.join(test_images_dir, 'images', filename)
108:         image_id = filename.split('.')[0]
109:         depth = depths[depths['id'] == image_id]['z'].values[0]
110: 
111:         metadata.setdefault('file_path_image', []).append(image_filepath)
112:         metadata.setdefault('file_path_mask', []).append(None)
113:         metadata.setdefault('is_train', []).append(0)
114:         metadata.setdefault('id', []).append(image_id)
115:         metadata.setdefault('z', []).append(depth)
116:         metadata.setdefault('salt_exists', []).append(0)
117:"
R20;False_Positive;True_Positive;./files/EvalID_4/result_collector.py;128;Incorrectly detected (not in ground truth);"120:               'bag_size_85_percentile',
121:               'bag_size_95_percentile',
122:           ]]
123:       )
124:   )
125:   clusters = KMeans(n_clusters=4, random_state=42).fit_predict(x_train)
126:   metrics_df['bag_size_clusters'] = clusters
127:   cluster_dict = {
128:       i: metrics_df[metrics_df['bag_size_clusters'] == i][
129:           'bag_size_85_percentile'
130:       ].mean()
131:       for i in range(4)
132:   }
133:   name_dict = zip(
134:       np.sort(cluster_dict.values()),
135:       ['Very Short-tailed', 'Short-tailed', 'Long-tailed', 'Very Long-tailed'],
136:   )"
R20;False_Positive;True_Positive;./files/EvalID_91/borzoi_satg_gene_gpu_focused_ism.py;521;Incorrectly detected (not in ground truth);"513:     scores_h5.create_dataset('ism_end', data=np.array(genes_ism_end))
514:     scores_h5.create_dataset('strand', data=np.array(genes_strand, dtype='S'))
515: 
516:     # load model fold
517:     seqnn_model = seqnn.SeqNN(params_model)
518:     seqnn_model.restore(model_folder + ""/f"" + str(fold_ix) + ""c0/model0_best.h5"", 0, by_name=False)
519:     seqnn_model.build_slice(targets_df.index, False)
520: 
521:     track_scale = targets_df.iloc[0]['scale']
522:     track_transform = 3. / 4.
523: 
524:     for shift in options.shifts :
525:       print('Processing shift %d' % shift, flush=True)
526: 
527:       for rev_comp in ([False, True] if options.rc == 1 else [False]) :
528: 
529:         if options.rc == 1 :"
R20;False_Positive;True_Positive;./files/EvalID_10/speech_synthesis.py;63;Incorrectly detected (not in ground truth);"55:         """"""
56:         logging.info(""Loading data from {}"".format(file_path))
57:         lines = pd.read_csv(file_path,""\t"")
58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]
66:             speaker = ""global"" if ""speaker"" not in headers else lines[""speaker""][l]
67:             self.entries.append(
68:                 tuple([wav_filename, wav_length, transcript, speaker])
69:             )
70:             if speaker not in self.speakers:
71:                 self.speakers.append(speaker)"
R20;False_Positive;True_Positive;./files/EvalID_10/speech_synthesis.py;64;Incorrectly detected (not in ground truth);"56:         logging.info(""Loading data from {}"".format(file_path))
57:         lines = pd.read_csv(file_path,""\t"")
58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]
66:             speaker = ""global"" if ""speaker"" not in headers else lines[""speaker""][l]
67:             self.entries.append(
68:                 tuple([wav_filename, wav_length, transcript, speaker])
69:             )
70:             if speaker not in self.speakers:
71:                 self.speakers.append(speaker)
72:"
R20;False_Positive;True_Positive;./files/EvalID_10/speech_synthesis.py;65;Incorrectly detected (not in ground truth);"57:         lines = pd.read_csv(file_path,""\t"")
58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]
66:             speaker = ""global"" if ""speaker"" not in headers else lines[""speaker""][l]
67:             self.entries.append(
68:                 tuple([wav_filename, wav_length, transcript, speaker])
69:             )
70:             if speaker not in self.speakers:
71:                 self.speakers.append(speaker)
72: 
73:         speakers_ids = list(range(len(self.speakers)))"
R20;False_Positive;True_Positive;./files/EvalID_10/speech_synthesis.py;66;Incorrectly detected (not in ground truth);"58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]
66:             speaker = ""global"" if ""speaker"" not in headers else lines[""speaker""][l]
67:             self.entries.append(
68:                 tuple([wav_filename, wav_length, transcript, speaker])
69:             )
70:             if speaker not in self.speakers:
71:                 self.speakers.append(speaker)
72: 
73:         speakers_ids = list(range(len(self.speakers)))
74:         self.speakers_dict = dict(zip(self.speakers, speakers_ids))"
R20;False_Negative;True_Negative;./files/EvalID_6/report.py;114;Missed detection (in ground truth but not detected);"106:         avg_fbeta = np.zeros(len(avg_names))
107:         for i, average in enumerate(avg_names):
108:             _, _, avg_beta, _ = precision_recall_fscore_support(
109:                 y_true=y_true, y_pred=y_pred, average=average, beta=beta, labels=labels
110:             )
111:             avg_fbeta[i] = avg_beta
112:         report.insert(3, ""f-beta"", fbeta, True)
113: 
114:     metrics[""support""][""macro""] = support.sum()
115:     metrics[""precision""][""accuracy""] = accuracy
116:     if y_scores is not None:
117:         metrics[""auc""][""macro""] = roc_auc_score(
118:             y_true, y_scores, multi_class=""ovr"", average=""macro""
119:         )
120:         metrics[""auc""][""weighted""] = roc_auc_score(
121:             y_true, y_scores, multi_class=""ovr"", average=""weighted""
122:         )"
R20;False_Negative;True_Negative;./files/EvalID_6/report.py;115;Missed detection (in ground truth but not detected);"107:         for i, average in enumerate(avg_names):
108:             _, _, avg_beta, _ = precision_recall_fscore_support(
109:                 y_true=y_true, y_pred=y_pred, average=average, beta=beta, labels=labels
110:             )
111:             avg_fbeta[i] = avg_beta
112:         report.insert(3, ""f-beta"", fbeta, True)
113: 
114:     metrics[""support""][""macro""] = support.sum()
115:     metrics[""precision""][""accuracy""] = accuracy
116:     if y_scores is not None:
117:         metrics[""auc""][""macro""] = roc_auc_score(
118:             y_true, y_scores, multi_class=""ovr"", average=""macro""
119:         )
120:         metrics[""auc""][""weighted""] = roc_auc_score(
121:             y_true, y_scores, multi_class=""ovr"", average=""weighted""
122:         )
123:     metrics = pd.DataFrame(metrics, index=avg_names + [""accuracy""])"
R20;False_Negative;True_Negative;./files/EvalID_6/report.py;117;Missed detection (in ground truth but not detected);"109:                 y_true=y_true, y_pred=y_pred, average=average, beta=beta, labels=labels
110:             )
111:             avg_fbeta[i] = avg_beta
112:         report.insert(3, ""f-beta"", fbeta, True)
113: 
114:     metrics[""support""][""macro""] = support.sum()
115:     metrics[""precision""][""accuracy""] = accuracy
116:     if y_scores is not None:
117:         metrics[""auc""][""macro""] = roc_auc_score(
118:             y_true, y_scores, multi_class=""ovr"", average=""macro""
119:         )
120:         metrics[""auc""][""weighted""] = roc_auc_score(
121:             y_true, y_scores, multi_class=""ovr"", average=""weighted""
122:         )
123:     metrics = pd.DataFrame(metrics, index=avg_names + [""accuracy""])
124: 
125:     result = pd.concat((report, metrics)).fillna("""")"
R20;False_Negative;True_Negative;./files/EvalID_6/report.py;120;Missed detection (in ground truth but not detected);"112:         report.insert(3, ""f-beta"", fbeta, True)
113: 
114:     metrics[""support""][""macro""] = support.sum()
115:     metrics[""precision""][""accuracy""] = accuracy
116:     if y_scores is not None:
117:         metrics[""auc""][""macro""] = roc_auc_score(
118:             y_true, y_scores, multi_class=""ovr"", average=""macro""
119:         )
120:         metrics[""auc""][""weighted""] = roc_auc_score(
121:             y_true, y_scores, multi_class=""ovr"", average=""weighted""
122:         )
123:     metrics = pd.DataFrame(metrics, index=avg_names + [""accuracy""])
124: 
125:     result = pd.concat((report, metrics)).fillna("""")
126: 
127:     if beta:
128:         result[""f-beta""][""macro""] = avg_fbeta[0]"
R20;False_Negative;True_Negative;./files/EvalID_6/report.py;128;Missed detection (in ground truth but not detected);"120:         metrics[""auc""][""weighted""] = roc_auc_score(
121:             y_true, y_scores, multi_class=""ovr"", average=""weighted""
122:         )
123:     metrics = pd.DataFrame(metrics, index=avg_names + [""accuracy""])
124: 
125:     result = pd.concat((report, metrics)).fillna("""")
126: 
127:     if beta:
128:         result[""f-beta""][""macro""] = avg_fbeta[0]
129:         result[""f-beta""][""micro""] = avg_fbeta[1]
130:         result[""f-beta""][""weighted""] = avg_fbeta[2]
131:     return result
132: 
133: 
134: __all__ = [""get_classification_report""]"
R20;False_Negative;True_Negative;./files/EvalID_6/report.py;129;Missed detection (in ground truth but not detected);"121:             y_true, y_scores, multi_class=""ovr"", average=""weighted""
122:         )
123:     metrics = pd.DataFrame(metrics, index=avg_names + [""accuracy""])
124: 
125:     result = pd.concat((report, metrics)).fillna("""")
126: 
127:     if beta:
128:         result[""f-beta""][""macro""] = avg_fbeta[0]
129:         result[""f-beta""][""micro""] = avg_fbeta[1]
130:         result[""f-beta""][""weighted""] = avg_fbeta[2]
131:     return result
132: 
133: 
134: __all__ = [""get_classification_report""]"
R20;False_Negative;True_Negative;./files/EvalID_6/report.py;130;Missed detection (in ground truth but not detected);"122:         )
123:     metrics = pd.DataFrame(metrics, index=avg_names + [""accuracy""])
124: 
125:     result = pd.concat((report, metrics)).fillna("""")
126: 
127:     if beta:
128:         result[""f-beta""][""macro""] = avg_fbeta[0]
129:         result[""f-beta""][""micro""] = avg_fbeta[1]
130:         result[""f-beta""][""weighted""] = avg_fbeta[2]
131:     return result
132: 
133: 
134: __all__ = [""get_classification_report""]"
R20;False_Negative;False_Negative;./files/EvalID_2/clean.py;107;Missed detection (in ground truth but not detected);"99:     # have to cast ndarray to hashable type in get_baseline()
100:     x = tuple([el[0] for el in x]) if x.__class__.__name__ == 'ndarray' else x
101:     return x
102: 
103: 
104: def one_hot_code(df1, sens_dict):
105:     cols = df1.columns
106:     for c in cols:
107:         if isinstance(df1[c][0], str):
108:             column = df1[c]
109:             df1 = df1.drop(c, 1)
110:             unique_values = list(set(column))
111:             n = len(unique_values)
112:             if n > 2:
113:                 for i in range(n):
114:                     col_name = '{}.{}'.format(c, i)
115:                     col_i = ["
R21;True_Positive;True_Positive;./files/EvalID_93/utils.py;46;Correctly detected;"38:     Args:
39:         path_real: string path to csv with real data
40:         path_fake: string path to csv with real data
41:         real_sep: separator of the real csv
42:         fake_sep: separator of the fake csv
43:         drop_columns: names of columns to drop.
44:     Return: Tuple with DataFrame containing the real data and DataFrame containing the synthetic data.
45:     """"""
46:     real = pd.read_csv(path_real, sep=real_sep, low_memory=False)
47:     fake = pd.read_csv(path_fake, sep=fake_sep, low_memory=False)
48:     if set(fake.columns.tolist()).issubset(set(real.columns.tolist())):
49:         real = real[fake.columns]
50:     elif drop_columns is not None:
51:         real = real.drop(drop_columns, axis=1)
52:         try:
53:             fake = fake.drop(drop_columns, axis=1)
54:         except:"
R21;True_Positive;True_Positive;./files/EvalID_93/utils.py;47;Correctly detected;"39:         path_real: string path to csv with real data
40:         path_fake: string path to csv with real data
41:         real_sep: separator of the real csv
42:         fake_sep: separator of the fake csv
43:         drop_columns: names of columns to drop.
44:     Return: Tuple with DataFrame containing the real data and DataFrame containing the synthetic data.
45:     """"""
46:     real = pd.read_csv(path_real, sep=real_sep, low_memory=False)
47:     fake = pd.read_csv(path_fake, sep=fake_sep, low_memory=False)
48:     if set(fake.columns.tolist()).issubset(set(real.columns.tolist())):
49:         real = real[fake.columns]
50:     elif drop_columns is not None:
51:         real = real.drop(drop_columns, axis=1)
52:         try:
53:             fake = fake.drop(drop_columns, axis=1)
54:         except:
55:             print(f""Some of {drop_columns} were not found on fake.index."")"
R21;True_Positive;True_Positive;./files/EvalID_12/build_numpy_cache.py;133;Correctly detected;"125: 
126:     if not data_with_header.exists():
127:         with open(str(data_with_header), 'w') as fh:
128:             fh.write('Date,Time,Open,High,Low,Close,Volume\n')
129:             with open(str(data_path), 'r') as f:
130:                 fh.write(f.read())
131: 
132:     with monit.section(""Read data""):
133:         df = pd.read_csv(str(data_with_header))
134:     df = parse(df)
135:     with monit.section(""Filter pre-market data""):
136:         df = filter_premarket(df)
137: 
138:     with monit.section(""To numpy""):
139:         dates, packets = to_numpy(df)
140: 
141:     with monit.section(""Save""):"
R21;True_Positive;True_Positive;./files/EvalID_84/dataset.py;61;Correctly detected;"53: 
54:     return dataset_path
55: 
56: 
57: def load_dataset(ml_task_type=None, dataset_name=None):
58: 
59:     dataset_path = validate_dataset(ml_task_type, dataset_name)
60: 
61:     return pd.read_csv(dataset_path)
62: 
63: 
64: def validate_dataframe(df, mandatory_columns):
65: 
66:     columns_exist = all([col in df.columns for col in mandatory_columns])
67: 
68:     if not columns_exist:
69:         raise MissingColumns("
R21;True_Positive;True_Positive;./files/EvalID_81/dataset.py;26;Correctly detected;"18: 
19:         self.name = name ## original name, e.g., ogbg-hib
20: 
21:         if meta_dict is None:
22:             self.dir_name = '_'.join(name.split('-')) ## replace hyphen with underline, e.g., ogbg_hiv
23:             self.original_root = root
24:             self.root = osp.join(root, self.dir_name)
25: 
26:             master = pd.read_csv(os.path.join(os.path.dirname(__file__), 'master.csv'), index_col=0, keep_default_na=False)
27:             if not self.name in master:
28:                 error_mssg = 'Invalid dataset name {}.\n'.format(self.name)
29:                 error_mssg += 'Available datasets are as follows:\n'
30:                 error_mssg += '\n'.join(master.keys())
31:                 raise ValueError(error_mssg)
32:             self.meta_info = master[self.name]
33: 
34:         else:"
R21;True_Positive;True_Positive;./files/EvalID_81/dataset.py;115;Correctly detected;"107:                 additional_edge_files = self.meta_info['additional edge files'].split(',')
108: 
109:             if self.binary:
110:                 self.graphs = read_binary_graph_raw(raw_dir, add_inverse_edge = add_inverse_edge)
111:             else:
112:                 self.graphs = read_csv_graph_raw(raw_dir, add_inverse_edge = add_inverse_edge, additional_node_files = additional_node_files, additional_edge_files = additional_edge_files)
113: 
114:             if self.task_type == 'subtoken prediction':
115:                 labels_joined = pd.read_csv(osp.join(raw_dir, 'graph-label.csv.gz'), compression='gzip', header = None).values
116:                 # need to split each element into subtokens
117:                 self.labels = [str(labels_joined[i][0]).split(' ') for i in range(len(labels_joined))]
118:             else:
119:                 if self.binary:
120:                     self.labels = np.load(osp.join(raw_dir, 'graph-label.npz'))['graph_label']
121:                 else:
122:                     self.labels = pd.read_csv(osp.join(raw_dir, 'graph-label.csv.gz'), compression='gzip', header = None).values
123:"
R21;True_Positive;True_Positive;./files/EvalID_81/dataset.py;122;Correctly detected;"114:             if self.task_type == 'subtoken prediction':
115:                 labels_joined = pd.read_csv(osp.join(raw_dir, 'graph-label.csv.gz'), compression='gzip', header = None).values
116:                 # need to split each element into subtokens
117:                 self.labels = [str(labels_joined[i][0]).split(' ') for i in range(len(labels_joined))]
118:             else:
119:                 if self.binary:
120:                     self.labels = np.load(osp.join(raw_dir, 'graph-label.npz'))['graph_label']
121:                 else:
122:                     self.labels = pd.read_csv(osp.join(raw_dir, 'graph-label.csv.gz'), compression='gzip', header = None).values
123: 
124:             print('Saving...')
125:             torch.save({'graphs': self.graphs, 'labels': self.labels}, pre_processed_file_path, pickle_protocol=4)
126: 
127: 
128:     def get_idx_split(self, split_type = None):
129:         if split_type is None:
130:             split_type = self.meta_info['split']"
R21;True_Positive;True_Positive;./files/EvalID_81/dataset.py;138;Correctly detected;"130:             split_type = self.meta_info['split']
131: 
132:         path = osp.join(self.root, 'split', split_type)
133: 
134:         # short-cut if split_dict.pt exists
135:         if os.path.isfile(os.path.join(path, 'split_dict.pt')):
136:             return torch.load(os.path.join(path, 'split_dict.pt'))
137: 
138:         train_idx = pd.read_csv(osp.join(path, 'train.csv.gz'), compression='gzip', header = None).values.T[0]
139:         valid_idx = pd.read_csv(osp.join(path, 'valid.csv.gz'), compression='gzip', header = None).values.T[0]
140:         test_idx = pd.read_csv(osp.join(path, 'test.csv.gz'), compression='gzip', header = None).values.T[0]
141: 
142:         return {'train': train_idx, 'valid': valid_idx, 'test': test_idx}
143: 
144:     def __getitem__(self, idx):
145:         '''Get datapoint with index'''
146:"
R21;True_Positive;True_Positive;./files/EvalID_81/dataset.py;139;Correctly detected;"131: 
132:         path = osp.join(self.root, 'split', split_type)
133: 
134:         # short-cut if split_dict.pt exists
135:         if os.path.isfile(os.path.join(path, 'split_dict.pt')):
136:             return torch.load(os.path.join(path, 'split_dict.pt'))
137: 
138:         train_idx = pd.read_csv(osp.join(path, 'train.csv.gz'), compression='gzip', header = None).values.T[0]
139:         valid_idx = pd.read_csv(osp.join(path, 'valid.csv.gz'), compression='gzip', header = None).values.T[0]
140:         test_idx = pd.read_csv(osp.join(path, 'test.csv.gz'), compression='gzip', header = None).values.T[0]
141: 
142:         return {'train': train_idx, 'valid': valid_idx, 'test': test_idx}
143: 
144:     def __getitem__(self, idx):
145:         '''Get datapoint with index'''
146: 
147:         if isinstance(idx, (int, np.integer)):"
R21;True_Positive;True_Positive;./files/EvalID_81/dataset.py;140;Correctly detected;"132:         path = osp.join(self.root, 'split', split_type)
133: 
134:         # short-cut if split_dict.pt exists
135:         if os.path.isfile(os.path.join(path, 'split_dict.pt')):
136:             return torch.load(os.path.join(path, 'split_dict.pt'))
137: 
138:         train_idx = pd.read_csv(osp.join(path, 'train.csv.gz'), compression='gzip', header = None).values.T[0]
139:         valid_idx = pd.read_csv(osp.join(path, 'valid.csv.gz'), compression='gzip', header = None).values.T[0]
140:         test_idx = pd.read_csv(osp.join(path, 'test.csv.gz'), compression='gzip', header = None).values.T[0]
141: 
142:         return {'train': train_idx, 'valid': valid_idx, 'test': test_idx}
143: 
144:     def __getitem__(self, idx):
145:         '''Get datapoint with index'''
146: 
147:         if isinstance(idx, (int, np.integer)):
148:             return self.graphs[idx], self.labels[idx]"
R21;True_Positive;True_Positive;./files/EvalID_86/reader.py;59;Correctly detected;"51: 
52:     It should support any CSV file format.
53: 
54:     Returns:
55:         tuple(numpy array, numpy array): texts and classes
56: 
57:     """"""
58: 
59:     df = pd.read_csv(filepath)
60:     df.iloc[:,1].fillna('MISSINGVALUE', inplace=True)
61: 
62:     texts_list = []
63:     for j in range(0, df.shape[0]):
64:         texts_list.append(df.iloc[j,1])
65: 
66:     classes = df.iloc[:,2:]
67:     classes_list = classes.values.tolist()"
R21;True_Positive;True_Positive;./files/EvalID_86/reader.py;89;Correctly detected;"81: 
82:     It should support any CSV file format.
83: 
84:     Returns:
85:         tuple(numpy array, numpy array): texts and classes
86: 
87:     """"""
88: 
89:     df = pd.read_csv(filepath)
90:     df.iloc[:,1].fillna('MISSINGVALUE', inplace=True)
91: 
92:     texts_list = []
93:     for j in range(0, df.shape[0]):
94:         texts_list.append(df.iloc[j,0])
95: 
96:     classes = df.iloc[:,1:]
97:     classes_list = classes.values.tolist()"
R21;True_Positive;True_Positive;./files/EvalID_86/reader.py;119;Correctly detected;"111: 
112:     It should support any CSV file format.
113: 
114:     Returns:
115:         numpy array: texts
116: 
117:     """"""
118: 
119:     df = pd.read_csv(filepath)
120:     df.iloc[:,1].fillna('MISSINGVALUE', inplace=True)
121: 
122:     texts_list = []
123:     for j in range(0, df.shape[0]):
124:         texts_list.append(df.iloc[j,1])
125: 
126:     return np.asarray(texts_list)
127:"
R21;True_Positive;True_Positive;./files/EvalID_86/reader.py;244;Correctly detected;"236:     Classification of the datatype follows a 3-level hierarchy, so the possible 3 classes are returned.
237:     dataSubtype and leafDatatype are optional
238: 
239:     Returns:
240:         tuple(numpy array, numpy array, numpy array, numpy array):
241:             texts, datatype, datasubtype, leaf datatype
242: 
243:     """"""
244:     df = pd.read_csv(filepath)
245:     df = df[pd.notnull(df['text'])]
246:     if 'datatype' in df.columns:
247:         df = df[pd.notnull(df['datatype'])]
248:     if 'reuse' in df.columns:
249:         df = df[pd.notnull(df['reuse'])]
250:     df.iloc[:,1].fillna('NA', inplace=True)
251: 
252:     # shuffle, note that this is important for the reuse prediction, the following shuffle in place"
R21;True_Positive;True_Positive;./files/EvalID_91/borzoi_satg_gene_gpu_focused_ism.py;300;Correctly detected;"292:     params = json.load(params_open)
293:   params_model = params['model']
294:   params_train = params['train']
295:   seq_len = params_model['seq_length']
296: 
297:   if options.targets_file is None:
298:     parser.error('Must provide targets table to properly handle strands.')
299:   else:
300:     targets_df = pd.read_csv(options.targets_file, sep='\t', index_col=0)
301: 
302:   # prep strand
303:   orig_new_index = dict(zip(targets_df.index, np.arange(targets_df.shape[0])))
304:   targets_strand_pair = np.array([orig_new_index[ti] for ti in targets_df.strand_pair])
305:   targets_strand_df = targets_prep_strand(targets_df)
306:   num_targets = len(targets_strand_df)
307: 
308:   # specify relative target indices"
R21;True_Positive;True_Positive;./files/EvalID_91/borzoi_satg_gene_gpu_focused_ism.py;341;Correctly detected;"333: 
334:   #Make copy of unfiltered gene list
335:   gene_list_all = gene_list.copy()
336: 
337:   #################################################################
338:   # load tissue gene list
339: 
340:   #Load gene dataframe and select tissue
341:   gene_df = pd.read_csv(options.gene_file, sep='\t')
342:   gene_df = gene_df.query(""tissue == '"" + str(options.tissue) + ""'"").copy().reset_index(drop=True)
343:   gene_df = gene_df.drop(columns=['Unnamed: 0'])
344: 
345:   print(""len(gene_df) = "" + str(len(gene_df)))
346: 
347:   #Truncate by maximum number of genes
348:   gene_df = gene_df.iloc[:options.max_n_genes].copy().reset_index(drop=True)
349:"
R21;True_Positive;True_Positive;./files/EvalID_91/borzoi_satg_gene_gpu_focused_ism.py;407;Correctly detected;"399:       keep_index = []
400:       for gene in gene_list :
401:         keep_index.append(gene_dict[gene])
402: 
403:       #Optionally compute pseudo-counts
404:       if options.aggregate_tracks is not None and options.pseudo_qtl is not None :
405: 
406:         #Load gene dataframe and select active tissue
407:         gene_df_all = pd.read_csv(options.gene_file, sep='\t')
408:         gene_df_all = gene_df_all.query(""tissue == '"" + str(scores_h5_tissue) + ""'"").copy().reset_index(drop=True)
409:         gene_df_all = gene_df_all.drop(columns=['Unnamed: 0'])
410: 
411:         #Get list of genes for active tissue
412:         tissue_genes_all = gene_df_all['gene_base'].values.tolist()
413: 
414:         #Filter transcriptome gene list
415:         gene_list_tissue = [gene for gene in gene_list_all if gene.split(""."")[0] in set(tissue_genes_all)]"
R21;True_Positive;True_Positive;./files/EvalID_11/converter.py;32;Correctly detected;"24: FLAGS = flags.FLAGS
25: 
26: flags.DEFINE_string('input_csv_prefix', None,
27:                     'Name used during the csv generation.')
28: flags.DEFINE_enum('format', None, ['anndata'], 'Format to convert the data to.')
29: 
30: 
31: def csv_to_h5ad(csv_prefix):
32:   count = pd.read_csv(f'{csv_prefix}.counts.csv', index_col=0)
33:   metadata = pd.read_csv(f'{csv_prefix}.metadata.csv', index_col=0)
34:   featuredata = pd.read_csv(f'{csv_prefix}.featuredata.csv', index_col=0)
35:   adata = anndata.AnnData(X=count.transpose(), obs=metadata, var=featuredata)
36:   adata.write(f'{csv_prefix}.h5ad')
37: 
38: 
39: def main(unused_argv):
40:   if FLAGS.format == 'anndata':"
R21;True_Positive;True_Positive;./files/EvalID_11/converter.py;33;Correctly detected;"25: 
26: flags.DEFINE_string('input_csv_prefix', None,
27:                     'Name used during the csv generation.')
28: flags.DEFINE_enum('format', None, ['anndata'], 'Format to convert the data to.')
29: 
30: 
31: def csv_to_h5ad(csv_prefix):
32:   count = pd.read_csv(f'{csv_prefix}.counts.csv', index_col=0)
33:   metadata = pd.read_csv(f'{csv_prefix}.metadata.csv', index_col=0)
34:   featuredata = pd.read_csv(f'{csv_prefix}.featuredata.csv', index_col=0)
35:   adata = anndata.AnnData(X=count.transpose(), obs=metadata, var=featuredata)
36:   adata.write(f'{csv_prefix}.h5ad')
37: 
38: 
39: def main(unused_argv):
40:   if FLAGS.format == 'anndata':
41:     csv_to_h5ad(FLAGS.input_csv_prefix)"
R21;True_Positive;True_Positive;./files/EvalID_11/converter.py;34;Correctly detected;"26: flags.DEFINE_string('input_csv_prefix', None,
27:                     'Name used during the csv generation.')
28: flags.DEFINE_enum('format', None, ['anndata'], 'Format to convert the data to.')
29: 
30: 
31: def csv_to_h5ad(csv_prefix):
32:   count = pd.read_csv(f'{csv_prefix}.counts.csv', index_col=0)
33:   metadata = pd.read_csv(f'{csv_prefix}.metadata.csv', index_col=0)
34:   featuredata = pd.read_csv(f'{csv_prefix}.featuredata.csv', index_col=0)
35:   adata = anndata.AnnData(X=count.transpose(), obs=metadata, var=featuredata)
36:   adata.write(f'{csv_prefix}.h5ad')
37: 
38: 
39: def main(unused_argv):
40:   if FLAGS.format == 'anndata':
41:     csv_to_h5ad(FLAGS.input_csv_prefix)
42:"
R21;True_Positive;True_Positive;./files/EvalID_10/speech_synthesis.py;57;Correctly detected;"49:         self.text_featurizer = TextFeaturizer(self.hparams.text_config)
50:         if self.hparams.data_csv is not None:
51:             self.preprocess_data(self.hparams.data_csv)
52: 
53:     def preprocess_data(self, file_path):
54:         """"""generate a list of tuples (wav_filename, wav_length_ms, transcript, speaker).
55:         """"""
56:         logging.info(""Loading data from {}"".format(file_path))
57:         lines = pd.read_csv(file_path,""\t"")
58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]"
R21;False_Positive;True_Positive;./files/EvalID_19/predictor.py;73;Incorrectly detected (not in ground truth);"65:     just means one prediction per line, since there's a single column.
66:     """"""
67:     data = None
68: 
69:     # Convert from CSV to pandas
70:     if flask.request.content_type == ""text/csv"":
71:         data = flask.request.data.decode(""utf-8"")
72:         s = StringIO(data)
73:         data = pd.read_csv(s, header=None)
74:     else:
75:         return flask.Response(
76:             response=""This predictor only supports CSV data"", status=415, mimetype=""text/plain""
77:         )
78: 
79:     print(""Invoked with {} records"".format(data.shape[0]))
80: 
81:     # Do the prediction"
R21;False_Positive;True_Positive;./files/EvalID_24/data_loader.py;109;Incorrectly detected (not in ground truth);"101: 
102:   def __read_data__(self):
103: 
104:     # Standard scaler is used to scale data
105:     if self.scale:
106:       self.scaler = StandardScaler()
107:     # Loading dataset
108:     data_path = os.path.join(self.root_path, 'ECL.csv')
109:     inputs = pd.read_csv(data_path)
110:     inputs.fillna(method='ffill', inplace=True)
111:     inputs = reduce_mem_usage(inputs)
112: 
113:     # Dividing dataset into Training, Testing and Validation
114:     total_number_of_hours = inputs.shape[0]
115:     training_hours = int(inputs.shape[0] * 0.7)
116:     testing_hours = int(inputs.shape[0] * 0.2)
117:     validation_hours = total_number_of_hours - training_hours - testing_hours"
R21;False_Positive;True_Positive;./files/EvalID_24/data_loader.py;241;Incorrectly detected (not in ground truth);"233:      header_in_data: data headers
234: 
235:     Returns:
236:      x: data in the form of numpy array
237:      y: labels in the form of numpy array
238: 
239:     """"""
240: 
241:     x = pd.read_csv(file_name, sep='\t', header=0 if header_in_data else None)
242:     y = x[target_col].values
243:     x.drop(target_col, axis=1, inplace=True)
244:     return x, y
245: 
246:   def __read_data__(self):
247:     # Standard scaler is used to scale data
248:     if self.scale:
249:       self.data_scaler = StandardScaler()"
R21;False_Positive;True_Positive;./files/EvalID_24/data_loader.py;252;Incorrectly detected (not in ground truth);"244:     return x, y
245: 
246:   def __read_data__(self):
247:     # Standard scaler is used to scale data
248:     if self.scale:
249:       self.data_scaler = StandardScaler()
250:       self.label_scaler = StandardScaler()
251: 
252:     cols = pd.read_csv(self.cd_path, sep='\t', header=None)
253:     target_col = np.where(cols[1] == 'Label')[0][0]
254:     cat_cols = cols[cols[1] == 'Categ'][0].values
255:     train_x, train_y = self.read_file(self.train_path, target_col,
256:                                       self.header_in_data)
257:     test_x, test_y = self.read_file(self.test_path, target_col,
258:                                     self.header_in_data)
259:     # Divide the training data into trian and validation
260:     data = pd.concat([train_x, test_x])"
R21;False_Positive;True_Positive;./files/EvalID_50/_m5.py;34;Incorrectly detected (not in ground truth);"26:     if not os.path.exists(cal_path) or not os.path.exists(sales_path):
27:         raise RuntimeError(
28:             f""M5 data is available on Kaggle (https://www.kaggle.com/c/m5-forecasting-accuracy/data). ""
29:             f""You first need to agree to the terms of the competition before being able to download the data. ""
30:             f""After you have done that, please copy the files into {dataset_path}.""
31:         )
32: 
33:     # Read M5 data from dataset_path
34:     calendar = pd.read_csv(cal_path, parse_dates=True)
35:     calendar.sort_index(inplace=True)
36:     calendar.date = pd.to_datetime(calendar.date)
37: 
38:     sales_train_validation = pd.read_csv(
39:         sales_path,
40:         index_col=[""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""],
41:     )
42:     sales_train_validation.sort_index(inplace=True)"
R21;False_Positive;True_Positive;./files/EvalID_50/_m5.py;38;Incorrectly detected (not in ground truth);"30:             f""After you have done that, please copy the files into {dataset_path}.""
31:         )
32: 
33:     # Read M5 data from dataset_path
34:     calendar = pd.read_csv(cal_path, parse_dates=True)
35:     calendar.sort_index(inplace=True)
36:     calendar.date = pd.to_datetime(calendar.date)
37: 
38:     sales_train_validation = pd.read_csv(
39:         sales_path,
40:         index_col=[""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""],
41:     )
42:     sales_train_validation.sort_index(inplace=True)
43: 
44:     sales_train_evaluation = pd.read_csv(
45:         sales_test_path,
46:         index_col=[""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""],"
R21;False_Positive;True_Positive;./files/EvalID_50/_m5.py;44;Incorrectly detected (not in ground truth);"36:     calendar.date = pd.to_datetime(calendar.date)
37: 
38:     sales_train_validation = pd.read_csv(
39:         sales_path,
40:         index_col=[""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""],
41:     )
42:     sales_train_validation.sort_index(inplace=True)
43: 
44:     sales_train_evaluation = pd.read_csv(
45:         sales_test_path,
46:         index_col=[""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""],
47:     )
48:     sales_train_evaluation.sort_index(inplace=True)
49: 
50:     sell_prices = pd.read_csv(sell_prices_path, index_col=[""item_id"", ""store_id""])
51:     sell_prices.sort_index(inplace=True)
52:"
R21;False_Positive;True_Positive;./files/EvalID_50/_m5.py;50;Incorrectly detected (not in ground truth);"42:     sales_train_validation.sort_index(inplace=True)
43: 
44:     sales_train_evaluation = pd.read_csv(
45:         sales_test_path,
46:         index_col=[""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""],
47:     )
48:     sales_train_evaluation.sort_index(inplace=True)
49: 
50:     sell_prices = pd.read_csv(sell_prices_path, index_col=[""item_id"", ""store_id""])
51:     sell_prices.sort_index(inplace=True)
52: 
53:     @lru_cache(maxsize=None)
54:     def get_sell_price(item_id, store_id):
55:         return calendar.merge(
56:             sell_prices.loc[item_id, store_id], on=[""wm_yr_wk""], how=""left""
57:         ).sell_price
58:"
R21;False_Positive;True_Positive;./files/EvalID_22/ppca.py;240;Incorrectly detected (not in ground truth);"232: 
233: 
234: def main(_):
235:   print(""Loading data..."")
236:   dfs = []
237:   for filename in os.listdir(FLAGS.data):
238:     if filename.endswith("".csv""):
239:       dfs.append(
240:           pd.read_csv(os.path.join(FLAGS.data, filename), encoding=""utf-8""))
241:   data = pd.concat(dfs)
242:   print(""%d Examples"" % (len(set(data[""id""]))))
243:   print(""%d Annotations"" % len(data))
244: 
245:   if not os.path.isdir(FLAGS.plot_dir):
246:     os.makedirs(FLAGS.plot_dir)
247: 
248:   with open(FLAGS.emotion_file, ""r"") as f:"
R21;False_Positive;True_Positive;./files/EvalID_1/remove_label_tree.py;16;Incorrectly detected (not in ground truth);"8: import pandas as pd
9: 
10: logging.basicConfig(level=logging.INFO, format='(%(levelname)s): %(message)s')
11: 
12: def main(args):
13:     lostconfig = config.LOSTConfig()
14:     dbm = access.DBMan(lostconfig)
15:     if args.csv_file is not None:
16:         df = pd.read_csv(args.csv_file)
17:         name = df[df['parent_leaf_id'].isnull()]['name'].values[0]
18:     elif args.name is not None:
19:         name = args.name
20:     else:
21:         logging.error(""Either a *csv_file* or a *name* for a label tree needs to be provided!"")
22:         return
23:     root_leaf = next(filter(lambda x: x.name==name, dbm.get_all_label_trees()),None)
24:     if root_leaf is None:"
R21;False_Positive;True_Positive;./files/EvalID_8/evaluate.py;17;Incorrectly detected (not in ground truth);"9: except ImportError:
10:     torch = None
11: 
12: ### Evaluator for graph classification
13: class Evaluator:
14:     def __init__(self, name):
15:         self.name = name
16: 
17:         meta_info = pd.read_csv(os.path.join(os.path.dirname(__file__), 'master.csv'), index_col=0, keep_default_na=False)
18:         if not self.name in meta_info:
19:             print(self.name)
20:             error_mssg = 'Invalid dataset name {}.\n'.format(self.name)
21:             error_mssg += 'Available datasets are as follows:\n'
22:             error_mssg += '\n'.join(meta_info.keys())
23:             raise ValueError(error_mssg)
24: 
25:         self.num_tasks = int(meta_info[self.name]['num tasks'])"
R21;False_Positive;True_Positive;./files/EvalID_60/utils.py;82;Incorrectly detected (not in ground truth);"74:     starts -= 1
75:     ends = starts + lengths
76:     img = np.zeros(shape[1] * shape[0], dtype=np.uint8)
77:     for lo, hi in zip(starts, ends):
78:         img[lo:hi] = 255
79:     return img.reshape((shape[1], shape[0])).T
80: 
81: def get_salt_existence():
82:     train_mask = pd.read_csv(settings.LABEL_FILE)
83:     salt_exists_dict = {}
84:     for row in train_mask.values:
85:         salt_exists_dict[row[0]] = 0 if (row[1] is np.nan or len(row[1]) < 1) else 1
86:     return salt_exists_dict
87: 
88: def generate_metadata(train_images_dir, test_images_dir, depths_filepath):
89:     depths = pd.read_csv(depths_filepath)
90:     salt_exists_dict = get_salt_existence()"
R21;False_Positive;True_Positive;./files/EvalID_60/utils.py;89;Incorrectly detected (not in ground truth);"81: def get_salt_existence():
82:     train_mask = pd.read_csv(settings.LABEL_FILE)
83:     salt_exists_dict = {}
84:     for row in train_mask.values:
85:         salt_exists_dict[row[0]] = 0 if (row[1] is np.nan or len(row[1]) < 1) else 1
86:     return salt_exists_dict
87: 
88: def generate_metadata(train_images_dir, test_images_dir, depths_filepath):
89:     depths = pd.read_csv(depths_filepath)
90:     salt_exists_dict = get_salt_existence()
91: 
92:     metadata = {}
93:     for filename in tqdm(os.listdir(os.path.join(train_images_dir, 'images'))):
94:         image_filepath = os.path.join(train_images_dir, 'images', filename)
95:         mask_filepath = os.path.join(train_images_dir, 'masks', filename)
96:         image_id = filename.split('.')[0]
97:         depth = depths[depths['id'] == image_id]['z'].values[0]"
R21;False_Positive;True_Positive;./files/EvalID_60/utils.py;152;Incorrectly detected (not in ground truth);"144:     left = horizontal - right
145:     return (top, right, bottom, left)
146: 
147: 
148: def get_nfold_split(ifold, nfold=10, meta_version=1):
149:     if meta_version == 2:
150:         return get_nfold_split2(ifold, nfold)
151: 
152:     meta = pd.read_csv(settings.META_FILE, na_filter=False)
153:     meta_train = meta[meta['is_train'] == 1]
154: 
155:     kf = KFold(n_splits=nfold)
156:     for i, (train_index, valid_index) in enumerate(kf.split(meta_train[settings.ID_COLUMN].values.reshape(-1))):
157:         if i == ifold:
158:             break
159:     return meta_train.iloc[train_index], meta_train.iloc[valid_index]
160:"
R21;False_Positive;True_Positive;./files/EvalID_60/utils.py;162;Incorrectly detected (not in ground truth);"154: 
155:     kf = KFold(n_splits=nfold)
156:     for i, (train_index, valid_index) in enumerate(kf.split(meta_train[settings.ID_COLUMN].values.reshape(-1))):
157:         if i == ifold:
158:             break
159:     return meta_train.iloc[train_index], meta_train.iloc[valid_index]
160: 
161: def get_nfold_split2(ifold, nfold=10):
162:     meta_train = pd.read_csv(os.path.join(settings.DATA_DIR, 'train_meta2.csv'))
163: 
164:     with open(os.path.join(settings.DATA_DIR, 'train_split.json'), 'r') as f:
165:         train_splits = json.load(f)
166:     train_index = train_splits[str(ifold)]['train_index']
167:     valid_index = train_splits[str(ifold)]['val_index']
168: 
169:     return meta_train.iloc[train_index], meta_train.iloc[valid_index]
170:"
R21;False_Positive;True_Positive;./files/EvalID_60/utils.py;173;Incorrectly detected (not in ground truth);"165:         train_splits = json.load(f)
166:     train_index = train_splits[str(ifold)]['train_index']
167:     valid_index = train_splits[str(ifold)]['val_index']
168: 
169:     return meta_train.iloc[train_index], meta_train.iloc[valid_index]
170: 
171: 
172: def get_test_meta():
173:     meta = pd.read_csv(settings.META_FILE, na_filter=False)
174:     test_meta = meta[meta['is_train'] == 0]
175:     print(len(test_meta.values))
176:     return test_meta
177: 
178: if __name__ == '__main__':
179:     get_nfold_split(2)"
R21;False_Positive;True_Positive;./files/EvalID_23/util.py;52;Incorrectly detected (not in ground truth);"44:       Pandas dataframes with features for training and train_y and eval_y are
45:       numpy arrays with the corresponding labels.
46:     """"""
47:     # The % of data we should use for training
48:     TRAINING_SPLIT = 0.8
49: 
50:     # Download CSV and import into Pandas DataFrame
51:     file_stream = file_io.FileIO(data_file_url, mode='r')
52:     df = pd.read_csv(StringIO(file_stream.read()))
53:     df.index = df[df.columns[0]]
54:     df = df[['count']]
55: 
56:     scaler = StandardScaler()
57: 
58:     # Time series: split latest data into test set
59:     train = df.values[:int(TRAINING_SPLIT * len(df)), :]
60:     print(train)"
R21;False_Positive;True_Positive;./files/EvalID_40/stgat_data.py;88;Incorrectly detected (not in ground truth);"80:             station_file_content = station_file_content[
81:                 station_file_content['station'].isin(selected_stations)]
82:             # Append to dataset
83:             station_data = pd.concat([station_data, station_file_content])
84:     # Drop the 11 rows with missing values
85:     station_data = station_data.dropna(subset=['timestamp', outcome_var])
86:     station_data.head()
87:     station_data.shape
88:     station_metadata = pd.read_table(PeMS_metadata)
89:     station_metadata = station_metadata[['ID', 'Latitude', 'Longitude']]
90:     # Filter for selected stations
91:     station_metadata = station_metadata[station_metadata['ID'].isin(selected_stations)]
92:     station_metadata.head()
93:     # Keep only the required columns (time interval, station ID and the outcome variable)
94:     station_data = station_data[['timestamp', 'station', outcome_var]]
95:     station_data[outcome_var] = pd.to_numeric(station_data[outcome_var])
96:     # Reshape the dataset and aggregate the traffic speeds in each time interval"
R21;False_Positive;True_Positive;./files/EvalID_40/stgat_data.py;145;Incorrectly detected (not in ground truth);"137:     V = V.fillna(V.mean())
138:     V.to_csv(os.path.join(output_dir, 'V_{}.csv'.format(num_nodes)), index=True)
139:     W.to_csv(os.path.join(output_dir, 'W_{}.csv'.format(num_nodes)), index=False)
140:     station_metadata.to_csv(os.path.join(output_dir, 'station_meta_{}.csv'.format(num_nodes)), index=False)
141: 
142: 
143: def read_stgat_data(folder, num_nodes):
144:     device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")
145:     W = pd.read_csv(osp.join(folder, ""W_{}.csv"".format(num_nodes)))
146:     T_V = pd.read_csv(osp.join(folder, ""V_{}.csv"".format(num_nodes)))
147:     V = T_V.drop('timestamp',axis=1)
148:     num_samples, num_nodes = V.shape
149:     scaler = StandardScaler()
150: 
151: 
152:     # format graph for pyg layer inputs
153:     G = sp.coo_matrix(W)"
R21;False_Positive;True_Positive;./files/EvalID_40/stgat_data.py;146;Incorrectly detected (not in ground truth);"138:     V.to_csv(os.path.join(output_dir, 'V_{}.csv'.format(num_nodes)), index=True)
139:     W.to_csv(os.path.join(output_dir, 'W_{}.csv'.format(num_nodes)), index=False)
140:     station_metadata.to_csv(os.path.join(output_dir, 'station_meta_{}.csv'.format(num_nodes)), index=False)
141: 
142: 
143: def read_stgat_data(folder, num_nodes):
144:     device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")
145:     W = pd.read_csv(osp.join(folder, ""W_{}.csv"".format(num_nodes)))
146:     T_V = pd.read_csv(osp.join(folder, ""V_{}.csv"".format(num_nodes)))
147:     V = T_V.drop('timestamp',axis=1)
148:     num_samples, num_nodes = V.shape
149:     scaler = StandardScaler()
150: 
151: 
152:     # format graph for pyg layer inputs
153:     G = sp.coo_matrix(W)
154:     edge_index = torch.tensor(np.array([G.row, G.col]), dtype=torch.int64).to(device)"
R21;False_Positive;True_Positive;./files/EvalID_20/wine.py;153;Incorrectly detected (not in ground truth);"145: 
146: def main(_):
147:   num_parallel_thetas = FLAGS.num_parallel_thetas
148:   num_theta_batches = FLAGS.num_theta_batches
149:   num_steps_autoencoder = 0 if FLAGS.uniform_weights else TRAINING_STEPS
150: 
151:   input_dim = len(FEATURES)
152: 
153:   training_df = pd.read_csv(FLAGS.training_data_path, header=0, sep=',')
154:   testing_df = pd.read_csv(FLAGS.testing_data_path, header=0, sep=',')
155:   validation_df = pd.read_csv(FLAGS.validation_data_path, header=0, sep=',')
156: 
157:   add_price_quantiles(training_df)
158:   add_price_quantiles(testing_df)
159:   add_price_quantiles(validation_df)
160: 
161:   train_labels = np.log(training_df['price'])"
R21;False_Positive;True_Positive;./files/EvalID_20/wine.py;154;Incorrectly detected (not in ground truth);"146: def main(_):
147:   num_parallel_thetas = FLAGS.num_parallel_thetas
148:   num_theta_batches = FLAGS.num_theta_batches
149:   num_steps_autoencoder = 0 if FLAGS.uniform_weights else TRAINING_STEPS
150: 
151:   input_dim = len(FEATURES)
152: 
153:   training_df = pd.read_csv(FLAGS.training_data_path, header=0, sep=',')
154:   testing_df = pd.read_csv(FLAGS.testing_data_path, header=0, sep=',')
155:   validation_df = pd.read_csv(FLAGS.validation_data_path, header=0, sep=',')
156: 
157:   add_price_quantiles(training_df)
158:   add_price_quantiles(testing_df)
159:   add_price_quantiles(validation_df)
160: 
161:   train_labels = np.log(training_df['price'])
162:   validation_labels = np.log(validation_df['price'])"
R21;False_Positive;True_Positive;./files/EvalID_20/wine.py;155;Incorrectly detected (not in ground truth);"147:   num_parallel_thetas = FLAGS.num_parallel_thetas
148:   num_theta_batches = FLAGS.num_theta_batches
149:   num_steps_autoencoder = 0 if FLAGS.uniform_weights else TRAINING_STEPS
150: 
151:   input_dim = len(FEATURES)
152: 
153:   training_df = pd.read_csv(FLAGS.training_data_path, header=0, sep=',')
154:   testing_df = pd.read_csv(FLAGS.testing_data_path, header=0, sep=',')
155:   validation_df = pd.read_csv(FLAGS.validation_data_path, header=0, sep=',')
156: 
157:   add_price_quantiles(training_df)
158:   add_price_quantiles(testing_df)
159:   add_price_quantiles(validation_df)
160: 
161:   train_labels = np.log(training_df['price'])
162:   validation_labels = np.log(validation_df['price'])
163:   test_labels = np.log(testing_df['price'])"
R21;False_Positive;True_Positive;./files/EvalID_53/kaldi2json.py;58;Incorrectly detected (not in ground truth);"50:     output_names = list(required_data.keys())
51: 
52:     # check if required files exist
53:     for name, file in required_data.items():
54:         if not os.path.exists(file):
55:             raise ValueError(f""{os.path.basename(file)} is not in {kaldi_folder}."")
56: 
57:     # read wav.scp
58:     wavscp = pd.read_csv(required_data[""audio_filepath""], sep="" "", header=None)
59:     if wavscp.shape[1] > 2:
60:         logging.warning(
61:             f""""""More than two columns in 'wav.scp': {wavscp.shape[1]}.
62:             Maybe it contains pipes? Pipe processing can be slow at runtime.""""""
63:         )
64:         wavscp = pd.read_csv(
65:             required_data[""audio_filepath""],
66:             sep=""^([^ ]+) "","
R21;False_Positive;True_Positive;./files/EvalID_51/script_download_data.py;122;Incorrectly detected (not in ground truth);"114: 
115:     csv_path = os.path.join(data_folder, ""LD2011_2014.txt"")
116:     zip_path = csv_path + "".zip""
117: 
118:     download_and_unzip(url, zip_path, csv_path, data_folder)
119: 
120:     print(""Aggregating to hourly data"")
121: 
122:     df = pd.read_csv(csv_path, index_col=0, sep="";"", decimal="","")
123:     df.index = pd.to_datetime(df.index)
124:     df.sort_index(inplace=True)
125: 
126:     # Used to determine the start and end dates of a series
127:     output = df.resample(""1h"").mean().replace(0.0, np.nan)
128: 
129:     earliest_time = output.index.min()
130:     # Filter to match range used by other academic papers"
R21;False_Positive;True_Positive;./files/EvalID_51/script_download_data.py;357;Incorrectly detected (not in ground truth);"349:     """"""
350:     required_files = ['sales_train_evaluation.csv', 'sales_test_evaluation.csv',
351:                       'sell_prices.csv', 'calendar.csv', 'weights_validation.csv',
352:                       'weights_evaluation.csv']
353: 
354:     for file in required_files:
355:         assert os.path.exists(os.path.join(data_folder, file)), ""There are files missing from the data_folder. Please download following files from https://github.com/Mcompetitions/M5-methods""
356: 
357:     core_frame = pd.read_csv(os.path.join(data_folder, ""sales_train_evaluation.csv""))
358:     test_frame = pd.read_csv(os.path.join(data_folder, ""sales_test_evaluation.csv""))
359:     # Add 28 prediction values for final model evaluation
360:     core_frame = core_frame.merge(test_frame, on=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])
361:     del test_frame
362: 
363:     id_vars = [""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""]
364:     ts_cols = [col for col in core_frame.columns if col not in id_vars]
365:"
R21;False_Positive;True_Positive;./files/EvalID_51/script_download_data.py;358;Incorrectly detected (not in ground truth);"350:     required_files = ['sales_train_evaluation.csv', 'sales_test_evaluation.csv',
351:                       'sell_prices.csv', 'calendar.csv', 'weights_validation.csv',
352:                       'weights_evaluation.csv']
353: 
354:     for file in required_files:
355:         assert os.path.exists(os.path.join(data_folder, file)), ""There are files missing from the data_folder. Please download following files from https://github.com/Mcompetitions/M5-methods""
356: 
357:     core_frame = pd.read_csv(os.path.join(data_folder, ""sales_train_evaluation.csv""))
358:     test_frame = pd.read_csv(os.path.join(data_folder, ""sales_test_evaluation.csv""))
359:     # Add 28 prediction values for final model evaluation
360:     core_frame = core_frame.merge(test_frame, on=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])
361:     del test_frame
362: 
363:     id_vars = [""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""]
364:     ts_cols = [col for col in core_frame.columns if col not in id_vars]
365: 
366:     core_frame['id'] = core_frame.item_id + '_' + core_frame.store_id"
R21;False_Positive;True_Positive;./files/EvalID_51/script_download_data.py;367;Incorrectly detected (not in ground truth);"359:     # Add 28 prediction values for final model evaluation
360:     core_frame = core_frame.merge(test_frame, on=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])
361:     del test_frame
362: 
363:     id_vars = [""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""]
364:     ts_cols = [col for col in core_frame.columns if col not in id_vars]
365: 
366:     core_frame['id'] = core_frame.item_id + '_' + core_frame.store_id
367:     prices = pd.read_csv(os.path.join(data_folder, ""sell_prices.csv""))
368:     calendar = pd.read_csv(os.path.join(data_folder, ""calendar.csv""))
369: 
370:     calendar = calendar.sort_values('date')
371:     calendar['d'] = [f'd_{i}' for i in range(1, calendar.shape[0]+1)]
372: 
373:     core_frame = core_frame.melt(
374:         id_vars,
375:         value_vars=ts_cols,"
R21;False_Positive;True_Positive;./files/EvalID_51/script_download_data.py;368;Incorrectly detected (not in ground truth);"360:     core_frame = core_frame.merge(test_frame, on=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])
361:     del test_frame
362: 
363:     id_vars = [""id"", ""item_id"", ""dept_id"", ""cat_id"", ""store_id"", ""state_id""]
364:     ts_cols = [col for col in core_frame.columns if col not in id_vars]
365: 
366:     core_frame['id'] = core_frame.item_id + '_' + core_frame.store_id
367:     prices = pd.read_csv(os.path.join(data_folder, ""sell_prices.csv""))
368:     calendar = pd.read_csv(os.path.join(data_folder, ""calendar.csv""))
369: 
370:     calendar = calendar.sort_values('date')
371:     calendar['d'] = [f'd_{i}' for i in range(1, calendar.shape[0]+1)]
372: 
373:     core_frame = core_frame.melt(
374:         id_vars,
375:         value_vars=ts_cols,
376:         var_name='d',"
R21;False_Positive;True_Positive;./files/EvalID_51/script_download_data.py;429;Incorrectly detected (not in ground truth);"421:                                Please download following files from https://pems.dot.ca.gov/?dnode=Clearinghouse
422:                                {missing}""""""
423: 
424:         fnames = [os.path.join(data_folder, f) for f in fnames]
425:         return fnames
426: 
427: 
428:     def load_single_day(path, header, ids=None):
429:         df = pd.read_csv(path, header=None)
430:         df = df.rename(columns = lambda i: header[i])
431:         df.drop(columns=[c for c in df.columns if 'Lane' in c] + ['District'], inplace=True)
432:         if ids:
433:             df = df[df['Station'].isin(ids)]
434:         df['Timestamp'] = pd.to_datetime(df['Timestamp'])
435: 
436:         # Identify gaps in timelines
437:         num_gaps = 0"
R21;False_Positive;True_Positive;./files/EvalID_51/script_download_data.py;519;Incorrectly detected (not in ground truth);"511:     df = pd.concat(dfs)
512:     df['id'] = df['Station']
513:     df.reset_index(drop=True, inplace=True)
514:     df.to_csv(os.path.join(data_folder, 'pems_bay.csv'))
515:     print(""Pems dataset created"")
516:     # Construct graph
517:     print(""Constructing graph"")
518:     metafile= 'd04_text_meta_2017_01_04.txt'
519:     meta = pd.read_csv(os.path.join(data_folder, metafile), delimiter='\t', index_col='ID')
520:     meta = meta.loc[ids]
521:     nodes_loc = meta.loc[:,['Latitude', 'Longitude']].values
522:     graph = construct_graph(nodes_loc)
523:     normalized_loc = nodes_loc - nodes_loc.min(axis=0)
524:     normalized_loc /= normalized_loc.max(axis=0)
525:     graph.ndata['normalized_loc'] = torch.Tensor(normalized_loc) #Used for pretty printing
526:     pickle.dump(graph, open(os.path.join(data_folder, 'graph.bin'), 'wb'))
527:"
R21;False_Positive;True_Positive;./files/EvalID_52/afd_profile.py;29;Incorrectly detected (not in ground truth);"21:     """""" convert json config file into a python dict  """"""
22:     with open(config, 'r') as f:
23:         config_dict = json.load(f)[0]
24:     return config_dict
25: 
26: # -- load data --
27: def get_dataframe(config):
28:     """""" load csv into python dataframe """"""
29:     df = pd.read_csv(config['input_file'], low_memory=False)
30:     return df
31: 
32: # --
33: def get_overview(config, df):
34:     """""" return details of the dataframe and any issues found  """"""
35:     overview_msg = {}
36:     df = df.copy()
37:     column_cnt = len(df.columns)"
R21;False_Positive;True_Positive;./files/EvalID_39/gen_tstatistic.py;151;Incorrectly detected (not in ground truth);"143:     A pandas dataframe with all imported records.
144:   """"""
145: 
146:   filenames = tf.gfile.Glob(data_directory + '/' + '*.csv')
147: 
148:   df = []
149:   for filename in filenames:
150:     with tf.gfile.Open(filename) as f:
151:       df_ = pd.read_csv(f)
152:       df.append(df_)
153: 
154:   df_ = pd.concat(df, ignore_index=False)
155:   return df_
156: 
157: 
158: def main(argv):
159:   del argv  # Unused."
R21;False_Positive;True_Positive;./files/EvalID_7/speech_synthesis.py;57;Incorrectly detected (not in ground truth);"49:         self.text_featurizer = TextFeaturizer(self.hparams.text_config)
50:         if self.hparams.data_csv is not None:
51:             self.preprocess_data(self.hparams.data_csv)
52: 
53:     def preprocess_data(self, file_path):
54:         """"""generate a list of tuples (wav_filename, wav_length_ms, transcript, speaker).
55:         """"""
56:         logging.info(""Loading data from {}"".format(file_path))
57:         lines = pd.read_csv(file_path,""\t"")
58:         headers = lines.keys()
59:         lines_num = lines.shape[0]
60:         self.entries = []
61:         self.speakers = []
62:         for l in range(lines_num):
63:             wav_filename = lines[""wav_filename""][l]
64:             wav_length = lines[""wav_length_ms""][l]
65:             transcript = lines[""transcript""][l]"
R21;False_Positive;True_Positive;./files/EvalID_2/clean.py;60;Incorrectly detected (not in ground truth);"52:     and the label should be 0/1
53:         :param attributes: Filename for the attributes of the dataset. The file should have each column name in a list,
54:          and under this list should have 0 for an unprotected attribute, 1 for a protected attribute, and 2 for the
55:           attribute of the label.
56:         :param centered: boolean flag that determines whether to center the input covariates.
57:         :return X, X_prime, y: pandas dataframes of attributes, sensitive attributes, labels
58:     """"""
59: 
60:     df = pd.read_csv(dataset)
61:     sens_df = pd.read_csv(attributes)
62: 
63:     ## Get and remove label Y
64:     y_col = [str(c) for c in sens_df.columns if sens_df[c][0] == 2]
65:     print('label feature: {}'.format(y_col))
66:     if (len(y_col) > 1):
67:         raise ValueError('More than 1 label column used')
68:     if (len(y_col) < 1):"
R21;False_Positive;True_Positive;./files/EvalID_2/clean.py;61;Incorrectly detected (not in ground truth);"53:         :param attributes: Filename for the attributes of the dataset. The file should have each column name in a list,
54:          and under this list should have 0 for an unprotected attribute, 1 for a protected attribute, and 2 for the
55:           attribute of the label.
56:         :param centered: boolean flag that determines whether to center the input covariates.
57:         :return X, X_prime, y: pandas dataframes of attributes, sensitive attributes, labels
58:     """"""
59: 
60:     df = pd.read_csv(dataset)
61:     sens_df = pd.read_csv(attributes)
62: 
63:     ## Get and remove label Y
64:     y_col = [str(c) for c in sens_df.columns if sens_df[c][0] == 2]
65:     print('label feature: {}'.format(y_col))
66:     if (len(y_col) > 1):
67:         raise ValueError('More than 1 label column used')
68:     if (len(y_col) < 1):
69:         raise ValueError('No label column used')"
R21;False_Positive;True_Positive;./files/EvalID_2/clean.py;153;Incorrectly detected (not in ground truth);"145: 
146: 
147: def get_data(dataset):
148:     # Helper for main method
149:     """"""Given name of dataset, load in the three datasets associated from the clean.py file
150:     :param dataset:
151:     :return:
152:     """"""
153:     X = pd.read_csv('dataset/' + dataset + '_features.csv')
154:     X_prime = pd.read_csv('dataset/' + dataset + '_protectedfeatures.csv')
155:     y = pd.read_csv('dataset/' + dataset + '_labels.csv',
156:                     names=['index', 'label'])
157:     y = y['label']
158:     return X, X_prime, y"
R21;False_Positive;True_Positive;./files/EvalID_2/clean.py;154;Incorrectly detected (not in ground truth);"146: 
147: def get_data(dataset):
148:     # Helper for main method
149:     """"""Given name of dataset, load in the three datasets associated from the clean.py file
150:     :param dataset:
151:     :return:
152:     """"""
153:     X = pd.read_csv('dataset/' + dataset + '_features.csv')
154:     X_prime = pd.read_csv('dataset/' + dataset + '_protectedfeatures.csv')
155:     y = pd.read_csv('dataset/' + dataset + '_labels.csv',
156:                     names=['index', 'label'])
157:     y = y['label']
158:     return X, X_prime, y"
R21;False_Positive;True_Positive;./files/EvalID_92/visualize.py;18;Incorrectly detected (not in ground truth);"10:     import matplotlib.pyplot as plt
11:     import os
12:     image_path = os.path.join(bucket_name, commit_sha, 'visualization.png')
13:     image_url = os.path.join('https://storage.googleapis.com', bucket_name.lstrip('gs://'), commit_sha, 'visualization.png')
14:     html_path = os.path.join(bucket_name, 'kaggle.html')
15:     # ouptut visualization to a file
16: 
17:     import pandas as pd
18:     df_train = pd.read_csv(train_file_path)
19:     sns.set()
20:     cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']
21:     sns.pairplot(df_train[cols], size = 3)
22:     plt.savefig('visualization.png')
23:     from tensorflow.python.lib.io import file_io
24:     file_io.copy('visualization.png', image_path)
25:     rendered_template = """"""
26:     <html>"
R21;False_Positive;True_Positive;./files/EvalID_17/ukb.py;518;Incorrectly detected (not in ground truth);"510:                                                       id_to_encoded_images)
511:       _write_tf_examples(tf_examples, output_path)
512:       _print_status('completed successfully', tfrecord_path, output_path)
513:     except ValueError:
514:       _print_status('FAILED', tfrecord_path, output_path)
515:       return tfrecord_path
516: 
517:   with gfile.GFile(path_simulations) as f:
518:     features = pd.read_csv(f)
519: 
520:   if path_input == path_output:
521:     raise ValueError('Input and Output path should be different!')
522: 
523:   # Global variable inside this function
524:   label_records = {
525:       _to_basename(record['image_id']): record
526:       for record in features.to_dict('records')"
R21;False_Positive;True_Positive;./files/EvalID_17/ukb.py;638;Incorrectly detected (not in ground truth);"630:     scaler = MinMaxScaler()
631:     scaler.fit(full.reshape(-1, 1))
632:     mu1 = scaler.transform(mu1.reshape(-1, 1))
633:     mu0 = scaler.transform(mu0.reshape(-1, 1))
634:     return mu1, mu0
635: 
636:   if load_features:
637:     with gfile.GFile(os.path.join(path, hx_name), mode='rt') as f:
638:       features = pd.read_csv(f)
639: 
640:   features_only = features.drop(['image_id'], axis=1)
641:   output = pd.DataFrame(features['image_id'])
642: 
643:   if extract_id:
644:     eid = [_eid_from_image_id(item) for item in features['image_id']]
645:     output['eid'] = eid
646:   for i in range(b):"
R21;False_Positive;True_Positive;./files/EvalID_5/evaluate.py;17;Incorrectly detected (not in ground truth);"9: except ImportError:
10:     torch = None
11: 
12: ### Evaluator for link property prediction
13: class Evaluator:
14:     def __init__(self, name):
15:         self.name = name
16: 
17:         meta_info = pd.read_csv(os.path.join(os.path.dirname(__file__), 'master.csv'), index_col=0, keep_default_na=False)
18:         if not self.name in meta_info:
19:             print(self.name)
20:             error_mssg = 'Invalid dataset name {}.\n'.format(self.name)
21:             error_mssg += 'Available datasets are as follows:\n'
22:             error_mssg += '\n'.join(meta_info.keys())
23:             raise ValueError(error_mssg)
24: 
25:         self.eval_metric = meta_info[self.name]['eval metric']"
R21;False_Positive;True_Positive;./files/EvalID_88/crime.py;204;Incorrectly detected (not in ground truth);"196:   scales = np.power(np.random.uniform(size=(count, 1)), 1 / dim)
197:   points *= scales * sampling_radius
198:   return points
199: 
200: 
201: def main(_):
202:   num_steps_autoencoder = 0 if FLAGS.uniform_weights else TRAINING_STEPS
203: 
204:   training_df = pd.read_csv(FLAGS.training_data_path, header=0, sep=',')
205:   testing_df = pd.read_csv(FLAGS.testing_data_path, header=0, sep=',')
206:   validation_df = pd.read_csv(FLAGS.validation_data_path, header=0, sep=',')
207: 
208:   train_labels = training_df['label']
209:   validation_labels = validation_df['label']
210:   test_labels = testing_df['label']
211:   train_population = training_df['population']
212:   train_features = training_df[FEATURES]"
R21;False_Positive;True_Positive;./files/EvalID_88/crime.py;205;Incorrectly detected (not in ground truth);"197:   points *= scales * sampling_radius
198:   return points
199: 
200: 
201: def main(_):
202:   num_steps_autoencoder = 0 if FLAGS.uniform_weights else TRAINING_STEPS
203: 
204:   training_df = pd.read_csv(FLAGS.training_data_path, header=0, sep=',')
205:   testing_df = pd.read_csv(FLAGS.testing_data_path, header=0, sep=',')
206:   validation_df = pd.read_csv(FLAGS.validation_data_path, header=0, sep=',')
207: 
208:   train_labels = training_df['label']
209:   validation_labels = validation_df['label']
210:   test_labels = testing_df['label']
211:   train_population = training_df['population']
212:   train_features = training_df[FEATURES]
213:   validation_features = validation_df[FEATURES]"
R21;False_Positive;True_Positive;./files/EvalID_88/crime.py;206;Incorrectly detected (not in ground truth);"198:   return points
199: 
200: 
201: def main(_):
202:   num_steps_autoencoder = 0 if FLAGS.uniform_weights else TRAINING_STEPS
203: 
204:   training_df = pd.read_csv(FLAGS.training_data_path, header=0, sep=',')
205:   testing_df = pd.read_csv(FLAGS.testing_data_path, header=0, sep=',')
206:   validation_df = pd.read_csv(FLAGS.validation_data_path, header=0, sep=',')
207: 
208:   train_labels = training_df['label']
209:   validation_labels = validation_df['label']
210:   test_labels = testing_df['label']
211:   train_population = training_df['population']
212:   train_features = training_df[FEATURES]
213:   validation_features = validation_df[FEATURES]
214:   test_features = testing_df[FEATURES]"
R21;False_Negative;True_Negative;./files/EvalID_13/api_handler.py;401;Missed detection (in ground truth but not detected);"393:         Args:
394:             table_name (str): the table name
395:         Returns:
396:             RESPONSE_TYPE.TABLE
397:         """"""
398: 
399:         result = self._get_table(Identifier(table_name)).get_columns()
400: 
401:         df = pd.DataFrame(result, columns=['Field'])
402:         df['Type'] = 'str'
403: 
404:         return Response(RESPONSE_TYPE.TABLE, df)
405: 
406:     def get_tables(self) -> Response:
407:         """"""
408:         Return list of entities
409:         Returns:"
R21;False_Negative;True_Negative;./files/EvalID_13/api_handler.py;414;Missed detection (in ground truth but not detected);"406:     def get_tables(self) -> Response:
407:         """"""
408:         Return list of entities
409:         Returns:
410:             RESPONSE_TYPE.TABLE
411:         """"""
412:         result = list(self._tables.keys())
413: 
414:         df = pd.DataFrame(result, columns=['table_name'])
415:         df['table_type'] = 'BASE TABLE'
416: 
417:         return Response(RESPONSE_TYPE.TABLE, df)
418: 
419: 
420: class APIChatHandler(APIHandler):
421: 
422:     def get_chat_config(self):"
R21;False_Negative;True_Negative;./files/EvalID_100/feature_server.py;172;Missed detection (in ground truth but not detected);"164: 
165:         # Convert the Protobuf object to JSON and return it
166:         return MessageToDict(
167:             response.proto, preserving_proto_field_name=True, float_precision=18
168:         )
169: 
170:     @app.post(""/push"", dependencies=[Depends(inject_user_details)])
171:     async def push(request: PushFeaturesRequest) -> None:
172:         df = pd.DataFrame(request.df)
173:         actions = []
174:         if request.to == ""offline"":
175:             to = PushMode.OFFLINE
176:             actions = [AuthzedAction.WRITE_OFFLINE]
177:         elif request.to == ""online"":
178:             to = PushMode.ONLINE
179:             actions = [AuthzedAction.WRITE_ONLINE]
180:         elif request.to == ""online_and_offline"":"
R21;False_Negative;True_Negative;./files/EvalID_100/feature_server.py;236;Missed detection (in ground truth but not detected);"228:             )
229:         except FeatureViewNotFoundException:
230:             return store.get_feature_view(  # type: ignore
231:                 feature_view_name, allow_registry_cache=allow_registry_cache
232:             )
233: 
234:     @app.post(""/write-to-online-store"", dependencies=[Depends(inject_user_details)])
235:     def write_to_online_store(request: WriteToFeatureStoreRequest) -> None:
236:         df = pd.DataFrame(request.df)
237:         feature_view_name = request.feature_view_name
238:         allow_registry_cache = request.allow_registry_cache
239:         resource = _get_feast_object(feature_view_name, allow_registry_cache)
240:         assert_permissions(resource=resource, actions=[AuthzedAction.WRITE_ONLINE])
241:         store.write_to_online_store(
242:             feature_view_name=feature_view_name,
243:             df=df,
244:             allow_registry_cache=allow_registry_cache,"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;98;Missed detection (in ground truth but not detected);"90:                 logger.debug(""Convert target to integer"")
91:                 self._categorical_y = LabelEncoder(try_to_fit_numeric=True)
92:                 self._categorical_y.fit(y_train)
93:                 y_train = pd.Series(self._categorical_y.transform(y_train))
94: 
95:             if PreprocessingCategorical.CONVERT_ONE_HOT in target_preprocessing:
96:                 logger.debug(""Convert target to one-hot coding"")
97:                 self._categorical_y = LabelBinarizer()
98:                 self._categorical_y.fit(pd.DataFrame({""target"": y_train}), ""target"")
99:                 y_train = self._categorical_y.transform(
100:                     pd.DataFrame({""target"": y_train}), ""target""
101:                 )
102: 
103:             if Scale.SCALE_LOG_AND_NORMAL in target_preprocessing:
104:                 logger.debug(""Scale log and normal"")
105: 
106:                 self._scale_y = Scale("
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;100;Missed detection (in ground truth but not detected);"92:                 self._categorical_y.fit(y_train)
93:                 y_train = pd.Series(self._categorical_y.transform(y_train))
94: 
95:             if PreprocessingCategorical.CONVERT_ONE_HOT in target_preprocessing:
96:                 logger.debug(""Convert target to one-hot coding"")
97:                 self._categorical_y = LabelBinarizer()
98:                 self._categorical_y.fit(pd.DataFrame({""target"": y_train}), ""target"")
99:                 y_train = self._categorical_y.transform(
100:                     pd.DataFrame({""target"": y_train}), ""target""
101:                 )
102: 
103:             if Scale.SCALE_LOG_AND_NORMAL in target_preprocessing:
104:                 logger.debug(""Scale log and normal"")
105: 
106:                 self._scale_y = Scale(
107:                     [""target""], scale_method=Scale.SCALE_LOG_AND_NORMAL
108:                 )"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;109;Missed detection (in ground truth but not detected);"101:                 )
102: 
103:             if Scale.SCALE_LOG_AND_NORMAL in target_preprocessing:
104:                 logger.debug(""Scale log and normal"")
105: 
106:                 self._scale_y = Scale(
107:                     [""target""], scale_method=Scale.SCALE_LOG_AND_NORMAL
108:                 )
109:                 y_train = pd.DataFrame({""target"": y_train})
110:                 self._scale_y.fit(y_train)
111:                 y_train = self._scale_y.transform(y_train)
112:                 y_train = y_train[""target""]
113: 
114:             if Scale.SCALE_NORMAL in target_preprocessing:
115:                 logger.debug(""Scale normal"")
116: 
117:                 self._scale_y = Scale([""target""], scale_method=Scale.SCALE_NORMAL)"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;118;Missed detection (in ground truth but not detected);"110:                 self._scale_y.fit(y_train)
111:                 y_train = self._scale_y.transform(y_train)
112:                 y_train = y_train[""target""]
113: 
114:             if Scale.SCALE_NORMAL in target_preprocessing:
115:                 logger.debug(""Scale normal"")
116: 
117:                 self._scale_y = Scale([""target""], scale_method=Scale.SCALE_NORMAL)
118:                 y_train = pd.DataFrame({""target"": y_train})
119:                 self._scale_y.fit(y_train)
120:                 y_train = self._scale_y.transform(y_train)
121:                 y_train = y_train[""target""]
122: 
123:         # columns preprocessing
124:         columns_preprocessing = self._params.get(""columns_preprocessing"")
125:         for column in columns_preprocessing:
126:             transforms = columns_preprocessing[column]"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;334;Missed detection (in ground truth but not detected);"326:                 if y_validation is not None and self._categorical_y is not None:
327:                     y_validation = pd.Series(
328:                         self._categorical_y.transform(y_validation)
329:                     )
330: 
331:             if PreprocessingCategorical.CONVERT_ONE_HOT in target_preprocessing:
332:                 if y_validation is not None and self._categorical_y is not None:
333:                     y_validation = self._categorical_y.transform(
334:                         pd.DataFrame({""target"": y_validation}), ""target""
335:                     )
336: 
337:             if Scale.SCALE_LOG_AND_NORMAL in target_preprocessing:
338:                 if self._scale_y is not None and y_validation is not None:
339:                     logger.debug(""Transform log and normalize"")
340:                     y_validation = pd.DataFrame({""target"": y_validation})
341:                     y_validation = self._scale_y.transform(y_validation)
342:                     y_validation = y_validation[""target""]"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;340;Missed detection (in ground truth but not detected);"332:                 if y_validation is not None and self._categorical_y is not None:
333:                     y_validation = self._categorical_y.transform(
334:                         pd.DataFrame({""target"": y_validation}), ""target""
335:                     )
336: 
337:             if Scale.SCALE_LOG_AND_NORMAL in target_preprocessing:
338:                 if self._scale_y is not None and y_validation is not None:
339:                     logger.debug(""Transform log and normalize"")
340:                     y_validation = pd.DataFrame({""target"": y_validation})
341:                     y_validation = self._scale_y.transform(y_validation)
342:                     y_validation = y_validation[""target""]
343: 
344:             if Scale.SCALE_NORMAL in target_preprocessing:
345:                 if self._scale_y is not None and y_validation is not None:
346:                     logger.debug(""Transform normalize"")
347:                     y_validation = pd.DataFrame({""target"": y_validation})
348:                     y_validation = self._scale_y.transform(y_validation)"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;347;Missed detection (in ground truth but not detected);"339:                     logger.debug(""Transform log and normalize"")
340:                     y_validation = pd.DataFrame({""target"": y_validation})
341:                     y_validation = self._scale_y.transform(y_validation)
342:                     y_validation = y_validation[""target""]
343: 
344:             if Scale.SCALE_NORMAL in target_preprocessing:
345:                 if self._scale_y is not None and y_validation is not None:
346:                     logger.debug(""Transform normalize"")
347:                     y_validation = pd.DataFrame({""target"": y_validation})
348:                     y_validation = self._scale_y.transform(y_validation)
349:                     y_validation = y_validation[""target""]
350: 
351:         # columns preprocessing
352:         if len(self._remove_columns) and X_validation is not None:
353:             cols_to_remove = [
354:                 col for col in X_validation.columns if col in self._remove_columns
355:             ]"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;430;Missed detection (in ground truth but not detected);"422:                 lower=np.finfo(np.float32).min + 1000,
423:                 upper=np.finfo(np.float32).max - 1000,
424:             )
425: 
426:         return X_validation, y_validation, sample_weight_validation
427: 
428:     def inverse_scale_target(self, y):
429:         if self._scale_y is not None:
430:             y = pd.DataFrame({""target"": y})
431:             y = self._scale_y.inverse_transform(y)
432:             y = y[""target""]
433:         return y
434: 
435:     def inverse_categorical_target(self, y):
436:         if self._categorical_y is not None:
437:             y = self._categorical_y.inverse_transform(y)
438:             y = y.astype(str)"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;487;Missed detection (in ground truth but not detected);"479:             if len(y.shape) == 1:
480:                 # binary classification
481:                 for label, value in self._categorical_y.to_json().items():
482:                     if value == 1:
483:                         pos_label = label
484:                     else:
485:                         neg_label = label
486:                 # threshold is applied in AutoML class
487:                 return pd.DataFrame(
488:                     {
489:                         ""prediction_{}"".format(neg_label): 1 - y,
490:                         ""prediction_{}"".format(pos_label): y,
491:                     }
492:                 )
493:             else:
494:                 # multiclass classification
495:                 if ""unique_values"" not in self._categorical_y.to_json():"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;512;Missed detection (in ground truth but not detected);"504:                         )
505:                     }
506: 
507:                 d = {}
508:                 cols = []
509:                 for i in range(y.shape[1]):
510:                     d[""prediction_{}"".format(labels[i])] = y[:, i]
511:                     cols += [""prediction_{}"".format(labels[i])]
512:                 df = pd.DataFrame(d)
513:                 df[""label""] = np.argmax(np.array(df[cols]), axis=1)
514: 
515:                 df[""label""] = df[""label""].map(labels)
516: 
517:                 return df
518:         else:  # self._categorical_y is None
519:             if ""ml_task"" in self._params:
520:                 if self._params[""ml_task""] == BINARY_CLASSIFICATION:"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;513;Missed detection (in ground truth but not detected);"505:                     }
506: 
507:                 d = {}
508:                 cols = []
509:                 for i in range(y.shape[1]):
510:                     d[""prediction_{}"".format(labels[i])] = y[:, i]
511:                     cols += [""prediction_{}"".format(labels[i])]
512:                 df = pd.DataFrame(d)
513:                 df[""label""] = np.argmax(np.array(df[cols]), axis=1)
514: 
515:                 df[""label""] = df[""label""].map(labels)
516: 
517:                 return df
518:         else:  # self._categorical_y is None
519:             if ""ml_task"" in self._params:
520:                 if self._params[""ml_task""] == BINARY_CLASSIFICATION:
521:                     return pd.DataFrame({""prediction_0"": 1 - y, ""prediction_1"": y})"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;521;Missed detection (in ground truth but not detected);"513:                 df[""label""] = np.argmax(np.array(df[cols]), axis=1)
514: 
515:                 df[""label""] = df[""label""].map(labels)
516: 
517:                 return df
518:         else:  # self._categorical_y is None
519:             if ""ml_task"" in self._params:
520:                 if self._params[""ml_task""] == BINARY_CLASSIFICATION:
521:                     return pd.DataFrame({""prediction_0"": 1 - y, ""prediction_1"": y})
522:                 elif self._params[""ml_task""] == MULTICLASS_CLASSIFICATION:
523:                     return pd.DataFrame(
524:                         data=y,
525:                         columns=[""prediction_{}"".format(i) for i in range(y.shape[1])],
526:                     )
527: 
528:         return pd.DataFrame({""prediction"": y})
529:"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;523;Missed detection (in ground truth but not detected);"515:                 df[""label""] = df[""label""].map(labels)
516: 
517:                 return df
518:         else:  # self._categorical_y is None
519:             if ""ml_task"" in self._params:
520:                 if self._params[""ml_task""] == BINARY_CLASSIFICATION:
521:                     return pd.DataFrame({""prediction_0"": 1 - y, ""prediction_1"": y})
522:                 elif self._params[""ml_task""] == MULTICLASS_CLASSIFICATION:
523:                     return pd.DataFrame(
524:                         data=y,
525:                         columns=[""prediction_{}"".format(i) for i in range(y.shape[1])],
526:                     )
527: 
528:         return pd.DataFrame({""prediction"": y})
529: 
530:     def to_json(self):
531:         preprocessing_params = {}"
R21;False_Negative;True_Negative;./files/EvalID_16/preprocessing.py;528;Missed detection (in ground truth but not detected);"520:                 if self._params[""ml_task""] == BINARY_CLASSIFICATION:
521:                     return pd.DataFrame({""prediction_0"": 1 - y, ""prediction_1"": y})
522:                 elif self._params[""ml_task""] == MULTICLASS_CLASSIFICATION:
523:                     return pd.DataFrame(
524:                         data=y,
525:                         columns=[""prediction_{}"".format(i) for i in range(y.shape[1])],
526:                     )
527: 
528:         return pd.DataFrame({""prediction"": y})
529: 
530:     def to_json(self):
531:         preprocessing_params = {}
532:         if self._remove_columns:
533:             preprocessing_params[""remove_columns""] = self._remove_columns
534:         if self._missing_values is not None and len(self._missing_values):
535:             mvs = []  # refactor
536:             for mv in self._missing_values:"
R21;False_Negative;True_Negative;./files/EvalID_93/utils.py;69;Missed detection (in ground truth but not detected);"61:         fake.columns = real.columns
62: 
63:     for col in fake.columns:
64:         fake[col] = fake[col].astype(real[col].dtype)
65:     return real, fake
66: 
67: 
68: def dict_to_df(data: Dict[str, Any]):
69:     return pd.DataFrame(
70:         {""result"": list(data.values())}, index=list(data.keys())
71:     )
72: 
73: 
74: class EvaluationResult(object):
75:     def __init__(
76:         self, name, content, prefix=None, appendix=None, notebook=False
77:     ):"
R21;False_Negative;True_Negative;./files/EvalID_85/db2_handler.py;144;Missed detection (in ground truth but not detected);"136:         with connection.cursor() as cur:
137:             try:
138:                 cur.execute(query)
139: 
140:                 if cur._result_set_produced:
141:                     result = cur.fetchall()
142:                     response = Response(
143:                         RESPONSE_TYPE.TABLE,
144:                         data_frame=pd.DataFrame(
145:                             result, columns=[x[0] for x in cur.description]
146:                         ),
147:                     )
148:                 else:
149:                     response = Response(RESPONSE_TYPE.OK)
150:                 connection.commit()
151:             except (OperationalError, ProgrammingError) as known_error:
152:                 logger.error(f""Error running query: {query} on {self.connection_data.get('database')}!"")"
R21;False_Negative;True_Negative;./files/EvalID_85/db2_handler.py;203;Missed detection (in ground truth but not detected);"195:                     ""TABLE_NAME"": table[""TABLE_NAME""],
196:                     ""TABLE_SCHEMA"": table[""TABLE_SCHEM""],
197:                     ""TABLE_TYPE"": table[""TABLE_TYPE""],
198:                 }
199:             )
200: 
201:         response = Response(
202:             RESPONSE_TYPE.TABLE,
203:             data_frame=pd.DataFrame(tables)
204:         )
205: 
206:         return response
207: 
208:     def get_columns(self, table_name: Text) -> Response:
209:         """"""
210:         Retrieves column details for a specified table in the IBM Db2 database.
211:"
R21;False_Negative;True_Negative;./files/EvalID_96/chromadb_handler.py;317;Missed detection (in ground truth but not detected);"309:         }
310: 
311:         if columns is not None:
312:             payload = {column: payload[column] for column in columns}
313: 
314:         # always include distance
315:         if distances is not None:
316:             payload[TableField.DISTANCE.value] = distances
317:         return pd.DataFrame(payload)
318: 
319:     def insert(self, table_name: str, data: pd.DataFrame):
320:         """"""
321:         Insert data into the ChromaDB database.
322:         """"""
323: 
324:         collection = self._client.get_or_create_collection(name=table_name)
325:"
R21;False_Negative;True_Negative;./files/EvalID_96/chromadb_handler.py;426;Missed detection (in ground truth but not detected);"418:             else:
419:                 raise Exception(f""Collection {table_name} does not exist!"")
420: 
421:     def get_tables(self) -> HandlerResponse:
422:         """"""
423:         Get the list of collections in the ChromaDB database.
424:         """"""
425:         collections = self._client.list_collections()
426:         collections_name = pd.DataFrame(
427:             columns=[""table_name""],
428:             data=[collection.name for collection in collections],
429:         )
430:         return Response(resp_type=RESPONSE_TYPE.TABLE, data_frame=collections_name)
431: 
432:     def get_columns(self, table_name: str) -> HandlerResponse:
433:         # check if collection exists
434:         try:"
R21;False_Negative;True_Negative;./files/EvalID_9/report.py;96;Missed detection (in ground truth but not detected);"88:         avg_precision, avg_recall, avg_f1, _ = precision_recall_fscore_support(
89:             y_true=y_true, y_pred=y_pred, average=average, labels=labels
90:         )
91: 
92:         avg_metrics = avg_precision, avg_recall, avg_f1
93:         for k, v in zip(metrics_names[:4], avg_metrics):
94:             metrics[k][average] = v
95: 
96:     report = pd.DataFrame(
97:         [precision, recall, f1, auc, support, r_support],
98:         columns=labels,
99:         index=metrics_names,
100:     ).T
101: 
102:     if beta is not None:
103:         _, _, fbeta, _ = precision_recall_fscore_support(
104:             y_true=y_true, y_pred=y_pred, average=None, beta=beta, labels=labels"
R21;False_Negative;True_Negative;./files/EvalID_9/report.py;123;Missed detection (in ground truth but not detected);"115:     metrics[""precision""][""accuracy""] = accuracy
116:     if y_scores is not None:
117:         metrics[""auc""][""macro""] = roc_auc_score(
118:             y_true, y_scores, multi_class=""ovr"", average=""macro""
119:         )
120:         metrics[""auc""][""weighted""] = roc_auc_score(
121:             y_true, y_scores, multi_class=""ovr"", average=""weighted""
122:         )
123:     metrics = pd.DataFrame(metrics, index=avg_names + [""accuracy""])
124: 
125:     result = pd.concat((report, metrics)).fillna("""")
126: 
127:     if beta:
128:         result[""f-beta""][""macro""] = avg_fbeta[0]
129:         result[""f-beta""][""micro""] = avg_fbeta[1]
130:         result[""f-beta""][""weighted""] = avg_fbeta[2]
131:     return result"
R21;False_Negative;True_Negative;./files/EvalID_95/eval_on_synthetic.py;123;Missed detection (in ground truth but not detected);"115:             sf.write(local_save_dir + ""noisy.wav"", noisy_np, fs)
116:             sf.write(local_save_dir + ""clean.wav"", clean_np, fs)
117:             sf.write(local_save_dir + ""estimate.wav"", est_clean_np, fs)
118:             # Write local metrics to the example folder.
119:             with open(local_save_dir + ""metrics.json"", ""w"") as f:
120:                 json.dump(utt_metrics, f, indent=0)
121: 
122:     # Save all metrics to the experiment folder.
123:     all_metrics_df = pd.DataFrame(series_list)
124:     return all_metrics_df
125: 
126: 
127: def load_wav_dic(wav_dic):
128:     """"""Load wavs files from a dictionary with path entries.
129: 
130:     Returns:
131:         tuple: noisy speech waveform, clean speech waveform."
R21;False_Negative;True_Negative;./files/EvalID_15/data_structures.py;501;Missed detection (in ground truth but not detected);"493:       else:
494:         slice_feature, slice_value = slice_spec.feature.value, slice_spec.value
495:       slice_features.append(str(slice_feature))
496:       slice_values.append(str(slice_value))
497:       attack_types.append(str(attack_result.attack_type))
498:       advantages.append(float(attack_result.get_attacker_advantage()))
499:       aucs.append(float(attack_result.get_auc()))
500: 
501:     df = pd.DataFrame({
502:         str(AttackResultsDFColumns.SLICE_FEATURE): slice_features,
503:         str(AttackResultsDFColumns.SLICE_VALUE): slice_values,
504:         str(AttackResultsDFColumns.ATTACK_TYPE): attack_types,
505:         str(PrivacyMetric.ATTACKER_ADVANTAGE): advantages,
506:         str(PrivacyMetric.AUC): aucs
507:     })
508:     return df
509:"
R21;False_Negative;True_Negative;./files/EvalID_98/twilio_handler.py;196;Missed detection (in ground truth but not detected);"188:                         del params['media_url']
189: 
190:                 params['body'] = text
191:                 params_to_send = {key: params[key] for key in insert_params if (key in params)}
192:                 ret_row = self.handler.send_sms(params_to_send, ret_as_dict=True)
193:                 ret_row['body'] = text
194:                 ret.append(ret_row)
195: 
196:         return pd.DataFrame(ret)
197: 
198: 
199: class TwilioHandler(APIHandler):
200: 
201:     def __init__(self, name=None, **kwargs):
202:         super().__init__(name)
203: 
204:         args = kwargs.get('connection_data', {})"
R21;False_Negative;True_Negative;./files/EvalID_98/twilio_handler.py;352;Missed detection (in ground truth but not detected);"344:                 'api_version': msg.api_version,
345:                 'uri': msg.uri,
346:                 # 'media_url': [media.uri for media in msg.media.list()]
347:                 # ... Add other properties as needed
348:             }
349:             data.append(msg_data)
350: 
351:         if df is True:
352:             return pd.DataFrame(data)
353:         return Response(RESPONSE_TYPE.TABLE, data_frame=pd.DataFrame(data))
354: 
355:     def list_phone_numbers(self, params, df=False):
356:         limit = int(params.get('limit', 100))
357:         args = {
358:             'limit': limit
359:         }
360:         args = {arg: val for arg, val in args.items() if val is not None}"
R21;False_Negative;True_Negative;./files/EvalID_98/twilio_handler.py;353;Missed detection (in ground truth but not detected);"345:                 'uri': msg.uri,
346:                 # 'media_url': [media.uri for media in msg.media.list()]
347:                 # ... Add other properties as needed
348:             }
349:             data.append(msg_data)
350: 
351:         if df is True:
352:             return pd.DataFrame(data)
353:         return Response(RESPONSE_TYPE.TABLE, data_frame=pd.DataFrame(data))
354: 
355:     def list_phone_numbers(self, params, df=False):
356:         limit = int(params.get('limit', 100))
357:         args = {
358:             'limit': limit
359:         }
360:         args = {arg: val for arg, val in args.items() if val is not None}
361:         phone_numbers = self.client.incoming_phone_numbers.list(**args)"
R21;False_Negative;True_Negative;./files/EvalID_98/twilio_handler.py;384;Missed detection (in ground truth but not detected);"376:                 'voice_url': number.voice_url,
377:                 'sms_url': number.sms_url,
378:                 'uri': number.uri,
379:                 # ... Add other properties as needed
380:             }
381:             data.append(num_data)
382: 
383:         if df is True:
384:             return pd.DataFrame(data)
385:         return Response(RESPONSE_TYPE.TABLE, data_frame=pd.DataFrame(data))"
R21;False_Negative;True_Negative;./files/EvalID_98/twilio_handler.py;385;Missed detection (in ground truth but not detected);"377:                 'sms_url': number.sms_url,
378:                 'uri': number.uri,
379:                 # ... Add other properties as needed
380:             }
381:             data.append(num_data)
382: 
383:         if df is True:
384:             return pd.DataFrame(data)
385:         return Response(RESPONSE_TYPE.TABLE, data_frame=pd.DataFrame(data))"
R21;False_Negative;True_Negative;./files/EvalID_89/predict_tools.py;144;Missed detection (in ground truth but not detected);"136:     match_ids: np.ndarray,
137:     sample_ids: np.ndarray,
138:     match_id_name: str,
139:     sample_id_name: str,
140:     label: np.array = None,
141:     threshold=0.5,
142:     classes: list = None,
143: ):
144:     df = pd.DataFrame()
145:     if len(pred.shape) == 1:
146:         df[PREDICT_SCORE] = np.array(pred)
147:     elif len(pred.shape) == 2:
148:         if pred.shape[1] == 1:
149:             df[PREDICT_SCORE] = np.array(pred).flatten()
150:         else:
151:             df[PREDICT_SCORE] = np.array(pred).tolist()
152:     else:"
R21;False_Negative;True_Negative;./files/EvalID_14/LinearGaussianCPD.py;107;Missed detection (in ground truth but not detected);"99:             and corresponding X values.
100: 
101:         states: All the input states that are jointly gaussian.
102: 
103:         Returns
104:         -------
105:         beta, variance (tuple): Returns estimated betas and the variance.
106:         """"""
107:         x_df = pd.DataFrame(data, columns=states)
108:         x_len = len(self.evidence)
109: 
110:         sym_coefs = []
111:         for i in range(0, x_len):
112:             sym_coefs.append(""b"" + str(i + 1) + ""_coef"")
113: 
114:         sum_x = x_df.sum()
115:         x = [sum_x[""(Y|X)""]]"
R21;False_Negative;True_Negative;./files/EvalID_14/LinearGaussianCPD.py;116;Missed detection (in ground truth but not detected);"108:         x_len = len(self.evidence)
109: 
110:         sym_coefs = []
111:         for i in range(0, x_len):
112:             sym_coefs.append(""b"" + str(i + 1) + ""_coef"")
113: 
114:         sum_x = x_df.sum()
115:         x = [sum_x[""(Y|X)""]]
116:         coef_matrix = pd.DataFrame(columns=sym_coefs)
117: 
118:         # First we compute just the coefficients of beta_1 to beta_N.
119:         # Later we compute beta_0 and append it.
120:         for i in range(0, x_len):
121:             x.append(self.sum_of_product(x_df[""(Y|X)""], x_df[self.evidence[i]]))
122:             for j in range(0, x_len):
123:                 coef_matrix.loc[i, sym_coefs[j]] = self.sum_of_product(
124:                     x_df[self.evidence[i]], x_df[self.evidence[j]]"
R3;False_Negative;True_Negative;./files/EvalID_78/inputs.py;63;Missed detection (in ground truth but not detected);"55:     Args:
56:         record: A Tensor of type string. Each string is a record/row in the csv
57:         and all records should have the same format.
58: 
59:     Returns:
60:         A dictionary with all column names and values for the record.
61:     """"""
62:     column_defaults = [
63:         tf.constant([], tf.string),
64:         tf.constant([], tf.string),
65:         tf.constant([], tf.int32)]
66:     column_names = ['img_file', 'subspecies', TARGET_COLUMN]
67:     columns = tf.decode_csv(record, record_defaults=column_defaults)
68:     return dict(zip(column_names, columns))
69: 
70: 
71: def _get_features_target_tuple(features):"
R3;False_Negative;True_Negative;./files/EvalID_78/inputs.py;64;Missed detection (in ground truth but not detected);"56:         record: A Tensor of type string. Each string is a record/row in the csv
57:         and all records should have the same format.
58: 
59:     Returns:
60:         A dictionary with all column names and values for the record.
61:     """"""
62:     column_defaults = [
63:         tf.constant([], tf.string),
64:         tf.constant([], tf.string),
65:         tf.constant([], tf.int32)]
66:     column_names = ['img_file', 'subspecies', TARGET_COLUMN]
67:     columns = tf.decode_csv(record, record_defaults=column_defaults)
68:     return dict(zip(column_names, columns))
69: 
70: 
71: def _get_features_target_tuple(features):
72:     """"""Separates features from target."
R3;False_Negative;True_Negative;./files/EvalID_78/inputs.py;65;Missed detection (in ground truth but not detected);"57:         and all records should have the same format.
58: 
59:     Returns:
60:         A dictionary with all column names and values for the record.
61:     """"""
62:     column_defaults = [
63:         tf.constant([], tf.string),
64:         tf.constant([], tf.string),
65:         tf.constant([], tf.int32)]
66:     column_names = ['img_file', 'subspecies', TARGET_COLUMN]
67:     columns = tf.decode_csv(record, record_defaults=column_defaults)
68:     return dict(zip(column_names, columns))
69: 
70: 
71: def _get_features_target_tuple(features):
72:     """"""Separates features from target.
73:"
R3;False_Negative;True_Negative;./files/EvalID_71/transforms.py;53;Missed detection (in ground truth but not detected);"45:         """""" Run op
46: 
47:         :param samples: Sample arrays
48:         :param labels: Label arrays
49:         :param mean: Mean (unused)
50:         :param stdev:  Std (unused)
51:         :return: Padded samples and labels
52:         """"""
53:         paddings = tf.constant([[0, 0], [0, 0], [0, 5], [0, 0]])
54:         samples = tf.pad(samples, paddings, ""CONSTANT"")
55:         if labels is None:
56:             return samples
57:         labels = tf.pad(labels, paddings, ""CONSTANT"")
58:         return samples, labels
59: 
60: 
61: class CenterCrop: # pylint: disable=R0903"
R3;False_Negative;True_Negative;./files/EvalID_71/transforms.py;116;Missed detection (in ground truth but not detected);"108:         :param samples: Sample arrays
109:         :param labels: Label arrays
110:         :param mean: Mean (unused)
111:         :param stdev:  Std (unused)
112:         :return: Cropped samples and labels
113:         """"""
114:         shape = samples.get_shape()
115:         min_ = tf.constant(self.margins, dtype=tf.float32)
116:         max_ = tf.constant([shape[0].value - self.shape[0] - self.margins[0],
117:                             shape[1].value - self.shape[1] - self.margins[1],
118:                             shape[2].value - self.shape[2] - self.margins[2]],
119:                            dtype=tf.float32)
120:         center = tf.random_uniform((len(self.shape),), minval=min_, maxval=max_)
121:         center = tf.cast(center, dtype=tf.int32)
122:         samples = samples[center[0]:center[0] + self.shape[0],
123:                           center[1]:center[1] + self.shape[1],
124:                           center[2]:center[2] + self.shape[2]]"
R3;False_Negative;True_Negative;./files/EvalID_73/tensorboard_pr_curve.py;119;Missed detection (in ground truth but not detected);"111:         tf.concat([
112:             tf.abs(distribution.sample([number_of_blues, 2], seed=132)),
113:             255 - tf.abs(distribution.sample([number_of_blues, 1], seed=153))
114:         ], axis=1),
115:         0, 255)
116: 
117:     # Assign each color a vector of 3 booleans based on its true label.
118:     labels = tf.concat([
119:         tf.tile(tf.constant([[True, False, False]]), (number_of_reds, 1)),
120:         tf.tile(tf.constant([[False, True, False]]), (number_of_greens, 1)),
121:         tf.tile(tf.constant([[False, False, True]]), (number_of_blues, 1)),
122:     ], axis=0)
123: 
124:     # We introduce 3 normal distributions. They are used to predict whether a
125:     # color falls under a certain class (based on distances from corners of the
126:     # color triangle). The distributions vary per color. We have the distributions
127:     # narrow over time."
R3;False_Negative;True_Negative;./files/EvalID_73/tensorboard_pr_curve.py;120;Missed detection (in ground truth but not detected);"112:             tf.abs(distribution.sample([number_of_blues, 2], seed=132)),
113:             255 - tf.abs(distribution.sample([number_of_blues, 1], seed=153))
114:         ], axis=1),
115:         0, 255)
116: 
117:     # Assign each color a vector of 3 booleans based on its true label.
118:     labels = tf.concat([
119:         tf.tile(tf.constant([[True, False, False]]), (number_of_reds, 1)),
120:         tf.tile(tf.constant([[False, True, False]]), (number_of_greens, 1)),
121:         tf.tile(tf.constant([[False, False, True]]), (number_of_blues, 1)),
122:     ], axis=0)
123: 
124:     # We introduce 3 normal distributions. They are used to predict whether a
125:     # color falls under a certain class (based on distances from corners of the
126:     # color triangle). The distributions vary per color. We have the distributions
127:     # narrow over time.
128:     initial_standard_deviations = [v + FLAGS.steps for v in (158, 200, 242)]"
R3;False_Negative;True_Negative;./files/EvalID_73/tensorboard_pr_curve.py;121;Missed detection (in ground truth but not detected);"113:             255 - tf.abs(distribution.sample([number_of_blues, 1], seed=153))
114:         ], axis=1),
115:         0, 255)
116: 
117:     # Assign each color a vector of 3 booleans based on its true label.
118:     labels = tf.concat([
119:         tf.tile(tf.constant([[True, False, False]]), (number_of_reds, 1)),
120:         tf.tile(tf.constant([[False, True, False]]), (number_of_greens, 1)),
121:         tf.tile(tf.constant([[False, False, True]]), (number_of_blues, 1)),
122:     ], axis=0)
123: 
124:     # We introduce 3 normal distributions. They are used to predict whether a
125:     # color falls under a certain class (based on distances from corners of the
126:     # color triangle). The distributions vary per color. We have the distributions
127:     # narrow over time.
128:     initial_standard_deviations = [v + FLAGS.steps for v in (158, 200, 242)]
129:     iteration = tf.compat.v1.placeholder(tf.int32, shape=[])"
R3;False_Negative;True_Negative;./files/EvalID_73/tensorboard_pr_curve.py;151;Missed detection (in ground truth but not detected);"143:             initial_standard_deviations[2] - iteration,
144:             dtype=tf.float32))
145: 
146:     # Make predictions (assign 3 probabilities to each color based on each color's
147:     # distance to each of the 3 corners). We seek double the area in the right
148:     # tail of the normal distribution.
149:     examples = tf.concat([true_reds, true_greens, true_blues], axis=0)
150:     probabilities_colors_are_red = (1 - red_predictor.cdf(
151:         tf.norm(tensor=examples - tf.constant([255., 0, 0]), axis=1))) * 2
152:     probabilities_colors_are_green = (1 - green_predictor.cdf(
153:         tf.norm(tensor=examples - tf.constant([0, 255., 0]), axis=1))) * 2
154:     probabilities_colors_are_blue = (1 - blue_predictor.cdf(
155:         tf.norm(tensor=examples - tf.constant([0, 0, 255.]), axis=1))) * 2
156: 
157:     predictions = (
158:         probabilities_colors_are_red,
159:         probabilities_colors_are_green,"
R3;False_Negative;True_Negative;./files/EvalID_73/tensorboard_pr_curve.py;153;Missed detection (in ground truth but not detected);"145: 
146:     # Make predictions (assign 3 probabilities to each color based on each color's
147:     # distance to each of the 3 corners). We seek double the area in the right
148:     # tail of the normal distribution.
149:     examples = tf.concat([true_reds, true_greens, true_blues], axis=0)
150:     probabilities_colors_are_red = (1 - red_predictor.cdf(
151:         tf.norm(tensor=examples - tf.constant([255., 0, 0]), axis=1))) * 2
152:     probabilities_colors_are_green = (1 - green_predictor.cdf(
153:         tf.norm(tensor=examples - tf.constant([0, 255., 0]), axis=1))) * 2
154:     probabilities_colors_are_blue = (1 - blue_predictor.cdf(
155:         tf.norm(tensor=examples - tf.constant([0, 0, 255.]), axis=1))) * 2
156: 
157:     predictions = (
158:         probabilities_colors_are_red,
159:         probabilities_colors_are_green,
160:         probabilities_colors_are_blue
161:     )"
R3;False_Negative;True_Negative;./files/EvalID_73/tensorboard_pr_curve.py;155;Missed detection (in ground truth but not detected);"147:     # distance to each of the 3 corners). We seek double the area in the right
148:     # tail of the normal distribution.
149:     examples = tf.concat([true_reds, true_greens, true_blues], axis=0)
150:     probabilities_colors_are_red = (1 - red_predictor.cdf(
151:         tf.norm(tensor=examples - tf.constant([255., 0, 0]), axis=1))) * 2
152:     probabilities_colors_are_green = (1 - green_predictor.cdf(
153:         tf.norm(tensor=examples - tf.constant([0, 255., 0]), axis=1))) * 2
154:     probabilities_colors_are_blue = (1 - blue_predictor.cdf(
155:         tf.norm(tensor=examples - tf.constant([0, 0, 255.]), axis=1))) * 2
156: 
157:     predictions = (
158:         probabilities_colors_are_red,
159:         probabilities_colors_are_green,
160:         probabilities_colors_are_blue
161:     )
162: 
163:     # This is the crucial piece. We write data required for generating PR curves."
R3;False_Negative;True_Negative;./files/EvalID_76/dataset.py;54;Missed detection (in ground truth but not detected);"46:     [HEIGHT, WIDTH, 1] the z depth.
47:   """"""
48:   shape = p_world.shape.as_list()
49:   height, width = shape[0], shape[1]
50:   p_world_homogeneous = tf.concat([p_world, tf.ones([height, width, 1])], -1)
51:   p_camera = tf.squeeze(
52:       tf.matmul(pose_w2c[tf.newaxis, tf.newaxis, :],
53:                 tf.expand_dims(p_world_homogeneous, -1)), -1)
54:   p_camera = p_camera*tf.constant([1., 1., -1.], shape=[1, 1, 3])
55:   p_image = tf.squeeze(tf.matmul(intrinsics[tf.newaxis, tf.newaxis, :],
56:                                  tf.expand_dims(p_camera, -1)), -1)
57:   z = p_image[:, :, -1:]
58:   return tf.math.divide_no_nan(p_image[:, :, :2], z), z
59: 
60: 
61: def image_to_world_projection(depth, intrinsics, pose_c2w):
62:   """"""Project points on the image to the world frame."
R3;False_Negative;True_Negative;./files/EvalID_76/dataset.py;85;Missed detection (in ground truth but not detected);"77:   p_pixel_homogeneous = tf.concat([tf.stack([xx, yy], axis=-1),
78:                                    tf.ones([height, width, 1])], -1)
79: 
80:   p_image = tf.squeeze(tf.matmul(
81:       tf.matrix_inverse(intrinsics[tf.newaxis, tf.newaxis, :]),
82:       tf.expand_dims(p_pixel_homogeneous, -1)), -1)
83: 
84:   z = depth*tf.reduce_sum(
85:       tf.math.l2_normalize(p_image, axis=-1)*tf.constant([[[0., 0., 1.]]]),
86:       axis=-1,
87:       keepdims=True)
88:   p_camera = z*p_image
89:   # convert to OpenGL coordinate system.
90:   p_camera = p_camera*tf.constant([1., 1., -1.], shape=[1, 1, 3])
91:   p_camera_homogeneous = tf.concat(
92:       [p_camera, tf.ones(shape=[height, width, 1])], -1)
93:   # Convert camera coordinates to world coordinates."
R3;False_Negative;True_Negative;./files/EvalID_76/dataset.py;90;Missed detection (in ground truth but not detected);"82:       tf.expand_dims(p_pixel_homogeneous, -1)), -1)
83: 
84:   z = depth*tf.reduce_sum(
85:       tf.math.l2_normalize(p_image, axis=-1)*tf.constant([[[0., 0., 1.]]]),
86:       axis=-1,
87:       keepdims=True)
88:   p_camera = z*p_image
89:   # convert to OpenGL coordinate system.
90:   p_camera = p_camera*tf.constant([1., 1., -1.], shape=[1, 1, 3])
91:   p_camera_homogeneous = tf.concat(
92:       [p_camera, tf.ones(shape=[height, width, 1])], -1)
93:   # Convert camera coordinates to world coordinates.
94:   p_world = tf.squeeze(
95:       tf.matmul(pose_c2w[tf.newaxis, tf.newaxis, :],
96:                 tf.expand_dims(p_camera_homogeneous, -1)), -1)
97:   return p_world
98:"
R3;False_Negative;True_Negative;./files/EvalID_76/dataset.py;124;Missed detection (in ground truth but not detected);"116:       pose1_c2w[:, :3] is the rotation and pose1_c2w[:, -1] is the translation.
117:     intrinsics: [3, 3] camera's intrinsic matrix.
118: 
119:   Returns:
120:     [HEIGHT, WIDTH] two overlap masks of the two inputs respectively.
121:   """"""
122: 
123:   pose1_w2c = tf.matrix_inverse(
124:       tf.concat([pose1_c2w, tf.constant([[0., 0., 0., 1.]])], 0))[:3]
125:   pose2_w2c = tf.matrix_inverse(
126:       tf.concat([pose2_c2w, tf.constant([[0., 0., 0., 1.]])], 0))[:3]
127: 
128:   p_world1 = image_to_world_projection(depth1, intrinsics, pose1_c2w)
129:   p_image1_in_2, z1_c2 = world_to_image_projection(
130:       p_world1, intrinsics, pose2_w2c)
131: 
132:   p_world2 = image_to_world_projection(depth2, intrinsics, pose2_c2w)"
R3;False_Negative;True_Negative;./files/EvalID_76/dataset.py;126;Missed detection (in ground truth but not detected);"118: 
119:   Returns:
120:     [HEIGHT, WIDTH] two overlap masks of the two inputs respectively.
121:   """"""
122: 
123:   pose1_w2c = tf.matrix_inverse(
124:       tf.concat([pose1_c2w, tf.constant([[0., 0., 0., 1.]])], 0))[:3]
125:   pose2_w2c = tf.matrix_inverse(
126:       tf.concat([pose2_c2w, tf.constant([[0., 0., 0., 1.]])], 0))[:3]
127: 
128:   p_world1 = image_to_world_projection(depth1, intrinsics, pose1_c2w)
129:   p_image1_in_2, z1_c2 = world_to_image_projection(
130:       p_world1, intrinsics, pose2_w2c)
131: 
132:   p_world2 = image_to_world_projection(depth2, intrinsics, pose2_c2w)
133:   p_image2_in_1, z2_c1 = world_to_image_projection(
134:       p_world2, intrinsics, pose1_w2c)"
R3;False_Negative;True_Negative;./files/EvalID_76/dataset.py;324;Missed detection (in ground truth but not detected);"316:     pano2_depth: [HEIGHT, WIDTH, 1] the panoramic depth map of pano2_rgb.
317: 
318:   Returns:
319:     ViewPair
320:   """"""
321:   ViewPair = collections.namedtuple(
322:       'ViewPair', ['img1', 'img2', 'mask1', 'mask2', 'fov', 'r', 't'])
323: 
324:   swap_yz = tf.constant([[1., 0., 0.], [0., 0., 1.], [0., -1., 0.]],
325:                         shape=[1, 3, 3])
326:   lookat_direction1 = math_utils.random_vector_on_sphere(
327:       1, [[-math.sin(math.pi/3), math.sin(math.pi/3)], [0., 2*math.pi]])
328:   lookat_direction1 = tf.squeeze(
329:       tf.matmul(swap_yz, tf.expand_dims(lookat_direction1, -1)), -1)
330: 
331:   lookat_direction2 = math_utils.uniform_sampled_vector_within_cone(
332:       lookat_direction1, math_utils.degrees_to_radians(max_rotation))"
R3;False_Negative;True_Negative;./files/EvalID_76/dataset.py;335;Missed detection (in ground truth but not detected);"327:       1, [[-math.sin(math.pi/3), math.sin(math.pi/3)], [0., 2*math.pi]])
328:   lookat_direction1 = tf.squeeze(
329:       tf.matmul(swap_yz, tf.expand_dims(lookat_direction1, -1)), -1)
330: 
331:   lookat_direction2 = math_utils.uniform_sampled_vector_within_cone(
332:       lookat_direction1, math_utils.degrees_to_radians(max_rotation))
333:   lookat_directions = tf.concat([lookat_direction1, lookat_direction2], 0)
334:   up1 = math_utils.uniform_sampled_vector_within_cone(
335:       tf.constant([[0., 0., 1.]]), math_utils.degrees_to_radians(max_tilt))
336:   up2 = math_utils.uniform_sampled_vector_within_cone(
337:       tf.constant([[0., 0., 1.]]), math_utils.degrees_to_radians(max_tilt))
338:   lookat_rotations = math_utils.lookat_matrix(
339:       tf.concat([up1, up2], 0), lookat_directions)
340:   sample_rotations = tf.matmul(
341:       tf.concat([r1, r2], 0), lookat_rotations, transpose_a=True)
342: 
343:   sampled_views = transformation.rectilinear_projection("
R3;False_Negative;True_Negative;./files/EvalID_76/dataset.py;337;Missed detection (in ground truth but not detected);"329:       tf.matmul(swap_yz, tf.expand_dims(lookat_direction1, -1)), -1)
330: 
331:   lookat_direction2 = math_utils.uniform_sampled_vector_within_cone(
332:       lookat_direction1, math_utils.degrees_to_radians(max_rotation))
333:   lookat_directions = tf.concat([lookat_direction1, lookat_direction2], 0)
334:   up1 = math_utils.uniform_sampled_vector_within_cone(
335:       tf.constant([[0., 0., 1.]]), math_utils.degrees_to_radians(max_tilt))
336:   up2 = math_utils.uniform_sampled_vector_within_cone(
337:       tf.constant([[0., 0., 1.]]), math_utils.degrees_to_radians(max_tilt))
338:   lookat_rotations = math_utils.lookat_matrix(
339:       tf.concat([up1, up2], 0), lookat_directions)
340:   sample_rotations = tf.matmul(
341:       tf.concat([r1, r2], 0), lookat_rotations, transpose_a=True)
342: 
343:   sampled_views = transformation.rectilinear_projection(
344:       tf.stack([pano1_rgb, pano2_rgb], 0),
345:       [output_height, output_width],"
R3;False_Negative;True_Negative;./files/EvalID_76/dataset.py;363;Missed detection (in ground truth but not detected);"355:   if pano1_depth is not None and pano2_depth is not None:
356:     sampled_depth = transformation.rectilinear_projection(
357:         tf.stack([pano1_depth, pano2_depth], 0),
358:         [output_height, output_width],
359:         output_fov,
360:         sample_rotations)
361: 
362:     fx = output_width*0.5/math.tan(math_utils.degrees_to_radians(output_fov)/2)
363:     intrinsics = tf.constant([[fx, 0., output_width*0.5],
364:                               [0., -fx, output_height*0.5],
365:                               [0., 0., 1.]])
366:     pose1_c2w = tf.concat([lookat_rotations[0], tf.expand_dims(t1, -1)], 1)
367:     pose2_c2w = tf.concat([lookat_rotations[1], tf.expand_dims(t2, -1)], 1)
368:     mask1, mask2 = overlap_mask(sampled_depth[0],
369:                                 pose1_c2w,
370:                                 sampled_depth[1],
371:                                 pose2_c2w,"
R3;False_Negative;True_Negative;./files/EvalID_87/resize.py;203;Missed detection (in ground truth but not detected);"195:   def version_11(cls, node, **kwargs):
196:     # x, roi, scales and sizes are all in NCHW format
197:     tensor_dict = kwargs[""tensor_dict""]
198:     x = tensor_dict[node.inputs[0]]
199:     x_shape = tf_shape(x)
200:     x_dtype = x.dtype
201:     scales = tensor_dict[node.inputs[2]] if node.inputs[2] != """" else None
202:     sizes = tensor_dict[node.inputs[3]] if len(
203:         node.inputs) == 4 else tf.constant([], dtype=tf.int64)
204:     coordinate_transformation_mode = node.attrs.get(
205:         ""coordinate_transformation_mode"", ""half_pixel"")
206:     extrapolation_value = node.attrs.get(""extrapolation_value"", 0.0)
207:     mode = node.attrs.get(""mode"", ""nearest"")
208:     roi = tensor_dict[node.inputs[1]] if node.inputs[1] != """" else None
209:     roi_dtype = roi.dtype if node.inputs[1] != """" else None
210: 
211:     if mode.lower() == ""linear"":"
R3;False_Negative;True_Negative;./files/EvalID_75/project.py;54;Missed detection (in ground truth but not detected);"46:   grid = _meshgrid_abs(img_height, img_width)
47:   grid = tf.tile(tf.expand_dims(grid, 0), [batch_size, 1, 1])
48:   cam_coords = _pixel2cam(depth, grid, intrinsic_mat_inv)
49:   ones = tf.ones([batch_size, 1, img_height * img_width])
50:   cam_coords_hom = tf.concat([cam_coords, ones], axis=1)
51:   egomotion_mat = _egomotion_vec2mat(egomotion, batch_size)
52: 
53:   # Get projection matrix for target camera frame to source pixel frame
54:   hom_filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])
55:   hom_filler = tf.tile(hom_filler, [batch_size, 1, 1])
56:   intrinsic_mat_hom = tf.concat(
57:       [intrinsic_mat, tf.zeros([batch_size, 3, 1])], axis=2)
58:   intrinsic_mat_hom = tf.concat([intrinsic_mat_hom, hom_filler], axis=1)
59:   proj_target_cam_to_source_pixel = tf.matmul(intrinsic_mat_hom, egomotion_mat)
60:   source_pixel_coords = _cam2pixel(cam_coords_hom,
61:                                    proj_target_cam_to_source_pixel)
62:   source_pixel_coords = tf.reshape(source_pixel_coords,"
R3;False_Negative;True_Negative;./files/EvalID_75/project.py;177;Missed detection (in ground truth but not detected);"169:   """"""
170:   translation = tf.slice(vec, [0, 0], [-1, 3])
171:   translation = tf.expand_dims(translation, -1)
172:   rx = tf.slice(vec, [0, 3], [-1, 1])
173:   ry = tf.slice(vec, [0, 4], [-1, 1])
174:   rz = tf.slice(vec, [0, 5], [-1, 1])
175:   rot_mat = _euler2mat(rz, ry, rx)
176:   rot_mat = tf.squeeze(rot_mat, squeeze_dims=[1])
177:   filler = tf.constant([0.0, 0.0, 0.0, 1.0], shape=[1, 1, 4])
178:   filler = tf.tile(filler, [batch_size, 1, 1])
179:   transform_mat = tf.concat([rot_mat, translation], axis=2)
180:   transform_mat = tf.concat([transform_mat, filler], axis=1)
181:   return transform_mat
182: 
183: 
184: def _bilinear_sampler(im, x, y, name='blinear_sampler'):
185:   """"""Perform bilinear sampling on im given list of x, y coordinates."
R3;False_Negative;True_Negative;./files/EvalID_75/project.py;218;Missed detection (in ground truth but not detected);"210:     # Constants.
211:     batch_size = tf.shape(im)[0]
212:     _, height, width, channels = im.get_shape().as_list()
213: 
214:     x = tf.to_float(x)
215:     y = tf.to_float(y)
216:     height_f = tf.cast(height, 'float32')
217:     width_f = tf.cast(width, 'float32')
218:     zero = tf.constant(0, dtype=tf.int32)
219:     max_y = tf.cast(tf.shape(im)[1] - 1, 'int32')
220:     max_x = tf.cast(tf.shape(im)[2] - 1, 'int32')
221: 
222:     # Scale indices from [-1, 1] to [0, width - 1] or [0, height - 1].
223:     x = (x + 1.0) * (width_f - 1.0) / 2.0
224:     y = (y + 1.0) * (height_f - 1.0) / 2.0
225: 
226:     # Compute the coordinates of the 4 pixels to sample from."
R3;False_Negative;True_Negative;./files/EvalID_97/hed.py;85;Missed detection (in ground truth but not detected);"77:         for x in range(s):
78:             for y in range(s):
79:                 ret[x, y] = (1 - abs(x / f - c)) * (1 - abs(y / f - c))
80:         return ret
81: 
82:     w = bilinear_conv_filler(filter_shape)
83:     w = np.repeat(w, ch * ch).reshape((filter_shape, filter_shape, ch, ch))
84: 
85:     weight_var = tf.constant(w, tf.float32,
86:                              shape=(filter_shape, filter_shape, ch, ch),
87:                              name='bilinear_upsample_filter')
88:     x = tf.pad(x, [[0, 0], [0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1]], mode='SYMMETRIC')
89:     out_shape = tf.shape(x) * tf.constant([1, 1, shape, shape], tf.int32)
90:     deconv = tf.nn.conv2d_transpose(x, weight_var, out_shape,
91:                                     [1, 1, shape, shape], 'SAME', data_format='NCHW')
92:     edge = shape * (shape - 1)
93:     deconv = deconv[:, :, edge:-edge, edge:-edge]"
R3;False_Negative;True_Negative;./files/EvalID_97/hed.py;89;Missed detection (in ground truth but not detected);"81: 
82:     w = bilinear_conv_filler(filter_shape)
83:     w = np.repeat(w, ch * ch).reshape((filter_shape, filter_shape, ch, ch))
84: 
85:     weight_var = tf.constant(w, tf.float32,
86:                              shape=(filter_shape, filter_shape, ch, ch),
87:                              name='bilinear_upsample_filter')
88:     x = tf.pad(x, [[0, 0], [0, 0], [shape - 1, shape - 1], [shape - 1, shape - 1]], mode='SYMMETRIC')
89:     out_shape = tf.shape(x) * tf.constant([1, 1, shape, shape], tf.int32)
90:     deconv = tf.nn.conv2d_transpose(x, weight_var, out_shape,
91:                                     [1, 1, shape, shape], 'SAME', data_format='NCHW')
92:     edge = shape * (shape - 1)
93:     deconv = deconv[:, :, edge:-edge, edge:-edge]
94: 
95:     if inp_shape[2]:
96:         inp_shape[2] *= shape
97:     if inp_shape[3]:"
R3;False_Negative;True_Negative;./files/EvalID_97/hed.py;109;Missed detection (in ground truth but not detected);"101: 
102: 
103: class Model(ModelDesc):
104:     def inputs(self):
105:         return [tf.TensorSpec([None, None, None, 3], tf.float32, 'image'),
106:                 tf.TensorSpec([None, None, None], tf.int32, 'edgemap')]
107: 
108:     def build_graph(self, image, edgemap):
109:         image = image - tf.constant([104, 116, 122], dtype='float32')
110:         image = tf.transpose(image, [0, 3, 1, 2])
111:         edgemap = tf.expand_dims(edgemap, 3, name='edgemap4d')
112: 
113:         def branch(name, l, up):
114:             with tf.variable_scope(name):
115:                 l = Conv2D('convfc', l, 1, kernel_size=1, activation=tf.identity,
116:                            use_bias=True,
117:                            kernel_initializer=tf.constant_initializer())"
R3;False_Negative;True_Negative;./files/EvalID_97/hed.py;117;Missed detection (in ground truth but not detected);"109:         image = image - tf.constant([104, 116, 122], dtype='float32')
110:         image = tf.transpose(image, [0, 3, 1, 2])
111:         edgemap = tf.expand_dims(edgemap, 3, name='edgemap4d')
112: 
113:         def branch(name, l, up):
114:             with tf.variable_scope(name):
115:                 l = Conv2D('convfc', l, 1, kernel_size=1, activation=tf.identity,
116:                            use_bias=True,
117:                            kernel_initializer=tf.constant_initializer())
118:                 while up != 1:
119:                     l = CaffeBilinearUpSample('upsample{}'.format(up), l, 2)
120:                     up = up // 2
121:                 return l
122: 
123:         with argscope(Conv2D, kernel_size=3, activation=tf.nn.relu), \
124:                 argscope([Conv2D, MaxPooling], data_format='NCHW'):
125:             l = Conv2D('conv1_1', image, 64)"
R3;False_Negative;True_Negative;./files/EvalID_97/hed.py;154;Missed detection (in ground truth but not detected);"146: 
147:             l = Conv2D('conv5_1', l, 512)
148:             l = Conv2D('conv5_2', l, 512)
149:             l = Conv2D('conv5_3', l, 512)
150:             b5 = branch('branch5', l, 16)
151: 
152:             final_map = Conv2D('convfcweight',
153:                                tf.concat([b1, b2, b3, b4, b5], 1), 1, kernel_size=1,
154:                                kernel_initializer=tf.constant_initializer(0.2),
155:                                use_bias=False, activation=tf.identity)
156:         costs = []
157:         for idx, b in enumerate([b1, b2, b3, b4, b5, final_map]):
158:             b = tf.transpose(b, [0, 2, 3, 1])
159:             output = tf.nn.sigmoid(b, name='output{}'.format(idx + 1))
160:             xentropy = class_balanced_sigmoid_cross_entropy(
161:                 b, edgemap,
162:                 name='xentropy{}'.format(idx + 1))"
R3;False_Negative;True_Negative;./files/EvalID_74/FISM.py;60;Missed detection (in ground truth but not detected);"52:             else:
53:                 self.labels = tf.placeholder(tf.float32, shape=[None], name=""labels"")
54: 
55:     def _create_variables(self):
56:         with tf.name_scope(""embedding""):  # The embedding initialization is unknown now
57:             initializer = tool.get_initializer(self.init_method, self.stddev)
58:             self.c1 = tf.Variable(initializer([self.num_items, self.embedding_size]),
59:                                   name='c1', dtype=tf.float32)
60:             self.c2 = tf.constant(0.0, tf.float32, [1, self.embedding_size], name='c2')
61:             self.embedding_Q_ = tf.concat([self.c1,self.c2], 0, name='embedding_Q_')
62:             self.embedding_Q = tf.Variable(initializer([self.num_items, self.embedding_size]),
63:                                            name='embedding_Q', dtype=tf.float32)
64:             self.bias = tf.Variable(tf.zeros(self.num_items),name='bias')
65: 
66:     def _create_inference(self, user_input, item_input, num_idx):
67:         with tf.name_scope(""inference""):
68:             embedding_p = tf.reduce_sum(tf.nn.embedding_lookup(self.embedding_Q_, user_input), 1)"
R3;False_Negative;True_Negative;./files/EvalID_74/FISM.py;71;Missed detection (in ground truth but not detected);"63:                                            name='embedding_Q', dtype=tf.float32)
64:             self.bias = tf.Variable(tf.zeros(self.num_items),name='bias')
65: 
66:     def _create_inference(self, user_input, item_input, num_idx):
67:         with tf.name_scope(""inference""):
68:             embedding_p = tf.reduce_sum(tf.nn.embedding_lookup(self.embedding_Q_, user_input), 1)
69:             embedding_q = tf.nn.embedding_lookup(self.embedding_Q, item_input)
70:             bias_i = tf.nn.embedding_lookup(self.bias, item_input)
71:             coeff = tf.pow(num_idx, -tf.constant(self.alpha, tf.float32, [1]))
72:             output = coeff * tf.reduce_sum(tf.multiply(embedding_p,embedding_q), 1) + bias_i
73:         return embedding_p, embedding_q, output
74: 
75:     def _create_loss(self):
76:         with tf.name_scope(""loss""):
77:             p1, q1, self.output = self._create_inference(self.user_input, self.item_input, self.num_idx)
78:             if self.is_pairwise is True:
79:                 _, q2, output_neg = self._create_inference(self.user_input_neg, self.item_input_neg, self.num_idx_neg)"
R3;False_Negative;True_Negative;./files/EvalID_69/vpn_pixels.py;137;Missed detection (in ground truth but not detected);"129:                 self.embedding_dim, use_bias=True
130:             ),  # , kernel_regularizer=tf.keras.regularizers.l2(WEIGHT_DECAY)
131:             tf.keras.layers.ReLU(),
132:             tf.keras.layers.Dense(self.embedding_dim)
133:         ],
134:         name='action_encoder')
135: 
136:     if self.num_augmentations == 0:
137:       dummy_state = tf.constant(
138:           np.zeros(shape=[1] + list(observation_spec.shape)))
139:       self.obs_spec = list(observation_spec.shape)
140:     else:  # account for padding of +4 everywhere and then cropping out 68
141:       dummy_state = tf.constant(np.zeros(shape=[1, 68, 68, 3]))
142:       self.obs_spec = [68, 68, 3]
143: 
144:     @tf.function
145:     def init_models():"
R3;False_Negative;True_Negative;./files/EvalID_69/vpn_pixels.py;141;Missed detection (in ground truth but not detected);"133:         ],
134:         name='action_encoder')
135: 
136:     if self.num_augmentations == 0:
137:       dummy_state = tf.constant(
138:           np.zeros(shape=[1] + list(observation_spec.shape)))
139:       self.obs_spec = list(observation_spec.shape)
140:     else:  # account for padding of +4 everywhere and then cropping out 68
141:       dummy_state = tf.constant(np.zeros(shape=[1, 68, 68, 3]))
142:       self.obs_spec = [68, 68, 3]
143: 
144:     @tf.function
145:     def init_models():
146:       critic_kwargs['encoder'](dummy_state)
147:       critic_kwargs['encoder_target'](dummy_state)
148:       self.action_encoder(
149:           tf.cast(tf.one_hot([1], depth=action_dim), tf.float32))"
R3;False_Negative;True_Negative;./files/EvalID_69/vpn_pixels.py;466;Missed detection (in ground truth but not detected);"458: 
459:   @tf.function
460:   def act(self, states, data_aug=False):
461:     if data_aug and self.num_augmentations > 0:
462:       states = states[0]
463:     if self.num_augmentations > 0:
464:       # use pad of 2 to bump 64 to 68 with 2 + 64 + 2 on each side
465:       img_pad = 2
466:       paddings = tf.constant(
467:           [[0, 0], [img_pad, img_pad], [img_pad, img_pad], [0, 0]],
468:           dtype=tf.int32)
469:       states = tf.cast(
470:           tf.pad(tf.cast(states * 255., tf.int32), paddings, 'SYMMETRIC'),
471:           tf.float32) / 255.
472: 
473:     q1, q2 = self.critic(states, stop_grad_features=True, actions=None)
474:     q = tf.minimum(q1, q2)"
R3;False_Negative;True_Negative;./files/EvalID_80/NAIS.py;79;Missed detection (in ground truth but not detected);"71:                 self.embedding_Q = tf.Variable(embed_initializer([self.num_items, self.embedding_size]),
72:                                                name='embedding_Q', dtype=tf.float32)
73:                 self.bias = tf.Variable(tf.zeros(self.num_items), name='bias')
74:             else:
75:                 self.c1 = tf.Variable(params[0], name='c1', dtype=tf.float32)
76:                 self.embedding_Q = tf.Variable(params[1], name='embedding_Q', dtype=tf.float32)
77:                 self.bias = tf.Variable(params[2], name=""bias"", dtype=tf.float32)
78: 
79:             self.c2 = tf.constant(0.0, tf.float32, [1, self.embedding_size], name='c2')
80:             self.embedding_Q_ = tf.concat([self.c1, self.c2], axis=0, name='embedding_Q_')
81: 
82:             # Variables for attention
83:             weight_initializer = tool.get_initializer(self.weight_init_method, self.stddev)
84:             if self.algorithm == 0:
85:                 self.W = tf.Variable(weight_initializer([self.embedding_size, self.weight_size]),
86:                                      name='Weights_for_MLP', dtype=tf.float32, trainable=True)
87:             else:"
R3;False_Negative;True_Negative;./files/EvalID_80/NAIS.py;110;Missed detection (in ground truth but not detected);"102:                 embedding_p = self._attention_mlp(embedding_q_ * embedding_q, embedding_q_, num_idx)
103:             else:
104:                 n = tf.shape(user_input)[1]
105:                 embedding_p = self._attention_mlp(tf.concat([embedding_q_, tf.tile(embedding_q, tf.stack([1, n, 1]))], 2),
106:                                                   embedding_q_, num_idx)
107: 
108:             embedding_q = tf.reduce_sum(embedding_q, 1)
109:             bias_i = tf.nn.embedding_lookup(self.bias, item_input)
110:             coeff = tf.pow(num_idx, tf.constant(self.alpha, tf.float32, [1]))
111:             output = coeff * tf.reduce_sum(embedding_p*embedding_q, 1) + bias_i
112: 
113:             return embedding_q_, embedding_q, output
114: 
115:     def _create_loss(self):
116:         with tf.name_scope(""loss""):
117:             p1, q1, self.output = self._create_inference(self.user_input,self.item_input,self.num_idx)
118:             if self.is_pairwise is True:"
R3;False_Negative;True_Negative;./files/EvalID_80/NAIS.py;172;Missed detection (in ground truth but not detected);"164: 
165:                 A_ = tf.reshape(tf.matmul(mlp_output, self.h), [b,n])  # (b*n, w) * (w, 1) => (None, 1) => (b, n)
166: 
167:                 # softmax for not mask features
168:                 exp_A_ = tf.exp(A_)
169:                 mask_mat = tf.sequence_mask(num_idx, maxlen = n, dtype=tf.float32)  # (b, n)
170:                 exp_A_ = mask_mat * exp_A_
171:                 exp_sum = tf.reduce_sum(exp_A_, 1, keepdims=True)  # (b, 1)
172:                 exp_sum = tf.pow(exp_sum, tf.constant(self.beta, tf.float32, [1]))
173: 
174:                 A = tf.expand_dims(tf.div(exp_A_, exp_sum), 2)  # (b, n, 1)
175: 
176:                 return tf.reduce_sum(A * embedding_q_, 1)
177: 
178:     def train_model(self):
179:         self.logger.info(self.evaluator.metrics_info())
180:         for epoch in range(1, self.num_epochs+1):"
R3;False_Negative;True_Negative;./files/EvalID_72/model.py;98;Missed detection (in ground truth but not detected);"90: 
91: # Create serving input function to be able to serve predictions later using provided inputs
92: def serving_input_fn():
93:     feature_placeholders = {
94:         'is_male': tf.placeholder(tf.string, [None]),
95:         'mother_age': tf.placeholder(tf.float32, [None]),
96:         'plurality': tf.placeholder(tf.string, [None]),
97:         'gestation_weeks': tf.placeholder(tf.float32, [None]),
98:         KEY_COLUMN: tf.placeholder_with_default(tf.constant(['nokey']), [None])
99:     }
100:     features = {
101:         key: tf.expand_dims(tensor, -1)
102:         for key, tensor in feature_placeholders.items()
103:     }
104:     return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)
105: 
106: # create metric for hyperparameter tuning"
R3;False_Negative;True_Negative;./files/EvalID_70/preprocess_legacy.py;90;Missed detection (in ground truth but not detected);"82:     tf.logging.info('Use customized resize method {}'.format(method))
83:     return tf.image.resize([image], [image_size, image_size], method)[0]
84:   tf.logging.info('Use default resize_bicubic.')
85:   return tf.image.resize_bicubic([image], [image_size, image_size])[0]
86: 
87: 
88: def _decode_and_random_crop(image_bytes, image_size, resize_method=None):
89:   """"""Make a random crop of image_size.""""""
90:   bbox = tf.constant([0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
91:   image = distorted_bounding_box_crop(
92:       image_bytes,
93:       bbox,
94:       min_object_covered=0.1,
95:       aspect_ratio_range=(3. / 4, 4. / 3.),
96:       area_range=(0.08, 1.0),
97:       max_attempts=10,
98:       scope=None)"
R3;False_Negative;True_Negative;./files/EvalID_70/preprocess_legacy.py;242;Missed detection (in ground truth but not detected);"234:                                  resize_method)
235:   else:
236:     image = preprocess_for_eval(image_bytes, image_size, resize_method)
237: 
238:   # Normalize images.
239:   image = tf.image.convert_image_dtype(image, dtype=image_dtype or tf.float32)
240:   mean_rgb = [0.485 * 255, 0.456 * 255, 0.406 * 255]
241:   stddev_rgb = [0.229 * 255, 0.224 * 255, 0.225 * 255]
242:   image -= tf.constant(mean_rgb, shape=(1, 1, 3), dtype=image.dtype)
243:   image /= tf.constant(stddev_rgb, shape=(1, 1, 3), dtype=image.dtype)
244:   return image"
R3;False_Negative;True_Negative;./files/EvalID_70/preprocess_legacy.py;243;Missed detection (in ground truth but not detected);"235:   else:
236:     image = preprocess_for_eval(image_bytes, image_size, resize_method)
237: 
238:   # Normalize images.
239:   image = tf.image.convert_image_dtype(image, dtype=image_dtype or tf.float32)
240:   mean_rgb = [0.485 * 255, 0.456 * 255, 0.406 * 255]
241:   stddev_rgb = [0.229 * 255, 0.224 * 255, 0.225 * 255]
242:   image -= tf.constant(mean_rgb, shape=(1, 1, 3), dtype=image.dtype)
243:   image /= tf.constant(stddev_rgb, shape=(1, 1, 3), dtype=image.dtype)
244:   return image"
R8;True_Positive;True_Positive;./files/EvalID_67/base_model.py;127;Correctly detected;"119: 
120:     def __init__(self, cfg, *args, **kwargs):
121:         super().__init__(cfg, *args, **kwargs)
122:         # loss weight
123:         self.w = cfg.MODEL.HYPER_PARAMS[0]
124: 
125:     def training_step(self, batch, batch_idx):
126:         ori_text, cor_text, det_labels = batch
127:         outputs = self.forward(ori_text, cor_text, det_labels)
128:         loss = self.w * outputs[1] + (1 - self.w) * outputs[0]
129:         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=len(ori_text))
130:         return loss
131: 
132:     def validation_step(self, batch, batch_idx):
133:         ori_text, cor_text, det_labels = batch
134:         outputs = self.forward(ori_text, cor_text, det_labels)
135:         loss = self.w * outputs[1] + (1 - self.w) * outputs[0]"
R8;True_Positive;True_Positive;./files/EvalID_67/base_model.py;134;Correctly detected;"126:         ori_text, cor_text, det_labels = batch
127:         outputs = self.forward(ori_text, cor_text, det_labels)
128:         loss = self.w * outputs[1] + (1 - self.w) * outputs[0]
129:         self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=len(ori_text))
130:         return loss
131: 
132:     def validation_step(self, batch, batch_idx):
133:         ori_text, cor_text, det_labels = batch
134:         outputs = self.forward(ori_text, cor_text, det_labels)
135:         loss = self.w * outputs[1] + (1 - self.w) * outputs[0]
136:         det_y_hat = (outputs[2] > 0.5).long()
137:         cor_y_hat = torch.argmax((outputs[3]), dim=-1)
138:         encoded_x = self.tokenizer(cor_text, padding=True, return_tensors='pt')
139:         encoded_x.to(self._device)
140:         cor_y = encoded_x['input_ids']
141:         cor_y_hat *= encoded_x['attention_mask']
142:"
R8;True_Positive;True_Positive;./files/EvalID_67/base_model.py;183;Correctly detected;"175:     def test_epoch_end(self, outputs) -> None:
176:         logger.info('Test.')
177:         self.validation_epoch_end(outputs)
178: 
179:     def predict(self, texts):
180:         inputs = self.tokenizer(texts, padding=True, return_tensors='pt')
181:         inputs.to(self.cfg.MODEL.DEVICE)
182:         with torch.no_grad():
183:             outputs = self.forward(texts)
184:             y_hat = torch.argmax(outputs[1], dim=-1)
185:             expand_text_lens = torch.sum(inputs['attention_mask'], dim=-1) - 1
186:         rst = []
187:         for t_len, _y_hat in zip(expand_text_lens, y_hat):
188:             rst.append(self.tokenizer.decode(_y_hat[1:t_len]).replace(' ', ''))
189:         return rst"
R8;True_Positive;True_Positive;./files/EvalID_83/base_model.py;38;Correctly detected;"30:         else:
31:             x = feat
32:         train_mask, val_mask = self.data.train_mask, self.data.val_mask
33:         best_loss_val = 100
34:         best_acc_val = 0
35:         for i in range(train_iters):
36:             self.train()
37:             optimizer.zero_grad()
38:             output = self.forward(x, edge_index, edge_weight)
39:             loss_train = F.nll_loss(output[train_mask], labels[train_mask])
40:             loss_train.backward()
41:             optimizer.step()
42: 
43:             if verbose and i % 50 == 0:
44:                 print('Epoch {}, training loss: {}'.format(i, loss_train.item()))
45: 
46:             self.eval()"
R8;True_Positive;True_Positive;./files/EvalID_83/base_model.py;48;Correctly detected;"40:             loss_train.backward()
41:             optimizer.step()
42: 
43:             if verbose and i % 50 == 0:
44:                 print('Epoch {}, training loss: {}'.format(i, loss_train.item()))
45: 
46:             self.eval()
47:             with torch.no_grad():
48:                 output = self.forward(x, edge_index)
49:             loss_val = F.nll_loss(output[val_mask], labels[val_mask])
50:             acc_val = utils.accuracy(output[val_mask], labels[val_mask])
51: 
52:             # if best_loss_val > loss_val:
53:             #     best_loss_val = loss_val
54:             #     best_output = output
55:             #     weights = deepcopy(self.state_dict())
56:"
R8;True_Positive;True_Positive;./files/EvalID_83/base_model.py;84;Correctly detected;"76: 
77:         labels = self.data.y
78:         train_mask, val_mask = self.data.train_mask, self.data.val_mask
79: 
80:         x, edge_index = self.data.x, self.data.edge_index
81:         for i in range(train_iters):
82:             self.train()
83:             optimizer.zero_grad()
84:             output = self.forward(x, edge_index)
85:             loss_train = F.nll_loss(output[train_mask+val_mask], labels[train_mask+val_mask])
86:             loss_train.backward()
87:             optimizer.step()
88: 
89:             if verbose and i % 50 == 0:
90:                 print('Epoch {}, training loss: {}'.format(i, loss_train.item()))
91: 
92:     def fit_with_val(self, pyg_data, train_iters=1000, initialize=True, patience=100, verbose=False, **kwargs):"
R8;True_Positive;True_Positive;./files/EvalID_83/base_model.py;121;Correctly detected;"113:         best_acc_val = 0
114:         best_epoch = 0
115: 
116:         x, edge_index = self.data.x, self.data.edge_index
117:         for i in range(train_iters):
118:             self.train()
119:             optimizer.zero_grad()
120: 
121:             output = self.forward(x, edge_index)
122: 
123:             loss_train = F.nll_loss(output[train_mask], labels[train_mask])
124:             loss_train.backward()
125:             optimizer.step()
126: 
127:             if verbose and i % 50 == 0:
128:                 print('Epoch {}, training loss: {}'.format(i, loss_train.item()))
129:"
R8;True_Positive;True_Positive;./files/EvalID_83/base_model.py;131;Correctly detected;"123:             loss_train = F.nll_loss(output[train_mask], labels[train_mask])
124:             loss_train.backward()
125:             optimizer.step()
126: 
127:             if verbose and i % 50 == 0:
128:                 print('Epoch {}, training loss: {}'.format(i, loss_train.item()))
129: 
130:             self.eval()
131:             output = self.forward(x, edge_index)
132:             loss_val = F.nll_loss(output[val_mask], labels[val_mask])
133:             acc_val = utils.accuracy(output[val_mask], labels[val_mask])
134:             # print(acc)
135: 
136:             # if best_loss_val > loss_val:
137:             #     best_loss_val = loss_val
138:             #     self.output = output
139:             #     weights = deepcopy(self.state_dict())"
R8;True_Positive;True_Positive;./files/EvalID_83/base_model.py;172;Correctly detected;"164:         Parameters
165:         ----------
166:         idx_test :
167:             node testing indices
168:         """"""
169:         self.eval()
170:         test_mask = self.data.test_mask
171:         labels = self.data.y
172:         output = self.forward(self.data.x, self.data.edge_index)
173:         # output = self.output
174:         loss_test = F.nll_loss(output[test_mask], labels[test_mask])
175:         acc_test = utils.accuracy(output[test_mask], labels[test_mask])
176:         print(""Test set results:"",
177:               ""loss= {:.4f}"".format(loss_test.item()),
178:               ""accuracy= {:.4f}"".format(acc_test.item()))
179:         return acc_test.item()
180:"
R8;True_Positive;True_Positive;./files/EvalID_83/base_model.py;191;Correctly detected;"183:         Returns
184:         -------
185:         torch.FloatTensor
186:             output (log probabilities)
187:         """"""
188:         self.eval()
189:         if x is None or edge_index is None:
190:             x, edge_index = self.data.x, self.data.edge_index
191:         return self.forward(x, edge_index, edge_weight)
192: 
193:     def _ensure_contiguousness(self,
194:                                x,
195:                                edge_idx,
196:                                edge_weight):
197:         if not x.is_sparse:
198:             x = x.contiguous()
199:         if hasattr(edge_idx, 'contiguous'):"
R8;False_Positive;True_Positive;./files/EvalID_64/scripted_seq2seq_generator.py;121;Incorrectly detected (not in ground truth);"113:                 tensors[""dict_weights""],
114:                 tensors[""dict_lengths""],
115:             )
116: 
117:         contextual_token_embedding: Optional[torch.Tensor] = None
118:         if ""contextual_token_embedding"" in tensors:
119:             contextual_token_embedding = tensors[""contextual_token_embedding""]
120: 
121:         hypos_etc = self.forward(
122:             actual_src_tokens,
123:             dict_feat,
124:             contextual_token_embedding,
125:             tensors[""src_lengths""],
126:         )
127:         predictions = [[pred for pred, _, _, _, _ in hypos_etc]]
128:         scores = [[score for _, score, _, _, _ in hypos_etc]]
129:         return (predictions, scores)"
R8;False_Positive;True_Positive;./files/EvalID_61/ppo_continuous_multiprocess.py;152;Incorrectly detected (not in ground truth);"144:         if state.is_cuda:
145:             zeros = zeros.cuda()
146:         log_std = self.log_std(zeros)
147: 
148:         return mean, log_std
149: 
150:     def get_action(self, state, deterministic=False):
151:         state = torch.FloatTensor(state).unsqueeze(0).to(device)
152:         mean, log_std = self.forward(state)
153:         std = log_std.exp()
154:         normal = Normal(mean, std)
155:         action = normal.sample()
156:         action = torch.clamp(action, -self.action_range, self.action_range)
157:         return action.squeeze(0)
158: 
159:     def sample_action(self,):
160:         a=torch.FloatTensor(self.num_actions).uniform_(-1, 1)"
R8;False_Positive;True_Positive;./files/EvalID_90/sac_pendulum.py;142;Incorrectly detected (not in ground truth);"134: 
135:         mean    = self.mean_linear(x)
136:         log_std = self.log_std_linear(x)
137:         log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)
138: 
139:         return mean, log_std
140: 
141:     def evaluate(self, state, epsilon=1e-6):
142:         mean, log_std = self.forward(state)
143:         std = log_std.exp()
144: 
145:         normal = Normal(0, 1)
146:         z      = normal.sample(mean.shape)
147:         action = torch.tanh(mean+ std*z.to(device))
148:         log_prob = Normal(mean, std).log_prob(mean+ std*z.to(device)) - torch.log(1 - action.pow(2) + epsilon)
149:         log_prob = log_prob.sum(dim=-1, keepdim=True)
150:         return action, log_prob, z, mean, log_std"
R8;False_Positive;True_Positive;./files/EvalID_90/sac_pendulum.py;155;Incorrectly detected (not in ground truth);"147:         action = torch.tanh(mean+ std*z.to(device))
148:         log_prob = Normal(mean, std).log_prob(mean+ std*z.to(device)) - torch.log(1 - action.pow(2) + epsilon)
149:         log_prob = log_prob.sum(dim=-1, keepdim=True)
150:         return action, log_prob, z, mean, log_std
151: 
152: 
153:     def get_action(self, state):
154:         state = torch.FloatTensor(state).unsqueeze(0).to(device)
155:         mean, log_std = self.forward(state)
156:         std = log_std.exp()
157: 
158:         normal = Normal(0, 1)
159:         z      = normal.sample(mean.shape).to(device)
160:         action = torch.tanh(mean + std*z)
161: 
162:         action  = action.cpu()#.detach().cpu().numpy()
163:         return action[0]"
R8;False_Positive;True_Positive;./files/EvalID_68/mnist_ptl_mini.py;92;Incorrectly detected (not in ground truth);"84:         x = torch.log_softmax(x, dim=1)
85:         return x
86: 
87:     def configure_optimizers(self):
88:         return torch.optim.Adam(self.parameters(), lr=self.lr)
89: 
90:     def training_step(self, train_batch, batch_idx):
91:         x, y = train_batch
92:         logits = self.forward(x)
93:         loss = F.nll_loss(logits, y)
94:         acc = self.accuracy(logits, y)
95:         self.log(""ptl/train_loss"", loss)
96:         self.log(""ptl/train_accuracy"", acc)
97:         return loss
98: 
99:     def validation_step(self, val_batch, batch_idx):
100:         x, y = val_batch"
R8;False_Positive;True_Positive;./files/EvalID_68/mnist_ptl_mini.py;101;Incorrectly detected (not in ground truth);"93:         loss = F.nll_loss(logits, y)
94:         acc = self.accuracy(logits, y)
95:         self.log(""ptl/train_loss"", loss)
96:         self.log(""ptl/train_accuracy"", acc)
97:         return loss
98: 
99:     def validation_step(self, val_batch, batch_idx):
100:         x, y = val_batch
101:         logits = self.forward(x)
102:         loss = F.nll_loss(logits, y)
103:         acc = self.accuracy(logits, y)
104:         return {""val_loss"": loss, ""val_accuracy"": acc}
105: 
106:     def validation_epoch_end(self, outputs):
107:         avg_loss = torch.stack([x[""val_loss""] for x in outputs]).mean()
108:         avg_acc = torch.stack([x[""val_accuracy""] for x in outputs]).mean()
109:         self.log(""ptl/val_loss"", avg_loss)"
R8;False_Positive;True_Positive;./files/EvalID_65/sgc.py;124;Incorrectly detected (not in ground truth);"116:         train_mask, val_mask = self.data.train_mask, self.data.val_mask
117: 
118:         early_stopping = patience
119:         best_loss_val = 100
120: 
121:         for i in range(train_iters):
122:             self.train()
123:             optimizer.zero_grad()
124:             output = self.forward(self.data)
125: 
126:             loss_train = F.nll_loss(output[train_mask], labels[train_mask])
127:             loss_train.backward()
128:             optimizer.step()
129: 
130:             if verbose and i % 10 == 0:
131:                 print('Epoch {}, training loss: {}'.format(i, loss_train.item()))
132:"
R8;False_Positive;True_Positive;./files/EvalID_65/sgc.py;134;Incorrectly detected (not in ground truth);"126:             loss_train = F.nll_loss(output[train_mask], labels[train_mask])
127:             loss_train.backward()
128:             optimizer.step()
129: 
130:             if verbose and i % 10 == 0:
131:                 print('Epoch {}, training loss: {}'.format(i, loss_train.item()))
132: 
133:             self.eval()
134:             output = self.forward(self.data)
135:             loss_val = F.nll_loss(output[val_mask], labels[val_mask])
136: 
137:             if best_loss_val > loss_val:
138:                 best_loss_val = loss_val
139:                 self.output = output
140:                 weights = deepcopy(self.state_dict())
141:                 patience = early_stopping
142:             else:"
R8;False_Positive;True_Positive;./files/EvalID_65/sgc.py;162;Incorrectly detected (not in ground truth);"154:         Parameters
155:         ----------
156:         idx_test :
157:             node testing indices
158:         """"""
159:         self.eval()
160:         test_mask = self.data.test_mask
161:         labels = self.data.y
162:         output = self.forward(self.data)
163:         # output = self.output
164:         loss_test = F.nll_loss(output[test_mask], labels[test_mask])
165:         acc_test = utils.accuracy(output[test_mask], labels[test_mask])
166:         print(""Test set results:"",
167:               ""loss= {:.4f}"".format(loss_test.item()),
168:               ""accuracy= {:.4f}"".format(acc_test.item()))
169:         return acc_test.item()
170:"
R8;False_Positive;True_Positive;./files/EvalID_65/sgc.py;180;Incorrectly detected (not in ground truth);"172:         """"""
173:         Returns
174:         -------
175:         torch.FloatTensor
176:             output (log probabilities) of SGC
177:         """"""
178: 
179:         self.eval()
180:         return self.forward(self.data)
181: 
182: 
183: if __name__ == ""__main__"":
184:     from deeprobust.graph.data import Dataset, Dpr2Pyg
185:     # from deeprobust.graph.defense import SGC
186:     data = Dataset(root='/tmp/', name='cora')
187:     adj, features, labels = data.adj, data.features, data.labels
188:     idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test"
R8;False_Positive;True_Positive;./files/EvalID_28/deepfool.py;92;Incorrectly detected (not in ground truth);"84:             maximum number of iteration for deepfool (default = 50)
85:         """"""
86:         self.num_classes = num_classes
87:         self.overshoot = overshoot
88:         self.max_iteration = max_iteration
89:         return True
90: 
91: def deepfool(model, image, num_classes, overshoot, max_iter, device):
92:     f_image = model.forward(image).data.cpu().numpy().flatten()
93:     output = (np.array(f_image)).flatten().argsort()[::-1]
94: 
95:     output = output[0:num_classes]
96:     label = output[0]
97: 
98:     input_shape = image.cpu().numpy().shape
99:     x = copy.deepcopy(image).requires_grad_(True)
100:     w = np.zeros(input_shape)"
R8;False_Positive;True_Positive;./files/EvalID_28/deepfool.py;103;Incorrectly detected (not in ground truth);"95:     output = output[0:num_classes]
96:     label = output[0]
97: 
98:     input_shape = image.cpu().numpy().shape
99:     x = copy.deepcopy(image).requires_grad_(True)
100:     w = np.zeros(input_shape)
101:     r_tot = np.zeros(input_shape)
102: 
103:     fs = model.forward(x)
104:     fs_list = [fs[0,output[k]] for k in range(num_classes)]
105:     current_pred_label = label
106: 
107:     for i in range(max_iter):
108: 
109:         pert = np.inf
110:         fs[0, output[0]].backward(retain_graph = True)
111:         grad_orig = x.grad.data.cpu().numpy().copy()"
R8;False_Positive;True_Positive;./files/EvalID_28/deepfool.py;138;Incorrectly detected (not in ground truth);"130:         # compute r_i and r_tot
131:         # Added 1e-4 for numerical stability
132:         r_i =  (pert+1e-4) * w / np.linalg.norm(w)
133:         r_tot = np.float32(r_tot + r_i)
134: 
135:         pert_image = image + (1+overshoot)*torch.from_numpy(r_tot).to(device)
136: 
137:         x = pert_image.detach().requires_grad_(True)
138:         fs = model.forward(x)
139: 
140:         if (not np.argmax(fs.data.cpu().numpy().flatten()) == label):
141:             break
142: 
143: 
144:     r_tot = (1+overshoot)*r_tot
145: 
146:     return pert_image, r_tot, i"
R8;False_Positive;True_Positive;./files/EvalID_66/ac.py;150;Incorrectly detected (not in ground truth);"142:             return x
143: 
144:     def select_action(self, state):
145:         '''
146:         only select action without the purpose of gradients flow, for interaction with env to
147:         generate samples
148:         '''
149:         if DETERMINISTIC:
150:             action = self.forward(state)
151: 
152:         if DISCRETE and not DETERMINISTIC:
153:             probs = self.forward(state)
154:             m = Categorical(probs)
155:             action = m.sample()
156: 
157:         if not DISCRETE and not DETERMINISTIC:
158:             self.action_range = 30."
R8;False_Positive;True_Positive;./files/EvalID_66/ac.py;153;Incorrectly detected (not in ground truth);"145:         '''
146:         only select action without the purpose of gradients flow, for interaction with env to
147:         generate samples
148:         '''
149:         if DETERMINISTIC:
150:             action = self.forward(state)
151: 
152:         if DISCRETE and not DETERMINISTIC:
153:             probs = self.forward(state)
154:             m = Categorical(probs)
155:             action = m.sample()
156: 
157:         if not DISCRETE and not DETERMINISTIC:
158:             self.action_range = 30.
159: 
160:             mean, log_std = self.forward(state)
161:             std = log_std.exp()"
R8;False_Positive;True_Positive;./files/EvalID_66/ac.py;160;Incorrectly detected (not in ground truth);"152:         if DISCRETE and not DETERMINISTIC:
153:             probs = self.forward(state)
154:             m = Categorical(probs)
155:             action = m.sample()
156: 
157:         if not DISCRETE and not DETERMINISTIC:
158:             self.action_range = 30.
159: 
160:             mean, log_std = self.forward(state)
161:             std = log_std.exp()
162:             normal = Normal(0, 1)
163:             z = normal.sample().to(device)
164:             action = self.action_range* torch.tanh(mean + std*z)
165: 
166:         return action.detach()
167: 
168:"
R8;False_Positive;True_Positive;./files/EvalID_66/ac.py;175;Incorrectly detected (not in ground truth);"167: 
168: 
169:     def evaluate_action(self, state):
170:         '''
171:         evaluate action within GPU graph, for gradients flowing through it
172:         '''
173:         state = torch.FloatTensor(state).unsqueeze(0).to(device) # state dim: (N, dim of state)
174:         if DETERMINISTIC:
175:             action = self.forward(state)
176:             return action.detach().cpu().numpy()
177: 
178:         elif DISCRETE and not DETERMINISTIC:  # actor-critic (discrete)
179:             probs = self.forward(state)
180:             m = Categorical(probs)
181:             action = m.sample().to(device)
182:             log_prob = m.log_prob(action)
183:"
R8;False_Positive;True_Positive;./files/EvalID_66/ac.py;179;Incorrectly detected (not in ground truth);"171:         evaluate action within GPU graph, for gradients flowing through it
172:         '''
173:         state = torch.FloatTensor(state).unsqueeze(0).to(device) # state dim: (N, dim of state)
174:         if DETERMINISTIC:
175:             action = self.forward(state)
176:             return action.detach().cpu().numpy()
177: 
178:         elif DISCRETE and not DETERMINISTIC:  # actor-critic (discrete)
179:             probs = self.forward(state)
180:             m = Categorical(probs)
181:             action = m.sample().to(device)
182:             log_prob = m.log_prob(action)
183: 
184:             return action.detach().cpu().numpy(), log_prob.squeeze(0), m.entropy().mean()
185: 
186:         elif not DISCRETE and not DETERMINISTIC: # soft actor-critic (continuous)
187:             self.action_range = 30."
R8;False_Positive;True_Positive;./files/EvalID_66/ac.py;190;Incorrectly detected (not in ground truth);"182:             log_prob = m.log_prob(action)
183: 
184:             return action.detach().cpu().numpy(), log_prob.squeeze(0), m.entropy().mean()
185: 
186:         elif not DISCRETE and not DETERMINISTIC: # soft actor-critic (continuous)
187:             self.action_range = 30.
188:             self.epsilon = 1e-6
189: 
190:             mean, log_std = self.forward(state)
191:             std = log_std.exp()
192:             normal = Normal(0, 1)
193:             z = normal.sample().to(device)
194:             action0 = torch.tanh(mean + std*z.to(device)) # TanhNormal distribution as actions; reparameterization trick
195:             action = self.action_range * action0
196: 
197:             log_prob = Normal(mean, std).log_prob(mean+ std*z.to(device)) - torch.log(1. - action0.pow(2) + self.epsilon) -  np.log(self.action_range)
198:             log_prob = log_prob.sum(dim=1, keepdim=True)"
R9;True_Positive;True_Positive;./files/EvalID_25/CachedMultipleNegativesRankingLoss.py;60;Correctly detected;"52:                     sentence_feature=sentence_feature,
53:                     with_grad=True,
54:                     copy_random_state=False,
55:                     random_states=random_states,
56:                 ),
57:                 grad,
58:             ):
59:                 surrogate = torch.dot(reps_mb.flatten(), grad_mb.flatten()) * grad_output
60:                 surrogate.backward()
61: 
62: 
63: class CachedMultipleNegativesRankingLoss(nn.Module):
64:     def __init__(
65:         self,
66:         model: SentenceTransformer,
67:         scale: float = 20.0,
68:         similarity_fct: callable[[Tensor, Tensor], Tensor] = util.cos_sim,"
R9;True_Positive;True_Positive;./files/EvalID_25/CachedMultipleNegativesRankingLoss.py;234;Correctly detected;"226:             batch_size,
227:             self.mini_batch_size,
228:             desc=""Preparing caches"",
229:             disable=not self.show_progress_bar,
230:         ):
231:             e = b + self.mini_batch_size
232:             scores: Tensor = self.similarity_fct(embeddings_a[b:e], embeddings_b) * self.scale
233:             loss_mbatch: torch.Tensor = self.cross_entropy_loss(scores, labels[b:e]) * len(scores) / batch_size
234:             loss_mbatch.backward()
235:             losses.append(loss_mbatch.detach())
236: 
237:         loss = sum(losses).requires_grad_()
238: 
239:         self.cache = [[r.grad for r in rs] for rs in reps]  # e.g. 3 * bsz/mbsz * (mbsz, hdim)
240: 
241:         return loss
242:"
R9;True_Positive;True_Positive;./files/EvalID_99/gp_models.py;80;Correctly detected;"72:     )
73:     scheduler = torch.optim.lr_scheduler.ExponentialLR(adam, gamma=gamma)
74: 
75:     report_frequency = 10
76: 
77:     # training loop
78:     for step in range(args.num_steps):
79:         loss = -gp.log_prob(data[0:T_train, :]).sum() / T_train
80:         loss.backward()
81:         adam.step()
82:         scheduler.step()
83: 
84:         if step % report_frequency == 0 or step == args.num_steps - 1:
85:             print(""[step %03d]  loss: %.3f"" % (step, loss.item()))
86: 
87:     # plot predictions for three output dimensions
88:     if args.plot:"
R9;True_Positive;True_Positive;./files/EvalID_32/recom_amr.py;226;Correctly detected;"218: 
219:                 Xuij = _inner(gamma_u, gamma_diff) + _inner(gamma_u, feat_diff.mm(E))
220: 
221:                 log_likelihood = torch.nn.functional.logsigmoid(Xuij).sum()
222: 
223:                 # adversarial part
224:                 feat_i.retain_grad()
225:                 feat_j.retain_grad()
226:                 log_likelihood.backward(retain_graph=True)
227:                 feat_i_delta = feat_i.grad
228:                 feat_j_delta = feat_j.grad
229: 
230:                 adv_feat_diff = feat_diff + (feat_i_delta - feat_j_delta)
231:                 adv_Xuij = _inner(gamma_u, gamma_diff) + _inner(
232:                     gamma_u, adv_feat_diff.mm(E)
233:                 )
234:"
R9;True_Positive;True_Positive;./files/EvalID_29/train.py;120;Correctly detected;"112:             with torch.jit.fuser(""fuser2""), amp.autocast(enabled=args.use_amp):
113:                 predictions = model(batch)
114:                 targets = batch['target'][:,config.encoder_length:,:]
115:                 p_losses = criterion(predictions, targets)
116:                 loss = p_losses.sum()
117:             if global_step == 0 and args.ema_decay:
118:                 model_ema(batch)
119:             if args.use_amp:
120:                 scaler.scale(loss).backward()
121: 
122:             else:
123:                 loss.backward()
124:             if not args.grad_accumulation or (global_step+1) % args.grad_accumulation == 0:
125:                 if args.use_amp:
126:                     scaler.unscale_(optimizer)
127:                 if args.clip_grad:
128:                     torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)"
R9;True_Positive;True_Positive;./files/EvalID_29/train.py;123;Correctly detected;"115:                 p_losses = criterion(predictions, targets)
116:                 loss = p_losses.sum()
117:             if global_step == 0 and args.ema_decay:
118:                 model_ema(batch)
119:             if args.use_amp:
120:                 scaler.scale(loss).backward()
121: 
122:             else:
123:                 loss.backward()
124:             if not args.grad_accumulation or (global_step+1) % args.grad_accumulation == 0:
125:                 if args.use_amp:
126:                     scaler.unscale_(optimizer)
127:                 if args.clip_grad:
128:                     torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)
129:                 if args.use_amp:
130:                     scaler.step(optimizer)
131:                     scaler.update()"
R9;True_Positive;True_Positive;./files/EvalID_31/torch_utils.py;123;Correctly detected;"115:                 flops = 0
116: 
117:             try:
118:                 for _ in range(n):
119:                     t[0] = time_sync()
120:                     y = m(x)
121:                     t[1] = time_sync()
122:                     try:
123:                         _ = (sum(yi.sum() for yi in y) if isinstance(y, list) else y).sum().backward()
124:                         t[2] = time_sync()
125:                     except Exception:  # no backward method
126:                         # print(e)  # for debug
127:                         t[2] = float('nan')
128:                     tf += (t[1] - t[0]) * 1000 / n  # ms per op forward
129:                     tb += (t[2] - t[1]) * 1000 / n  # ms per op backward
130:                 mem = torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0  # (GB)
131:                 s_in, s_out = (tuple(x.shape) if isinstance(x, torch.Tensor) else 'list' for x in (x, y))  # shapes"
R9;True_Positive;True_Positive;./files/EvalID_28/deepfool.py;110;Correctly detected;"102: 
103:     fs = model.forward(x)
104:     fs_list = [fs[0,output[k]] for k in range(num_classes)]
105:     current_pred_label = label
106: 
107:     for i in range(max_iter):
108: 
109:         pert = np.inf
110:         fs[0, output[0]].backward(retain_graph = True)
111:         grad_orig = x.grad.data.cpu().numpy().copy()
112: 
113:         for k in range(1, num_classes):
114:             zero_gradients(x)
115: 
116:             fs[0, output[k]].backward(retain_graph=True)
117:             cur_grad = x.grad.data.cpu().numpy().copy()
118:"
R9;True_Positive;True_Positive;./files/EvalID_27/run_bertology.py;96;Correctly detected;"88: 
89:         # Do a forward pass (not with torch.no_grad() since we need gradients for importance score - see below)
90:         outputs = model(**inputs, head_mask=head_mask)
91:         loss, logits, all_attentions = (
92:             outputs[0],
93:             outputs[1],
94:             outputs[-1],
95:         )  # Loss and logits are the first, attention the last
96:         loss.backward()  # Backpropagate to populate the gradients in the head mask
97: 
98:         if compute_entropy:
99:             for layer, attn in enumerate(all_attentions):
100:                 masked_entropy = entropy(attn.detach()) * inputs[""attention_mask""].float().unsqueeze(1)
101:                 attn_entropy[layer] += masked_entropy.sum(-1).sum(0).detach()
102: 
103:         if compute_importance:
104:             head_importance += head_mask.grad.abs().detach()"
R9;True_Positive;True_Positive;./files/EvalID_26/run_pretraining.py;511;Correctly detected;"503:                     divisor = args.gradient_accumulation_steps
504:                     if args.gradient_accumulation_steps > 1:
505:                         if not args.allreduce_post_accumulation:
506:                             # this division was merged into predivision
507:                             loss = loss / args.gradient_accumulation_steps
508:                             divisor = 1.0
509:                     if args.fp16:
510:                         with amp.scale_loss(loss, optimizer, delay_overflow_check=args.allreduce_post_accumulation) as scaled_loss:
511:                             scaled_loss.backward()
512:                     else:
513:                         loss.backward()
514:                     average_loss += loss.item()
515: 
516:                     if training_steps % args.gradient_accumulation_steps == 0:
517:                         lr_scheduler.step()  # learning rate warmup
518:                         global_step = take_optimizer_step(args, optimizer, model, overflow_buf, global_step)
519:"
R9;True_Positive;True_Positive;./files/EvalID_26/run_pretraining.py;513;Correctly detected;"505:                         if not args.allreduce_post_accumulation:
506:                             # this division was merged into predivision
507:                             loss = loss / args.gradient_accumulation_steps
508:                             divisor = 1.0
509:                     if args.fp16:
510:                         with amp.scale_loss(loss, optimizer, delay_overflow_check=args.allreduce_post_accumulation) as scaled_loss:
511:                             scaled_loss.backward()
512:                     else:
513:                         loss.backward()
514:                     average_loss += loss.item()
515: 
516:                     if training_steps % args.gradient_accumulation_steps == 0:
517:                         lr_scheduler.step()  # learning rate warmup
518:                         global_step = take_optimizer_step(args, optimizer, model, overflow_buf, global_step)
519: 
520:                     if global_step >= args.max_steps:
521:                         last_num_steps = int(training_steps / args.gradient_accumulation_steps) % args.log_freq"
R9;True_Positive;True_Positive;./files/EvalID_30/train.py;160;Correctly detected;"152:     total_steps=0
153:     ds_loss = torch.zeros(2).to(local_rank)
154: 
155:     for batch_idx, batch in enumerate(train_dataloader):
156:       batch = {k: v.to(device) for k, v in batch.items()}
157:       output = model(**batch)
158:       if dist.get_rank() == 0: print(f""Processing training batch {batch_idx}"")
159:       loss = output[""loss""]
160:       loss.backward()
161:       ds_loss[0] += loss.item()
162:       ds_loss[1] += len(batch[""input_ids""])
163:       optimizer.zero_grad()
164:       lr_scheduler.step()
165:       total_steps += 1
166:       if args.max_steps is not None and total_steps > args.max_steps:
167:         break
168:"
R9;False_Positive;True_Positive;./files/EvalID_63/linr_layer.py;208;Incorrectly detected (not in ground truth);"200:             .mean()
201:             .get_plain_text(group=self.group, dst=dst)
202:         )
203:         if dst is not None and dst != self.ctx.rank:
204:             return None
205:         return dz_mean_square
206: 
207:     def backward(self):
208:         self.z.backward(self.dz / self.dz.share.shape[0])
209: 
210: 
211: class SSHEOptimizerSGD:
212:     def __init__(self, ctx: Context, params, lr=0.05):
213:         self.ctx = ctx
214:         self.params = params
215:         self.lr = lr
216:"
R9;False_Positive;True_Positive;./files/EvalID_28/deepfool.py;116;Incorrectly detected (not in ground truth);"108: 
109:         pert = np.inf
110:         fs[0, output[0]].backward(retain_graph = True)
111:         grad_orig = x.grad.data.cpu().numpy().copy()
112: 
113:         for k in range(1, num_classes):
114:             zero_gradients(x)
115: 
116:             fs[0, output[k]].backward(retain_graph=True)
117:             cur_grad = x.grad.data.cpu().numpy().copy()
118: 
119:             # set new w_k and new f_k
120:             w_k = cur_grad - grad_orig
121:             f_k = (fs[0, output[k]] - fs[0, output[0]]).data.cpu().numpy()
122: 
123:             pert_k = abs(f_k)/np.linalg.norm(w_k.flatten())
124:"
R9;False_Negative;True_Negative;./files/EvalID_29/train.py;134;Missed detection (in ground truth but not detected);"126:                     scaler.unscale_(optimizer)
127:                 if args.clip_grad:
128:                     torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)
129:                 if args.use_amp:
130:                     scaler.step(optimizer)
131:                     scaler.update()
132:                 else:
133:                     optimizer.step()
134:                 optimizer.zero_grad()
135:                 if args.ema_decay:
136:                     model_ema.update(model)
137: 
138:             if args.distributed_world_size > 1:
139:                 dist.all_reduce(p_losses)
140:                 p_losses /= args.distributed_world_size
141:                 loss = p_losses.sum()
142:"