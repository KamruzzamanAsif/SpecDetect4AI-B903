# Smell: Gradients Not Cleared before Backward Propagation

## Motivation  

In PyTorch (and PaddlePaddle) the gradients computed `duringloss.backward()` are accumulated in each tensor’s 
`.grad` field.
If you forget to flush them between batches, the new gradients are added to the old ones, which causes  

exploding or drifting parameter updates, unstable learning curves, misleading loss / metric plots.

The canonical reset is:

```python
# PyTorch
optimizer.zero_grad()
loss.backward()
optimizer.step()

# PaddlePaddle
loss.backward()
optimizer.step()
optimizer.clear_grad()

```


---

## Detection strategy (high‑level view)

1. **AST parsing**  
   Convert the file to an AST and enrich it with `parent` pointers.

2. **Locate backward / gradient calls**  
     Every ast.Call whose attribute is backward.

3. **Check for a matching reset**
    For each call:
    • look before the line for `optimizer.zero_grad()` (PyTorch)
    • or, in a Paddle environment, look after for `optimizer.clear_grad()`,
    • ignore calls inside with `torch.no_grad()`: blocks.


4. **Reporting**  

   When such a node is found the detector emits 
   REPORT: `optimizer.zero_grad()` not called before `backward()` at line `n `.



---

## Practical meaning for the developer 


### Missing reset
```python
for data, target in loader:
    output = model(data)
    loss   = criterion(output, target)
    loss.backward()        # ← accumulates forever
    optimizer.step()

```

### Refactor
```python
for data, target in loader:
    optimizer.zero_grad()  # ① flush old grads
    output = model(data)
    loss   = criterion(output, target)
    loss.backward()        # ② compute new grads
    optimizer.step()       # ③ update parameters

```

### PaddlePaddle variant
```python
loss.backward()
optimizer.step()
optimizer.clear_grad()     # mandatory flush

```


---

## Limitations  

- Inter-file analysis is not done: if `zero_grad()` is hidden in a helper
function the rule may raise a false positive. 

- Custom wrappers (`model.backward_loss()`, `opt.reset()`, etc.) are not
recognised unless their names are added to the pattern list. 

- The rule checks presence, not exact order in complex control-flow
(e.g. `zero-grad` inside an if whose condition sometimes skips).

