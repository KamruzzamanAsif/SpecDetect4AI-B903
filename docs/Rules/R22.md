# Smell: No Scaling Before Scale-Sensitive Operations

## Motivation  

Many classical ML algorithms are sensitive to the absolute scale of the
features.  

Examples:

- linear models with ℓ₂ regularisation (LogisticRegression, Ridge, …),  
- maximum-margin methods (SVC),  
- gradient-based optimisers (SGDClassifier / Regressor, neural nets),  
- distance-based methods (KMeans, KNeighborsClassifier),  
- variance analysis (PCA).

If you call `.fit()` on such estimators with raw, heterogeneously-scaled
columns, you may obtain:

- poor convergence or failure to converge,  
- domination of high-magnitude features,  
- coefficients that are hard to interpret,  
- warnings about ill-conditioned matrices.

Best practice is to apply a normalisation step `(StandardScaler, MinMaxScaler, ...)` immediately before the fit, or to wrap both stages in a `sklearn.pipeline.Pipeline`.

```python
#  good
pipe = make_pipeline(StandardScaler(), LogisticRegression())
pipe.fit(X_train, y_train)
```


---

## Detection strategy (high‑level view)

1. **AST parsing**  
   Build the AST and attach `parent` links (`add_parent_info` predicate).

2. **Collect scale-sensitive estimators**  
    `gather_scale_sensitive_ops` finds assignments like `model = LogisticRegression(...)` or `km = KMeans()`
and stores the variable name → estimator type mapping.

3. **Find .fit() calls**
    `isScaleSensitiveFit(node, variable_ops)` returns `True` when `model.fit(...)` is invoked and model belongs to the above mapping.

4. **Check for an earlier scaler**
    (predicate `hasPrecedingScaler()` – not shown in the snippet) scans the code before the `.fit()` line for a transformer whose class is recognised as a scaler or for a Pipeline that contains one.

5. **Excluse validation pipelines**
    `isPartOfValidatedPipeline() `discards cases already wrapped in a `sklearn.pipeline.Pipeline/ColumnTransformer`.


6. **Reporting**  

   When a scale-sensitive .fit() is found with no prior scaling step,
   the rule emitsREPORT: Call to a sensitive function detected without prior scaling ... at line `n `.



---

## Practical meaning for the developer 


### Missing scaling
```python
model = SVC(kernel="rbf")
model.fit(X_train, y_train)            # ←raw pixels and ages mixed!
```

### Refactor
```python
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
model    = SVC(kernel="rbf")
model.fit(X_train_s, y_train)
```

### Or even better
```python
pipe = make_pipeline(StandardScaler(), SVC(kernel="rbf"))
pipe.fit(X_train, y_train)
```


---

## Limitations  

- The rule only recognises a fixed list of `scalers / estimators.Custom` transformers or exotic libraries must be added manually.  
- Inter-file analysis is not performed; scaling done in another module
will not be detected.  
- Complex feature-engineering graphs (e.g. `ColumnTransformer` assembled dynamically) may evade the heuristic and raise false positives or miss genuine issues.

